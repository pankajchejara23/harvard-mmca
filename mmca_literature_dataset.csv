year,citation,sensors,data,lg_metric,sm_metric,org_metric,org_outcome,sm_outcome,lg_outcome,instrument,settings,analysis
1999,Modelling Symmetry of Activity as an Indicator of Collocated Group Collaboration,camera,kinesiology,head,head motion,head rotation,focus of attention,coordination,process,researcher coded,ecological,unsup. machine learning
1999,Mediated attention with multimodal augmented reality,camera:motion detecting sensor,eye gaze:kinesiology,gaze:head,visual attention:head motion,eye gaze directions:head movements,focus of attention,coordination,process,researcher coded,ecological,sup. machine learning:sup. machine learning
2000,"RemoteCoDe: Robotic Embodiment for Enhancing Peripheral
Awareness in Remote Collaboration Tasks",camera:microphone,video:audio,body:verbal,gross body motion:speech participation,gesture:communication quality,task performance,performance,product,researcher coded,ecological,ANOVA:ANOVA
2001,"Estimating conversational dominance in multiparty interaction
","wearable sensor, wristband:ecg:other",eda:ecg:other,physiological,combined,physiological compliance,task performance:task performance:coordination,performance:performance:coordination,product:product:process,log data:researcher coded:researcher coded,lab,regression:regression:regression
2001,Coordinating cognition: The costs and benefits of shared gaze during collaborative search,eye tracker:microphone,eye gaze:audio,gaze:verbal,visual attention:speech features,fixation:speech utterance,conversational attention,attention,process,researcher coded,ecological,t-test:t-test
2001,Putting the Pieces Together: Multimodal Analysis of Social Attention in Meetings,camera:microphone,eye gaze:audio,gaze:verbal,visual attention:speech features,gaze focus:sound focus,focus of attention,group dynamics,process,researcher coded,lab,sup. machine learning:sup. machine learning
2002,Implicit user-adaptive system engagement in speech and pen interfaces,camera:microphone:motion detecting sensor,eye gaze:audio:kinesiology,gaze:head:verbal,visual attention:head motion:speech features,eye gaze directions:head movements:speech features,focus of attention,coordination,process,researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning
2002,Coordinating spatial referencing using shared gaze,camera,eye gaze:audio,gaze,visual attention,gaze awareness,task performance:collaboration quality,performance:collaboration,product:process,log data:log data,lab,ANOVA:ANOVA
2003,A Look Is Worth a Thousand Words: Full Gaze Awareness in Video-Mediated Conversation,digital pen,bvp,body,gesture drawing,gesture drawing on video feed,task performance,collaboration,process,log data,lab,ANOVA
2003,EEG in classroom: EMD features to detect situational interest of students during learning,eye tracker,eye gaze,gaze,visual attention,focus of attention,task performance:quality of assistance:communication efficiency,performance:coordination:coordination,product:process:process,log data:questionnaire:,lab,ANOVA:ANOVA:ANOVA
2004,Analyzing and predicting focus of attention in remote collaborative tasks,eye tracker,eye gaze,gaze,eye motion,eye gaze trace,performance,performance,product,log data,lab,ANOVA
2005,Automatically Recognizing Facial Indicators of Frustration: A Learning-centric Analysis,eye tracker,eye gaze,gaze,eye motion,eye movements,task performance,performance,product,task outcome,lab,ANOVA
2005,"Influencing social dynamics in meetings through a peripheral display
",log data,log data,log data:log data:log data,task-related:task-related:task-related,task properties:people's actions:message content,focus attention,coordination,process,log data,lab,sup machine learning:sup machine learning:sup machine learning
2006,Reciprocal attentive communication in remote meeting with a humanoid robot,camera:microphone,video:audio,body:verbal,worker's actions:dialogue,gross body movements:dialogue episodes,focus of attention,collaboration,process,researcher coded,lab,sup. machine learning:sup. machine learning
2006,"Towards Adapting Fantasy, Curiosity and Challenge in
Multimodal Dialogue Systems for Preschoolers",microphone,audio,verbal:verbal:verbal,speech instructions:adjacent utterances:speech features,speech features:speech features:speech style,amplitude difference:command usage,collaboration:coordination,process:process,log data:researcher coded,NS,t-test:t-test:t-test:t-test:t-test:t-test
2007,Multimodal Real-Time Focus of Attention Estimation in SmartRooms,eye tracker,eye gaze,gaze:gaze,visual attention:visual attention,reaction to eye contact:user-initiative joint attention,subconscious interest:favorable feeling,engagement:interpersonal relationship,process:process,self report:self report,lab,t-test:t-test
2007,"Using Audio and Video Features to Classify the Most
Dominant Person in a Group Meeting",eye tracker,eye gaze,gaze,visual attention,eye movements,conversational contents:orienting effect,communication:coordination,process:process,log data:researcher coded,lab,ANOVA:t-test
2007,Towards High-Level Human Activity Recognition through Computer Vision and Temporal Logic,microphone:camera,audio:kinesiology,verbal:verbal:body,speech features:speech features:gross body motion,speech length:speaking energy:motion activity,dominance,group dynamics,process,researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning
2007,Recognizing communicative facial expressions for discovering interpersonal emotions in group meetings,eye tracker:microphone,eye gaze:audio,gaze:verbal,visual attention:speech participation,attention:speech time,quality of collaboration,collaboration,process,log data,lab,t-test:t-test
2007,Gaze quality assisted automatic recognition of social contexts in collaborative Tetris,camera,video,gaze,gaze frequency,gaze frequency,conversational attention,attention,process,questionnaire,ecological,Tukey’s HSD test
2008,"Automatic prediction of individual performance from"" thin slices"" of social behavior",camera:microphone,video:audio,body:verbal,gross body motion:speech features,gross body movements:speech features,personality,group composition,condition,questionnaire,lab,sup. machine learning:sup. machine learning
2008,"Supervised machine learning in multimodal learning analytics
for estimating success in project-based learning",camera,video,body,gross body motion,physical interactivity,cps competencies,performance,product,researcher coded,lab,ANOVA
2008,Multi-party focus of attention recognition in meetings from head pose and multimodal contextual cues,microphone:digital pen,audio:log data,verbal:log data,speech features:task-related,speech amplitude:pen pressure,task performance,performance,product,research coded,lab,t-test:t-test
2008,"Coordination of Communication: Effects of Shared Visual
Context on Collaborative Work",microphone:camera:log data,audio:kinesiology:log data,verbal:head:log data,speech features:head motion:task-related,speech features:head rotations:slide data,focus of attention,coordination,process,researcher coded,lab,unsup. machine learning:unsup. machine learning:unsup. machine learning
2008,Effectsof head-mounted and scene-oriented video systems onremote collaboration on physical tasks,camera,kinesiology,head,head motion,head rotation,focus of attention,coordination,process,researcher coded,lab,unsup. machine learning
2008,Visual focus of attention estimation from head pose posterior probability distributions,microphone,audio,verbal,speech features,speech time,speaking dynamics:group interaction:distraction level,group dynamics:engagement:engagement,process:process:process,log data:researcher coded:researcher coded,ecological,ANOVA:ANOVA:ANOVA
2008,Biosignals reflect pair-dynamics in collaborative work: EDA and ECG study of pair-programming in a classroom environmen,camera,kinesiology,head,head motion,head pose,focus of attention,coordination,process,researcher coded,lab,unsup. machine learning
2008,Many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities,eye tracker,eye gaze,gaze:gaze,visual attention:visual attention,concept map fixation time ratio:number of concept-map eye-gaze transitions,learning measures,performance,product,questionnaire,lab,negative correlation:negative correlation
2008,Gestural Communication over Video Stream: Supporting Multimodal Interaction for Remote Collaborative Physical Tasks,eye tracker:microphone,eye gaze:audio,gaze:verbal,visual attention:speech participation,shared gaze:shared voice,task performance,performance,product,log data,lab,t-test:t-test
2008,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,camera:microphone,video:audio,verbal:gaze,speech features:visual attention,audio energy features:visual attention,dominance,group composition,condition,researcher coded,lab,sup. machine learning:sup. machine learning
2009,Estimating focus of attention based on gaze and sound,microphone,audio,verbal:verbal:verbal:verbal:verbal:verbal:verbal,speech features:speech features:speech features:speech features:speech features:speech features:speech features,speech length:speaking turns:successful interruptions:backchannels:fraction of overlapped speech:fraction of silence,type of meeting:collocated meeting inference:participation,group dynamics:group dynamics:group dynamics,process:process:process,log data:log data:log data,lab,unsup.machine.learning:unsup.machine.learning:unsup.machine.learning
2009,Toward Open-Microphone Engagementfor Multiparty Interactions,eeg sensor,eeg:ecg:eda:bvp,physiological:physiological:physiological:physiological,brain:brain:brain:brain,physiological:physiological:physiological:physiological,affective states,affective,process,self-report,lab,machine learning:machine learning:machine learning:machine learning
2009,Modeling the Personality of Participants During Group Interactions,microphone,audio,verbal,speech features,speech usage,task completion:fantasy levels,performance:affective,product:process,researcher coded:researcher coded,lab,correlation:correlation
2009,Combining audio and video to predict helpers' focus of attention in multiparty remote collaboration on physical tasks,camera:microphone,video:audio,body:verbal,body:speech features,gross body movements:speech features,personality,group composition,condition,questionnaire,lab,sup. machine learning:sup. machine learning
2009,Predicting remote versus collocated group interactions using nonverbal cues,camera:microphone,video:audio,body:verbal,body:speech features,gross body movements:speech features,task performance,product,product,researcher coded,lab,unsup-machine learning:unsup-machine learning
2009,The NISPI framework: Analysing collaborative problem-solving from students' physical interactions,log data,log data,log data,task-related,task-context features,task performance,perspective:argument,product:product,researcher coded,lab,Wilcoxon Signed Ranks test:Wilcoxon Signed Ranks test
2009,Gaze-communicative behavior of stuffed-toy robot with joint attention and eye contact based on ambient gaze-tracking,eye tracker,eye gaze,gaze:head:verbal,visual attention:head motion:speech features,reaction time,search time:error rate,attention:performance,process:product,log data:researcher coded,lab,t-test:t-test
2009,Affective e-Learning: Using “Emotional” Data to Improve Learning in Pervasive Learning Environment,camera,video,head,facial expressions,affective state,emotion network,collaboration,process,researcher coded,lab,unsup. machine learnig
2010,Multimodal Analysis of Group Attitudes Towards Meeting Management,eye tracker:microphone,eye gaze:audio,gaze:verbal,visual attention:speech features,shared gaze:shared speech,task performance,performance,product,log data,lab,t-test:t-test
2010,Modeling focus of attention for meeting indexing,eye tracker:own application,eye gaze:log data,gaze:gaze:log data,visual attention:eye motion:task-related,gaze fixations:gaze saccades:task actions,group contexts,group dynamics,process,researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning
2010,Modeling focus of attention for meeting indexing based on multiple cues,camera:microphone,eye gaze:video:audio,gaze:head:verbal,visual attention:head motion:speech features,eye gaze directions:head movements:speech features,focus of attention,coordination,process,researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning
2010,"Toward Automated Detection of Phase Changes in Team Collaboration
",camera:microphone:camera,video:audio:kinesiology,gaze:body:verbal,visual attention:gross body motion:speech features,focus of attention:gestures:speech features,group activity,group dynamics,process,researcher coded,ecological,unsup. machine learning:unsup. machine learning:unsup. machine learning
2010,Understanding collaborative program comprehension: Interlacing gaze and dialogues,microphone,audio,verbal,speech features,speech time,verbal participation,communication,process,survey,lab,t-test
2010,See What I’m Saying? Using Dyadic Mobile Eye Tracking to Study Collaborative Reference,camera:microphone,video:audio,verbal:gaze:gaze,speech participation:visual attention:visual attention,speech length:attention received per person:attention given by person,personality,group composition,condition,self-report,lab,sup. machine learning:sup. machine learning:sup. machine learning
2010,Acoustic-Prosodic Entrainment and Rapport in Collaborative Learning Dialogues,eye tracker:log data,eye gaze:log data,gaze:gaze:gaze:log data:log data,visual attention:eye motion:visual attention:task-related:task-related,gaze location:gaze saccades:gaze fixation:player actions:zoid acceleration,social context,group composition,condition,assigned,lab,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2011,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,interactive tabletop,log data,log data,task-related,events,group performance,performance,product,researcher coded,lab,unsup. machine learning
2011,Employing Social Gaze and Speaking Activity for Automatic Determination of the Extraversion Trait,video camera array,kinesiology,body,hand motion,gesture type and location,frequency of utterances:subjective workload,communication:cognitive engagement,process:process,calculation:survey,lab,ANOVA:ANOVA
2011,Multi-modal analysis of small-group conversational dynamics,microphone:log data,audio:log data,verbal:log data,speech features:physical participation,verbal participation:physical participation,collaboration,collaboration,process,researcher coded,ecological,unsup. machine learning:unsup. machine learning
2011,Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration,eye tracker,eye gaze,gaze,visual attention,gaze overlap,conversation contents,communication,process,researcher coded,lab,regression
2011,"Multimodal recognition of personality traits in social interactions
Authors
",camera,video,body,gesture fluency,gesture fluency,communicative intent,collaboration,process,researcher coded,lab,ANOVA
2011,Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs,microphone:own application,audio:log data,verbal:log data:verbal:verbal:log data,speech participation:task-related:speech participation:speech participation:task-related,speech quantity:physical participation quantity:number of active participants in group:verbal participation symmetry among group:physical participation symmetry among group,collaboration,communication,process,researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2012,Estimation of success in collaborative learning based on multimodal learning analytics features,microphone:digital pen,audio:log data,verbal:verbal:verbal:verbal:verbal:verbal:log data:log data:log data:log data:log data:log data,speech participation:speech features:speech features:speech features:speech features:speech features:text:text:text:text:text:text,pause duration:energy:articulation rate:fundamental frequency:peak slope:spectral stationarity:writing rate:writing area:aspect ration:pressure:uninterrupted writing:pause distribution/average pauses,leadership:expertise,group composition:group composition,condition:condition,assigned:task outcome,lab,t-test:t-test:t-test:t-test:t-test:t-test:t-test:t-test
2012,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,microphone:camera,audio:video,verbal:verbal:verbal:gaze:gaze,speech participation:speech participation:speech participation:visual attention:visual attention,group participation speaking cues:silence and overlap cues:speaking distribution cues:visual attention:group looking cues,group composition:social perception:group performance,group composition:interpersonal relationship:performance,condition:process:product,questionnaire:questionnaire:negative distance between expert list and group list,lab,correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:regression:regression:regression:regression:regression
2012,Effects of a coactor's focus of attention on task performance,eye tracker:microphone,eye gaze:audio,gaze:gaze:verbal:verbal,visual attention:visual attention:speech features:speech participation,amount of eye gaze at others:amount of mutual gaze:amount of speech:breaking a silence,dominance,group dynamics,process,questionnaire,lab,regression:regression:regression:regression
2013,Deep neural networks for collaborative learning analytics: Evaluating team collaborations using student gaze point prediction,eye tracker:eye tracker,eye gaze:video,gaze:gaze,visual attention:eye physiology,joint visual attention:cognitive load (from pupil size,learning gain,learning,product,pre-post test,lab,mediation:mediation
2013,Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery,camera:microphone:digital pen,video:audio:log data,log data:body:body:verbal:verbal:verbal:verbal:verbal:log data:log data:log data:log data:log data:log data,task-related:gross body motion:location:speech participation:speech participation:speech content:speech content:speech content:text:text:text:text:text:text,calculator use:total movement:distance from the center of the table:number of interventions:speech time:times numbers were mentioned:times mathematical terms were mentioned:times commands were pronounced:total number of pen strokes:average number of points:average stroke time length:average stroke path length:average stroke displacement:average stroke pressure,task performance:expertise,performance:group composition,product:condition,researcher coded:researcher coded,lab,regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2013,On the Same Wavelength: Exploring Team Neurosynchrony in Undergraduate Dyads Solving a Cyberlearning Problem With Collaborative Script,eye tracker:ns,eye gaze:audio,gaze:verbal:gaze,visual attention:speech participation:visual attention,gaze episodes:dialogue episodes:gaze transitions,level of understanding,performance,product,researcher coded,lab,ANOVA
2013,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,microphone:camera,audio:video,verbal:gaze:gaze,speech participation:visual attention:visual attention,speaking activity:visual attention:audio-visual,personality:group interaction:dominance:task performance:leadership,group composition:interpersonal relationship:interpersonal relationship:performance:interpersonal relationship,condition:process:process:product:process,"neo-ffi, prf:questionnaire:questionnaire:expert grading:questionnaire",lab,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2013,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,microphone:interactive tabletop,audio:log data,verbal:log data,speech content:touch,sequences of verbal utterances:sequences of meaningful actions,colloboration quality,coordination,process,researcher coded,lab,unsup. machine learning:unsup. machine learning
2013,Effects of Knowledge Interdependence with the Partner on Visual and Action Transactivity in Collaborative Concept Mapping,kinect,kinesiology,body:body,gross body motion:gross body motion,posture estimation:hand-to-face gesture,learning gain,cognitive load,product,pre-post test,lab,negative correlation
2013,Predicting Collaborative Learning Quality through Physiological Synchrony Recorded by Wearable Biosensors,microphone:kinect:kinect,audio:video:kinesiology,body:body:head:verbal,gross body motion:hand motion:head motion:speech participation,upper body agitation:hand agitation:head orientation:speech length/turns,agreement,coordination,process,experts evaluation,lab; ecological,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2013,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,microphone,audio,verbal:verbal:verbal:verbal:verbal:verbal:verbal:verbal,speech participation:speech participation:speech participation:speech participation:speech participation:speech participation:speech participation:speech participation,duration of all vocalisations:average duration of vocalisation:standard deviation of vocalisation:probability of a transition from floor to a vocalisation:probability of a transition from a vocalisation to floor:probability of transitioning from a group vocalisation to speaker vocalisation:probability of transitioning from a speaker vocalisation to a group vocalisation:uncertainty in the transitions originating from a speaker,expertise,group composition,condition,task outcome,lab,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2013,Modelling and Identifying Collaborative Situations in a Collocated Multi-display Groupware Setting,microphone:own application,audio:log data,verbal:verbal:log data:log data,speech participation:speech participation:touch:touch,speech time and frequency:symmetry of speech among group:total number of touch actions:symmetry of touch actions among group,collaboration,coordination,process,researcher coded,lab; ecological,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2013,Physiological Linkage of Dyadic Gaming Experience,eye tracker:microphone,eye gaze:audio,gaze:gaze:verbal:gaze,visual attention:visual attention:speech content:eye motion,focus of attention:together gaze:dialogue episode:gaze transitions,level of understanding:conversational features,performance:coordination,product:process,researcher coded:researcher coded,lab,ANOVA:ANOVA:mixed linear model:mixed linear model:mixed linear model:mixed linear model:ANOVA
2013,Analysing frequent sequential patterns of collaborative learning activity around an interactive tabletop,eye tracker,eye gaze,gaze,visual attention,joint visual attention,learning gain:collaboration quality,learning:coordination,product:process,learning test:researcher coded,lab,regression:regression
2013,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,eye tracker:eye tracker,eye gaze:video,gaze:gaze,visual attention:eye physiology,joint visual attention:cognitive load (from pupil size,learning gain,learning,product,pre-post test,lab,mediation:mediation
2014,(Dis)Engagement Maters: Identifying Efficacious Learning Practices with Multimodal Learning Analytics,kinect:kinect:own application,video:kinesiology:log data,verbal:head:body:log data,speech content:facial expressions:hand motion:task-related,dialogue acts:facial expressions:gesture:task actions,engagement:frustration:learning gain,cognitive engagement:affective:learning,process:process:product,self report:self report:pre-post test,ecological,regression:regression:regression:regression:regression:regression:regression:regression:regression
2014,Can Eye Help You?: Effects of Visualizing Eye Fixations on Remote Collaboration Scenarios for Physical Tasks,microphone,audio,verbal:verbal:verbal:verbal:verbal:verbal:verbal,speech features:speech features:speech features:speech features:speech features:speech features:speech features,pitch:intensity:voice quality:speaking rate:proximity:convergence:synchrony,conversational features,interpersonal relationship,process,mixed - researcher coded & self-report,lab,correlation:correlation:correlation
2014,Detecting Collaborative Dynamics Using Mobile Eye-Trackers,eye tracker:ns,eye gaze:log data,gaze:verbal:verbal:verbal:verbal,visual attention:speech content:speech content:speech content:speech content,joint visual attention:n-grams:cosine similarity scores:convergence measures:coherence metrics,learning gain:collaboration quality,learning:coordination,product:process,learning test:researcher coded,lab,ANOVA:ANOVA:ANOVA:ANOVA:correlation:correlation:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2014,Looking To Understand: The Coupling Between Speakers’ and Listeners’ Eye Movements and Its Relationship to Discourse Comprehension,eye tracker,eye gaze,gaze:gaze,visual attention:visual attention,joint visual attention:gaze recurrence,learning gain,learning,product,pre-post test,lab,correlation
2014,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,varioport 16-bit digital skin conductance amplifier:modified lead ii configuration,eda:ecg,physiological,eda,physiological linkage,involvement:emotion:emotion:level of understanding,cognitive engagement:affective:affective:learning,process:process:process:product,questionnaire:questionnaire:questionnaire:questionnaire,lab,regression:regression:regression:regression
2014,Using Dual Eye-Tracking to Evaluate Students' Collaboration with an Intelligent Tutoring System for Elementary-Level Fractions,eye tracker:computer audio chat:computer,eye gaze:audio:log data,log data:gaze:log data,task-related:visual attention:task-related,hint behavior:joint visual attention - collaboration quality:moments of good collaboration,learning gain,learning,product,learning test,ecological,correlation:correlation
2014,Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning,wearable sensor:wearable sensor,eda:ecg,physiological:physiological:physiological:physiological:physiological:physiological:physiological:physiological:physiological:physiological:physiological,eda:eda:eda:eda:eda:heart:heart:heart:heart:heart:heart,sm - eda:idm - eda:da - eda:cc - eda:wc - eda:sm - hr:idm - hr:da - hr:cc - hr:wc - hr low frequency:wc - hr high frequency,task performance:workload,performance:cognitive engagement,product:process,task outcome:questionnaire,lab,LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME:LME
2014,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,eye tracker,eye gaze,gaze,visual attention,network features,collaboration quality,coordination,process,researcher coded,lab,sup. machine learning:correlation
2015,"Emergent leaders through looking and speaking: from
audio-visual data to multimodal recognition",eye tracker:microphone,eye gaze:audio,gaze:verbal:verbal:verbal,visual attention:speech content:speech content:speech content,joint visual attention:simple linguistic features:convergence of linguistic styles:coherence,learning gain:collaboration:joint visual attention,learning:coordination:coordination,product:process:process,pre-post test:coding scheme:calculation,lab,correlation:correlation:correlation:correlation:ANOVA:sup. machine learning
2015,Unpacking the relationship between existing and new measures of physiological synchrony and collaborative learning: a mixed methods study,kinect:own application,kinesiology:log data,log data:log data:body:body:body:body,task-related:task-related:gross body motion:gross body motion:gross body motion:location,amount of exploration:types of exploration:amount of movement:type of movement:body synchrony:body distance,individual learning gain:group learning gain:leadership,learning:learning:group composition,product:product:condition,pre-post test:pre-post test:researcher coded,lab,correlation:correlation:correlation:correlation:ANOVA:ANOVA:correlation:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2015,Does Seeing One Another’s Gaze Affect Group Dialogue? A Computational Approach,kinect:own application,kinesiology:log data,log data:log data:body,task-related:task-related:gross body motion,amount of exploration:types of exploration:amount of movement:type of movement:body synchronization:body distance,individual learning gain:group learning gain:leadership,learning:learning:interpersonal relationship,product:product:process,pre-post test:pre-post test:coding,lab,correlation:correlation:correlation:correlation:ANOVA:ANOVA:correlation:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2015,Challenging Joint Visual Attention as a Proxy for Collaborative Performance,eye tracker:microphone,eye gaze:audio,gaze:verbal,visual attention:speech participation,joint visual attention:simple linguistic features:convergence of linguistic styles:coherence,learning:collaboration,learning:coordination,product:process,pre-post test:coding scheme,lab,correlation:ANOVA:sup. machine learning
2015,Dynamic Adaptive Gesturing Predicts Domain Expertise in Mathematics,camera:microphone,video:audio,verbal:verbal:verbal:head:body:body:gaze,speech participation:speech features:speech features:head motion:gross body motion:gross body motion:visual attention,speaking status:pitch:energy:head motion:body motion:motion energy images:gaze,personality,group composition,condition,self-report,lab,unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning
2015,Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics,eye tracker,eye gaze,gaze,visual attention,joint visual attention,collaboration quality:student performance:learning gain,coordination:performance:learning,process:product:product,researcher coded:task outcome:pre-post test,lab; ecological,correlation:regression:correlation
2015,A Network Analytic Approach to Gaze Coordination during a Collaborative Task,eye tracker,eye gaze,gaze:gaze:gaze,visual attention:visual attention:visual attention,perceptual with-me-ness (gaze:conceptual with-me-ness (gaze:gaze similarity,learning gain,learning,product,pre-post test,lab,correlation:correlation:correlation
2016,"Understanding Collaborative Program Comprehension: Interlacing Gaze and Dialogues
",eye tracker,eye gaze,gaze:gaze:gaze,visual attention:visual attention:visual attention,gaze visual agitation:gaze spatial entropy:return levels - evt,quality of collaboration,coordination,process,task performance,lab,ANOVA:ANOVA:correlation
2016,Expertise estimation based on simple multimodal features,eye tracker:camera:microphone,eye gaze:video:audio,gaze:body:verbal,visual attention:hand motion:speech participation,joint visual attention:gestures:speech time,group performance:learning gain,performance:learning,product:product,researcher coded:learning test,lab,qualitative:qualitative:qualitative:correlation
2016,Personality classification and behaviour interpretation: An approach based on feature categories,eye tracker,eye gaze,gaze,visual attention,joint visual attention,task performance:learning gain,performance:learning,product:product,calculation:pre-post test,lab,correlation:correlation
2016,Using Mobile Eye-Trackers to Unpack the Perceptual Benefits of a Tangible User Interface for Collaborative Learning,optical see-through head-mounted display,eye gaze,gaze,visual attention,gaze location,quality of remote collaboration:task completion time,performance:performance,product:product,questionnaire:task outcome,lab,Wilcoxon Signed Ranks test:Wilcoxon Signed Ranks test
2016,(Dis)Engagement Matters: Identifying Efficacious Learning Practices with Multimodal Learning Analytics,microphone,audio,verbal:verbal:verbal:verbal:verbal:verbal,speech participation:speech participation:speech participation:speech participation:speech participation:speech features,duration of speech by each student:duration in which each student was only speaker:duration of overlapping speech from pairs of students:duration of overlapping speech from all people:duration of silence for all people:prosodic and tone features,collaboration quality,coordination,process,researcher coded,ecological,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2016,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,wearable sensor,eda,physiological:physiological:physiological:physiological:physiological,eda:eda:eda:eda:eda,signal matching:instantaneous derivative matching:directional agreement:pearson’s correlation coefficient:fisher’s z-transform of pearson's correlation coefficient,collaborative will:collaborative learning product:group learning gain,interpersonal relationship:performance:learning,process:product:product,self report:researcher coded:researcher coded,lab,regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:regression:regression
2016,An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop,camera,video,gaze,visual attention,visual field of attention on a person features,leadership,group composition,condition,questionnaire,lab,correlation:sup. machine learning
2016,Brain-to-Brain Synchrony Tracks Real-World Dynamic Group Interactions in the Classroom,electrodes:electrodes:electrodes,eda:ecg:other,physiological:head:physiological,eda:facial expressions:heart,eda synchrony:smiling synchrony:heart rate sychrony,team cohesion:routine choice,interpersonal relationship:coordination,process:process,questionnaire:researcher coded,lab,regression:regression:regression
2016,Investigating Automatic Dominance Estimation in Groups From Visual Attention and Speaking Activity,camera:microphone,video:audio,head:head:verbal,facial expressions:facial expressions:speech features,intra-personal features:dyadic features:one vs all features,personality:social perception,group composition:interpersonal relationship,condition:process,self-reported survey:questionnaire,lab,Regression:Regression:Regression:Regression:Regression:Regression
2016,Toward Using Multi-Modal Learning Analytics to Support and Measure Collaboration in Co-Located Dyads,touch screen,log data,log data,touch,touch patterns,social regulation,coordination,process,rogat and linnenbrink-garcia’s framework,ecological,calculation
2017,Using Interlocutor-Modulated Attention BLSTM to Predict Personality Traits in Small Group Interaction,camera:microphone:digital pen:digital pen,video:audio:log data:other,log data:log data:log data:verbal,task-related:task-related:task-related:speech features,card movements:scrolling:zooming:audio features,collaboration:asymmetric contribution:cooperation,coordination:coordination:coordination,process:process:process,researcher coded:researcher coded:researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2017,Using Eye-Tracking Technology to Support Visual Coordination in Collaborative Problem-Solving Groups,eye tracker:microphone,eye gaze:audio,gaze:verbal,visual attention:speech content,(not:dialogue episodes,level of understanding,learning,product,researcher coded,lab,ANOVA:ANOVA:ANOVA
2017,Modeling Collaboration Patterns on an Interactive Tabletop in a Classroom Setting,camera,video,gaze:body:body:body,visual attention:location:hand motion:hand motion,count of faces looking at screen:distance between learners:mean distance between hands (dbh:hand motion speed,engagement:synchronisation:individual accountability,cognitive engagement:coordination:coordination,process:process:process,researcher coded:researcher coded:researcher coded,lab,regression:regression:regression:regression:regression
2017,Multimodal prediction of expertise and leadership in learning groups,kinect:microphone:arduino ide,kinesiology:audio:log data,head:body:body:body:verbal:log data:log data:log data:log data:log data,head motion:location:hand motion:hand motion:speech features:task-related:task-related:task-related:task-related:task-related,number of faces looking at screen:mean distance between learners:mean distance between hands (dbh:mean hand movement speed:mean audio level:arduino measure of complexity:arduino active hardware blocks:arduino active software blocks:arduino active blocks:student work phases,task performance,performance,product,researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2017,Unpacking Collaborative Learning processes during Hands-on Activities using Mobile Eye-Trackers,kinect:arduino ide:microphone,kinesiology:log data:audio,head:body:body:log data:log data:log data:log data:verbal,head motion:location:hand motion:task-related:task-related:task-related:task-related:speech features,faces looking at screen:distance between learners:mean distance between hands (dbh:number of active blocks:variety of hardware blocks:variety of software blocks:number of interconnections between blocks:audio level,task performance,performance,product,researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning
2017,Focused or Stuck Together: Multimodal Patterns Reveal Triads’ Performance in Collaborative Problem Solving,camera:microphone,video:audio,verbal:verbal:head,speech content:speech features:facial expressions,linguistic features:voice features:facial expression,social perception:social perception:social perception,interpersonal relationship:interpersonal relationship:interpersonal relationship,process:process:process,questionnaire:questionnaire:questionnaire,lab,correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation
2017,Multi-modal Social Signal Analysis for Predicting Agreement in Conversation Settings,eeg sensor,eeg,physiological,brain,brain synchrony,engagement:social dynamics,cognitive engagement:interpersonal relationship,process:process,questionnaire:questionnaire,ecological,regression:regression
2017,Improving Visibility of Remote Gestures in Distributed Tabletop Collaboration,microphone:log data,audio:log data,verbal:verbal:log data,speech participation:speech features:task-related,speech time:prosodic speech features:movement of objects,collaboration quality,coordination,process,researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning
2017,3D Tangibles Facilitate Joint Visual Attention in Dyads,eye tracker,eye gaze,gaze,visual attention,joint visual attention,collaboration quality:collaboration:learning gain:task performance,coordination:coordination:learning:performance,process:process:product:product,researcher coded:researcher coded:pre-post test:task outcome,lab,correlation:correlation
2017,Detecting Emergent Leader in a Meeting Environment,camera:microphone,video:audio,body:verbal:verbal:verbal,gross body motion:speech participation:speech participation:speech participation,head/body movement:(non:speaking turn duration/number:interruption,leadership,group composition,condition,researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2018,Evaluation of user gestures in multi-touch interaction: a case study in pair-programming,microphone,audio,verbal:verbal,speech features:speech features,speech features:linguisitc features,social perception,collaboration,process,self report,university ,sup. machine learning:sup. machine learning
2018,Dual Gaze as a Proxy for Collaboration in Informal Learning,kinect:camera,kinesiology:video,body:log data,hand motion:task-related,clustered hand/wrist movement:object manipulation,learning gain,learning,product,pre-post test,lab,sup. machine learning
2018,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",eye tracker (glasses,eye gaze,gaze:gaze,visual attention:eye motion,gaze fixations:gaze saccades,task performance,performance,product,researcher coded,lab,correlation:correlation
2018,Real-time mutual gaze perception enhances collaborative  learning and collaboration quality,kinect,kinesiology,body:body:body,gross body motion:gross body motion:location,joint movement:joint angle:dyad proximity,task performance:collaboration:learning gain,performance:coordination:learning,product:process:product,researcher coded:researcher coded:pre-post test,lab,correlation:correlation:correlation:correlation:correlation
2018,Meeting mediator: enhancing group collaboration with sociometric feedback,eeg sensor,eeg,physiological,brain,brain waves patterns,situational interest,coordination,process,questionnaire,lab,sup. machine learning
2018,Gaze quality assisted automatic recognition of social contexts in collaborative Tetris,eye tracker,eye gaze,gaze:gaze,visual attention:visual attention,joint visual attention:cycles of collaborative / individual work,learning gain:collaboration quality:task performance,learning:coordination:performance,product:process:product,pre-post test:researcher coded:performance score,lab,correlation:correlation:correlation:correlation
2018,Unraveling Students’ Interaction Around a Tangible Interface Using Multimodal Learning Analytics,kinect:camera,kinesiology:video,body:body,hand motion:task-related,clustered hand/wrist movement:object manipulation,learning gain,learning,product,pre-post test,lab,sup. machine learning
2018,Looking AT versus Looking THROUGH: A Dual Eye-tracking Study in MOOC Context,microphone,audio,verbal:verbal,speech features:speech content,speech features:linguistic features,group performance,performance,product,experts evaluation,lab,sup. machine learning:sup. machine learning
2018,Predicting Group Performance in Task-Based Interaction,ns:ns,audio:video,verbal:verbal:verbal:head,speech features:speech features:speech features:facial expressions,gemaps acoustic features:extended gemaps acoustic features:mfccs:facial action units,laughter:laughter:laughter,interpersonal relationship:interpersonal relationship:interpersonal relationship,process:process:process,computation:computation:researcher coded,lab,sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning:sup. machine learning
2018,Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose,kinect:kinect:wearable sensor,video:audio:eda,physiological:physiological:physiological:physiological:verbal,combined:combined:combined:combined:speech participation,signal matching:instantaneous derivative matching:directional agreement:pearson’s correlation coefficient:speech activity,learning gain:collaboration quality:collaboration quality:colaboration quality:collaboration quality,learning:communication:coordination:coordination:coordination,product:process:process:process:process,pre-post test:researcher coded:researcher coded:researcher coded:researcher coded,lab,correlation:correlation:correlation:correlation:ANOVA:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation
2018,"Are we together or not? The temporal interplay of monitoring, physiological arousal and physiological synchrony during a collaborative exam",kinect:microphone:irma matrix tof sensors,video:audio:other,verbal:gaze:verbal,speech participation:visual attention:speech participation,non-verbal speaking metrics:visual attention:verbal dominance and information metrics,leadership:perceived contribution,interpersonal relationship:interpersonal relationship,process:process,questionnaire:questionnaire,lab,correlation:correlation:correlation:correlation:correlation:correlation:regression:regression:regression:regression:regression:regression
2018,Dynamics of Visual Attention in Multiparty Collaborative Problem Solving using Multidimensional Recurrence Quantification Analysis,ns,audio,verbal,speech content,speech utterances,personality,group composition,condition,self-reported survey,lab,sup. machine learning
2018,An Alternate Statistical Lens to Look at Collaboration Data: Extreme Value Theory,kinect:eye tracker:wearable sensor:kinect,audio:eye gaze:eda:kinesiology,body:verbal,gross body motion:speech participation,total movement across upper body joints and body parts:talking time,collaboration quality,coordination,process,researcher coded,lab,correlation:correlation
2018,Modeling people's focus of attention,wearable sensor:wearable sensor,eda:ecg,physiological,combined,social physiological compliance (spc,task performance,performance,product,log data,ecological,minimum-width envelope
2018,An Integrated Observing Technic for Collaborative Learning: The Multimodal Learning Analytics Based on the Video Coding and EEG Data Mining,camera:microphone:log data,video:audio:log data,body:log data:body:body:verbal:log data,gross body motion:task-related:hand motion:hand motion:speech features:task-related,total number of faces looking toward the screen (flls:total number of connected arduino components (idevhw:mean distance between hands (dbh:mean hand movement speed (hms:mean audio level (aud:mean arduino components activity (idec,task performance,performance,product,researcher coded,lab,regression:regression:regression:regression:regression:regression
2018,Temporal analysis of multimodal data to predict collaborative learning outcomes,wristband,eda,physiological,eda,mean range-corrected eda responses,emotion - negative:emotion - positive,cognitive,process,self report:self report,lab,corrleation
2019,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance",eda sensor:camera,eda:video,physiological:physiological,eda:combined,eda peak detection:physiological concordance index,emotion,cognitive engagement,process,researcher coded,ecological,correlation:correlation
2019,Leveraging Mobile Eye-Trackers to Capture Joint Visual Attention in Co-Located Collaborative Learning,microphone:web camera:electrodes,audio:video:eda,verbal:body:physiological,speech features:gross body motion:eda,speech rate:face and upper body movement:galvanic skin response,collaboration quality:perceived valence:perceived arousal:task performance,interpersonal relationship:affective:affective:performance,process:process:process:product,questionnaire:self- report:self- report:task score,lab,unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning
2019,Does Seeing One Another’s Gaze Affect Group Dialogue?,eye tracker,eye gaze,gaze:gaze:gaze,visual attention:visual attention:visual attention,gaze area of interest:cross-recurrence quantification analysis:multidimensional recurrence quantification analysis,construction of shared knowledge:negotiation:coordination:task performance:group performance,coordination:coordination:coordination:performance:performance,process:process:process:product:product,researcher coded:researcher coded:researcher coded:expert evaluation:self-reported survey,lab,regression:regression:regression:regression:regression:regression:regression
2019,Going beyond what is visible: What multichannel data can reveal about interaction in the context of collaborative learning?,eye tracker,eye gaze,gaze,visual attention,evt of spatial entropy,collaboration outcome:learning gain,performance:learning,product:product,ns:pre-post test,lab,regression:regression
2019,"The relationships between learner variables, tool-usage behaviour and performance",eeg sensor,audio:eeg,physiological,brain,brain synchrony,group performance:group performance,performance:coordination,product:process,researcher coded:log data,lab,positive correlation:negative correlation
2019,Task-independent Multimodal Prediction of Group Performance Based on product Dimensions,camera:microphone:wearable sensor,video:audio:eda,head:physiological,facial expressions:eda,facial expression:physiological simultaneous arousal,type of working activity:group interaction,coordination:communication,process:process,researcher coded:researcher coded,NS,calculation:ANOVA:calculation:calculation
2019,Capturing and analyzing verbal and physical collaborative learning interactions at an enriched interactive tabletop,camera:microphone:digital pen and digital paper,video:audio:log data,body:body:body,hand motion:hand motion:hand motion,total manual gestures per second:iconic gestures per second:deictic gestures per second,expertise,group composition,condition,researcher coded,lab,Wilcoxon Signed Ranks test:Wilcoxon Signed Ranks test:Wilcoxon Signed Ranks test
2019,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,eye tracker:microphone:kinect:smart wristband,eye gaze:audio:kinesiology:eda,verbal:body:physiological:gaze,speech features:gross body motion:eda:visual attention,coh-metrix indices:physical synchrony:physiological synchrony (pc:joint visual attention,learning gain:collaboration:conversational features,learning:coordination:communication,product:process:process,pre-post test:researcher coded:computation,lab,correlation:correlation:correlation:correlation:correlation:correlation:sup. machine learning
2019,Toward Collaboration Sensing,microphone:wearable sensor:digital pen,audio:kinesiology:log data,verbal:verbal:head:verbal,speech participation:speech features:head motion:speech content,speaking turn features:acoustic features:head motion features:linguistic features,task performance,performance,product,researcher coded,lab,unsup. machine learning:unsup. machine learning:unsup. machine learning:unsup. machine learning
2020,Body synchrony in triadic interaction,eye tracker,eye gaze,gaze,visual attention,shared gaze,cognitive load:collaboration quality:task performance:gaze overlap,cognitive engagement:coordination:performance:coordination,process:process:product:process,questionnaire:self-report:task outcome:calculation,lab,ANOVA:ANOVA:ANOVA:ANOVA
2020,What does physiological synchrony reveal about metacognitive experiences and group performance?,kinect,kinesiology,body:body:body:body,location:location:location:location,time spent individually:time spent as a group:diversity of collaborative interactions:transition probabilities between collaborative state,technical ability:social ability:time spent in makerspace:emotion,performance:interpersonal relationship:performance:affective,product:process:product:process,researcher coded:researcher coded:researcher coded:survey,ecological,correlation:correlation:correlation:correlation:correlation:correlation
2020,Inter-brain synchrony in teams predicts collective performance,camera,video,body:body,hand motion:hand motion,metrics of frequency:metrics of hands distance,cps competence,collaboration,process,researcher coded,ecological,machine learning:machine learning
2020,An Application of Extreme Value Theory to Learning Analytics: Predicting Collaboration Outcome from Eye-Tracking Data,eye tracker:microphone:ns,eye gaze:audio:log data,gaze:gaze:gaze:gaze:verbal:verbal:verbal:verbal:verbal:log data,visual attention:visual attention:visual attention:eye physiology:speech features:speech features:speech features:speech features:speech content:task-related,gaze:entropy:similarity:cognitive load:auto-correlation coefficient:energy:shape of envelope:linear predictive coding:dialog:correct/incorrect/hint feedbacks,learning gain,learning,product,pre-post test,lab,correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation:correlation
2020,Collaboration on Procedural Problems May Support Conceptual Knowledge More than You May Think,eye tracker:emotient:headset,eye gaze:video:audio,gaze:verbal:head:log data,visual attention:speech features:facial expressions:task-related,"gaze features (fixation dispersion, number of fixations and mean fixation duration, mean saccade amplitude, joint attention:acoustic-prosodic information (fundamental frequency (pitch:facial features (face area, positive and negative valence, expressivity, face/upper body motion:task content feature",task performance,performance,product,task outcome,lab,AUROC:AUROC:AUROC:AUROC:AUROC:AUROC:AUROC:AUROC
2020,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning",camera:own application,video:log data,log data:body:log data,task-related:gross body motion:task-related,type of activity done in task:amount of face and body movement:target for discussion partner,task performance:perception of collaboration,performance:interpersonal relationship,product:process,researcher coded:self report,lab,correlation:correlation:correlation:correlation:correlation:correlation
2020,A Multimodal Exploration of Engineering Students Emotions and Electrodermal Activity in Design Activities,camera,eye gaze,gaze,visual attention,joint visual attention,learning gain,product,product,pre-post test,lab,correlation
2020,"Multimodal, Multiparty Modeling of Collaborative Problem Solving Performance",eye tracker:ns:own application,eye gaze:audio:log data,gaze:gaze:gaze:verbal,visual attention:visual attention:visual attention:speech participation,individual gaze (transition from image to text:gaze similarity (collaborative gaze:gaze similarity (gaze transition similarity:speech,learning gain:task performance,learning:learning,product:product,pre-post test:task outcome,lab,correlation:correlation:correlation:correlation
2020,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,smart wristband:microphone,eda:audio,physiological:physiological:physiological:physiological:physiological,eda:eda:eda:eda:eda,physiological synchrony (pc:physiological synchrony (da:physiological synchrony (sm:physiological synchrony (idm:cycles of physiological synchrony (pc,collaboration quality:task performance:learning gain,coordination:performance:learning,process:product:product,researcher coded:task outcome:pre-post test,lab,correlation:correlation:correlation:correlation
2020,Using the Tablet Gestures and Speech of Pairs of Students to Classify Their Collaboration,camera,video,body,gross body motion,body synchronization,cooperation:cultural style matching:language style matching:laughter,interpersonal relationship:group composition:group composition:interpersonal relationship,process:condition:condition:process,task outcome:video coding:video coding:video coding,lab,regression:regression:regression:regression
2020,Effects of Shared Gaze on Audio- Versus Text-Based Remote Collaborations,wearable sensor:wearable sensor,eda:ecg,physiological:physiological,eda:heart,eda synchrony:heart rate sychrony,collaboration quality,cognitive engagement:coordination:coordination,process,researcher coded,ecological,sup. machine learning:sup. machine learning
2020,Physiological evidence of interpersonal dynamics in a cooperative production task,electrodes,eda,physiological,eda,physiological synchrony (pc,confidenct:mental effort:task performance:task performance:emotion:group performance:gropu performance,affective:cognitive engagement:cognitive engagement:cognitive engagement:affective:performance:performance,process:process:process:process:process:product:product,questionnaire:questionnaire:questionnaire:questionnaire:questionnaire:questionnaire:performance score,lab,regression:regression:regression:regression:regression:regression:regression
2021,Another person's eye gaze as a cue in solving programming problems,eeg sensor,eeg,physiological,brain,inter-brain synchrony,tasks performance,performance,product,mixed - researcher coded & tests,ecological,correlation
2021,Eye gaze patterns in conversations: there is more to conversational agents than meets the eyes,eeg sensor,eeg,physiological,brain,mediation- anxiety level,learning gain,learning,product,researcher coded,lab,Mann-Whitney U and Kruskal-Wllis tests
2021,"Utilizing Interactive Surfaces to Enhance Learning, Collaboration and Engagement: Insights from Learners’ Gaze and Speech",eye tracker:computer system audio:ns,eye gaze:audio:log data,gaze:gaze:verbal,visual attention:visual attention:speech features,joint visual attention:joint mental effort:dialogue,learning performance,product,product,task outcome,lab,ANOVA:ANOVA
2021,Modelling Collaborative Problem-solving Competence with Transparent Learning Analytics: Is Video Data Enough?,camera:microphone:log data,video:audio:log data,head:verbal:log data:gaze,facial expressions:speech participation:task-related:gaze motion,affective states:speech activity:task- context:gaze behaviors,learning gain,learning,product,pre-post test,lab,Kruskal-Wallis:sup. machine learning:sup. machine learning:sup. machine learning
2022,Speakers’ eye gaze disambiguates referring expressions early during face-to-face conversation,camera,kinesiology,head,head motion,head rotation,peripheral awareness,coordination,process,researcher coded,lab,t-test
2022,Socialphysiological compliance as a determinant of team performance,microphone,audio,verbal,speech features,speech features,coordination,coordination,process,researcher coded,ecological,sup. machine learning
