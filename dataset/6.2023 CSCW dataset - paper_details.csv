ID updated,Paper_id_ new,,paper_id,coder,RQ,main_goal_keyword,main_goal_sentence,task,experimental_conditions,results,sample_size,feedback,study_setting,contributions,motivation,subject_demographics,broader_implications,inference_for_future_practice
1,1,,1,bert,"H1. Individuals are more aware of their own and their partners’ levels of participation when using Reflect. By validating this hypothesis, we would be able to conclude that the information displayed on the table is seen and assimilated into the user’s mental model of the conversation taking place.
H2. Groups that are shown their levels of partici- pation on Reflect are more balanced than those that are not. By validating this hypothesis, we would conclude that the information displayed on the table is used by the participants as a tool to reduce over or underparticipation.",Design,evaluate the effect Reflect has on collaborative work,"In this task, each subject was given a copy of investigation logs that included maps, interviews, and a snippet of a news article. They were asked to accuse one of three suspects of having committed the murder. Each individual version of the investigation logs contained certain important pieces of information that were not available in others. This ensured that all subjects were required to participate in the discussion in order to gather all the necessary informa- tion. This type of task, referred to as a hidden profile task, is often used in experiments involving group decision-making and information pooling","1) the students were shown their levels of participation, i.e., how much time each student talked (speaker- based condition)
2) they were shown the focus of the discussion, i.e., how much time was spent discussing the case of each of the three suspects in the murder mystery (topic-based condition)","Reflect leads to more balanced collaboration, but only under certain conditions",G=18; I=72,real-time,lab,Tool/application,We want something that changes people’s behavior / Need to improve existing practices,University; age unknown; male 44 female 28,NS,"We are thus currently exploring the use of pitch and other prosodic features of the voice in order to attribute to each speaker not only a participation level, but also a manner of participation and possibly even a role. By knowing which members of the group are engaging in interactions that foster learning (rather than which group members are simply speaking) the table might be able to provide more meaningful feedback to the group. "
2,2,,3,bert,"Question 1: task level and task unit level: How does the level of understanding relate to the prevalence of different gaze episodes?
Question 2: task unit level: How do the types of gaze episodes relate to the types of dialogue episodes?
Question 3: task unit level and operation level: How do different dialogue episodes relate to the different gaze transitions?",Model-building,use technology to measure the dynamics of interaction,pair programming comprehension task,NS,We identify four layers of interaction episodes at different time scales. Each layer spans across the whole interaction. The present study concerns the relationship between different layers at different time scales. The first and third layers are based on the utterances of the participants while the second and fourth layers are based on participants' gaze.,G=16; I=32,NS,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,University; aged from 18 to 29 years old,we showed that there is a relationship between gaze and dialogue indicators at different time scales. These relations help us understand the cognition that underlies program comprehension as well as the collaboration that underlies pair programming. The results are interesting enough to pursue further research in the same direction to find the causality between processes at different time scales.,NS
3,3,,4,edwin,"1) How different levels of structural competition were reflected in physiological linkage within dyads
2) and also whether the presence of AI agent had an effect.",Scientific hypothesis testing,"1) To test social presence under different levels of structural competition
2) To test if the presence of AI agent had an effect on social presence","The participants played HEDGEWARS (http://hedgewars.org), an open-source clone of a popular commercial game WORMS by Team 17 (http://worms.team17.com). HEDGEWARS is a turn-based artillery game with a two-dimensional map and ballistic shooting (Figure 1). In a typical multiplayer setting, each player controls a team of several hedgehogs. Teams may also be controlled by AI players, and it is possible to group any teams together so that they are playing on the same side (e.g., two human players against one AI player).",1) full cooperation; 2) competitive cooperation; 3) competition; 4) competition without AI allies,"1) A strong physiological linkage was found within dyads in all condistions, but the linkage scores did not differentiate between conflict modes.
2) The only signifcant difference in linkage between conditions was an increase when the AI agents were not present.",G=50; I=100,NS,lab,Model/metric; Theory,Modelling Collaborative Problem-solving Competence with Transparent Learning Analytics: Is Video Data Enough?,University; age 22.9; age 18-32; male 58 female 42,"On a more general level, the association of empathy and social presence related self-reports and physiological linkage in the results give support to the theory of embodied cognition, mimicry, and emotional contagion. These results contribute not only to games research in the form of basic understanding on the effects of different game modes and presence of AI agents, but also the development of physiological linkage as measure for social interaction and presence.","These results contribute not only to games research in the form of basic understanding on the effects of different game modes and presence of AI agents, but also the development of physiological linkage as measure for social interaction and presence."
4,4,,5,iulian,Can multimodal sensors be used to analyze group meetings unobtrusively?,Model-building,"To propose a novel method for head pose and VFOA estimation in group meetings using ceiling-mounted, downward-facing Kinects.",Lunar survival task,NS,Predict group leaders and major contributors with 90% and 100& accuracy respectively using automatically extracted metrics,G=10; I=36,NS,lab,Algorithm; Model/Metric,We need [better] way of measuring / detecting / predicting something,age 18-29,"..an active automated meeting facilitator could aid in group decision making in real time. It could mediate the meeting by subtly reminding participants to talk more or less based on the real-time estimated speaking balance in the room. The room could additionally detect productivity shifts due to inattentiveness or lack of coordination and instigate a new line of thought. Further- more, it could keep the meeting on track by correlating the agenda and the discussion schedule in real time.","With regard to the ToF sensing modality, we want to modify our VFOA estimation algorithm to include supervised learning techniques with more contextual cues, which should improve the accuracy. We also want to implement more sophisticated supervised learning techniques for predicting leadership scores and styles of leadership by combining other metrics involving coarse body pose extracted from the lower-resolution ToF sensors, and body and head activities, as discussed in [9]."
5,5,,6,bert,"(1) How do individuals in a group engage in monitoring their learning process during a collaborative exam situation?
(2) How does individual students’ monitoring frequency relate to physiological arousal?
(3) How does monitoring occur in situations when there is and is not physiological synchrony between the students?""",Model-building,The aim of this study is to understand how metacognitive monitoring occurs during a collaborative exam situation by examining how individual student contributions to monitoring processes are related to physiological arousal and physiological synchrony in groups,The data consist of video recordings from collaborative exam sessions lasting 90 minutes and physiological data captured from each student with Empatica 4.0 sensors,NS,"""Separate multiple regression analyses revealed that the task completion time was predicted by coherence measures for EDA and heart, but only at a trend level for breathing.
Task completion time was also predicted by heart cross correlation.
Team tracking error was predicted by coherence measures for EDA, heart and breathing, and also heart cross correlation.
While socialvisual contact did not have an impact, physiological compliance was predictive of improved performance, with coherence robust over all three physiological measures.
Heart cross correlation showed the strongest predictive relationships.""",G=4; I=12,NS,ecological setting,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,High school; age 19-20; male 23 female 8,We fully expect that measures of social physiological compliance may someday provide the field of human factors with an objective means to study complex sociotechnical systems.,NS
6,6,,10,steph,"RQ1: How to characterize and extract typical speaking and looking patterns
RQ2: To understand how group behavior patterns relate to how group members perform and perceive themselves and other members of their group.",Model-building,This paper addresses the task of mining typical behavioral patterns from small group face-to-face interactions and linking them to social-psychological group variables. ,Winter survival task: rank twelve items important for surviving a airplane crash in harsh winter conditions.,NS,"Our results show that both group behavior cues and topics have significant correlations with (and predictive information for) group variables such as group composition, group interpersonal perception, and group performance.","G=18, I=72",NS,lab,Model/metric ,We need a [better] way of measuring / detecting / predicting something,University; age 25.4,"The mining framework is powerful as it potentially allows to work with large datasets, make statistical inferences, and validate the results on datasets for which ground-truth is available.",NS
7,7,,11,edwin,"How different categories of laughter can be differentiated from audiovisual features, and to which extent they might convey different emotions (arousal and valence)",Model-building,To investigate automatic recognition of emotional laughter from spontaneous multimodal data,Winter survival task: rank twelve items important for surviving a airplane crash in harsh winter conditions.,NS,"Results show that voiced laughter performed best in the automatic recognition of arousal and valence for both audio and visual features. The context of production is further analysed and results show that, acted and spontaneous expressions of laughter produced by a same person can be differentiated from audiovisual signals, and multilingual induced expressions can be differentiated from those produced during interactions.",G=53; I=106,NS,lab,Algorithm; Dataset,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,NS,Future work will investigate how variabilities in the language and culture might impact performance on the automatic recognition of laughter.,NS
8,8,,12,bert,can linguistic features and speech features predict how well a group will perform on a well-defined task?,Model-building,"automatically predicting group performance on a task, using multimodal features derived from the group conversation","Winter Survival Task, role-playing a fictional scenario (designing and marketing a remote control device)",NS,"the best-performing models utilize both linguistic and acoustic features, and that linguistic features alone can also yield good performance on this task",G=28; I=?,NS,lab,Algorithm,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,NS,Being able to predict task performance based on the group interaction could be very valuable for providing feedback to collaborative teams in an online fashion.,NS
9,9,,14,callie,"RQ1. How does the priming affect learning? We want to see if there is a priming effect on the learning gain of the participants .
RQ2. How are the individual gaze patterns during the video related to the collaborative gaze patterns during the collaborative concept map phase?
RQ3.How are the individual and collaborative gaze patterns related to learning gain?",Observe learning; Metric-construct connection,"In this article we present an empirical study that sheds some light on the gaze features of the MOOC learners and the effect of priming the students in two different ways. As we will see, the priming method impacts the learning gain of the students; and the gaze features we propose are efficient enough to highlight the differences between the good MOOC learners from the poor ones.","Collaborative concept map, based on video lecture",1) textual priming 2) schema priming,What comes out of the present study is two different interaction styles that differentiate the good students from the poor students. The good students engage with the teacher/collaborating partner through the interface/display. While the poor students engage with the material only.,G=49; I=98,NS,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,University,The concepts of “looking through” and “looking at” could be seen as new interaction style categories.,NS
10,10,,15,edwin,"RQ1. Do the gaze behaviors of collaborating dyads during reference action sequences reflect a particular progression of gaze coordination?
RQ2. How does gaze coordination change in the different phases of a reference-action sequence?
RQ3. Do gaze behavior patterns differ in reference-action sequences that include breakdowns or repairs?",Model-building,"The goals of this study were (a) to explore patterns and progressions in the gaze coordination exhibited by dyads engaged in a simple instructional task, and (b) to develop and test a method for constructing detailed and nuanced models of gaze coordination.","Each dyad was assigned a sandwich-building task: one participant made verbal references to visible ingredients they would like added to their sandwich, while the other participant assembled those ingredients into a sandwich.",NS,Our analyses indicate: (a) properties and patterns of how gaze coordination unfolds throughout an interaction sequence; and (b) differences in gaze coordination patterns for interaction sequences that lead to breakdowns and repairs.,G=13; I=26,NS,lab,Algorithm,We need a [better] way of measuring / detecting / predicting something,University; age 22.9; age 18-32; male 58 female 42,"Such research could improve understanding of learning in collaborative interactions involving reference- action sequences, such as one-on-one tutoring or cooperative problem solving among students, and it could enable us to design learning technologies that more effectively promote and achieve coordination with human users.",NS
11,11,,17,steph,Can multimodal feature sets be used to predict whole-session retrospective self-reports of affect and learning gain within human-human tutoring ?,Model-building,This paper reports on the first study to investigate how multimodal feature sets can be used to predict whole-session retrospective self-reports of affect and learning gain within human-human tutoring,program in Java,NS,"The complete trimodal feature set was most predictive of each of the three tutoring outcomes, and bimodal features with dialogue were most predictive of each tutoring outcome. Importantly, the findings demonstrate that the role of nonverbal behavior may depend on the dialogue and task context in which it occurs",G=67; I=67,NS,ecological setting,Model/metric,We need a [better] way of measuring / detecting / predicting something,University; age 18.5,Future adaptive multimodal interfaces may leverage this detailed task-contextualized information to disambiguate affect and intervene effectively,NS
12,12,,18,steph,"RQ1: What are the different forms of gestural engagement in the context of a paired, hands-on learning experience?
RQ2: What are effective learning practices in the engineering design context?",Model-building,"The current paper builds on these ideas by taking a deeper look at different forms of gestural engagement in the context of a paired, hands-on learning experience. Furthermore, it leverages a combination of unsupervised and supervised machine learning algorithms to help draw inferences about effective learning practices from the engineering design context. ","Two engineering design tasks: 1) first task asked students to use one sheet of printer paper to construct a structure that could support one or more engineering textbooks at least three inches above a table 2)  In the second task students had 10 minutes to use one paper plate, two feet of tape, three wooden sticks and four straws to build a structure that could support a mass of 0.5 lb. as high off the table as possible",NS,"The combination of machine learning and human inference helps elucidate the practices that seem to correlate with student learning. In particular, both engagement and disengagement seem to correlate with student learning, albeit in a somewhat nuanced fashion","G=27, I=54",NS,lab,Algorithm,We need a [better] way of measuring / detecting / predicting something,University,"I argue that an intermediate model that leverages the affordances of multimodal data and computation, but leaves inference development to trained scholars could offer a viable alternative to purely qualitative or machine learning approaches.",NS
13,13,,19,bert,"Question 1: task level and task unit level: How does the level of understanding relate to the prevalence of different gaze episodes?
Question 2: task unit level: How do the types of gaze episodes relate to the types of dialogue episodes?
Question 3: task unit level and operation level: How do different dialogue episodes relate to the different gaze transitions?",Metric-construct connection,Validate new measure using gaze data across different temporal scales; describe gaze behavior of low- and high-understanding dyads in collaborative learning task,"read, understand, and explain functionality of a JAVA program.",NS,"There is a relationship between gaze (joint focus or not), dialogue indicators (task management, code description) at different time scales and participants' understanding of the program",G=16; I=32,NS,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,University; age 18-29; gender distribution unknownn,NS,NS
14,14,,20,edwin,How does having access to partner's eye gaze affect quality of collaboration and learning gains?,Design,"Our goal is to develop new ways of supporting the establishment of perceptual joint attention (as opposed to cognitive, or social joint attention).","During the first 12 minutes, dyads worked on 5 contrasting cases in neuroscience. They had to collaboratively explain how visual information is processed by the human brain based on what they have learned from the models described in Figure 1. They then read a text on the same topic for 12 minutes. Finally, they answered a learning test with questions on the terminology used, concepts taught and questions in which they needed to transfer their knowledge to a new situation.","1) visible gaze
2) no gaze",Results indicate that this real-time mutual gaze perception intervention helped students achieve a higher quality of collaboration and a higher learning gain.,G=21; I=42,real-time,lab,Tool/application,We want something that changes people’s behavior / Need to improve existing practices,Unversity; age mean 23; male 14 female 28,"These results provide strong evidence for the important contributions of real-time mutual gaze perception—a special form of technology-mediated shared attention—to the learning gains and collaboration quality of collaborative learning groups. Additional qualitative observations suggest that our intervention helped students on four dimensions: by supporting coordination, creating conventions, sharing cognition and by making knowledge-building a collective process rather than an individual one.",NS
15,15,,21,edwin,"The data mining task we set out to solve is to discover sequences of interactions between group members and the data slips at the tabletop that were more frequent in high-achieving groups than in low-achieving ones, and vice-versa.",Model-building,This paper introduces a data mining approach that exploits the log traces of a problem-solving tabletop application to extract patterns of activity in order to shed light on the strategies followed by groups of learners.,"The task provided to the students is to solve a mystery with an open question in any subject such as mathematics, history, or physics. Students are given the question and a number of data slips which may hold direct clues for solving the mystery, background information, or even red-herrings. They are asked to analyse these to formulate their answer to the question.",NS,"We found that high achievers favoured the strategy of reading, minimising and arranging immediately (cluster 1 mean = 124.75, cluster 2 mean = 61.25). On the contrary, low achievers used both strategies for the information gathering, performing more actions contained in Cluster 2 in which they did not close the slips immediately after reading (cluster 1 mean = 104.40, cluster 2 mean = 114.80).",G=6; I=18,NS,lab,Algorithm,We need a [better] way of measuring / detecting / predicting something,Elementary school; age 11-14,The goal of this line of research is to offer adapted support to groups in the form of direct feedback to students or to their facilitator. The insights obtained in the work reported in this paper are the first steps towards such adapted support that machine learning techniques can offer to the use of tabletop devices.,An important goal of our work is to mirror useful information about groups to help facilitators and the students themselves to reflect on and improve their learning activity.
16,16,,22,edwin,"RQ1: Are there ways to characterize the effect of our intervention on students’ discourse?
RQ2: Is it possible to find markers of productive learning trajectories?
RQ3: Is it possible to find markers of constructive collaborations?",Model-building,The goals of this paper are to: 1) explore a variety of computational techniques for analyzing the transcripts of students’ discussions; 2) examine whether any of those measures sheds new light on our previous results; and 3) test whether those metrics have any predictive power regarding learning outcomes.,"The structure of the activity was as follows: in the first step, students analyzed brain diagrams (12 minutes); in a second step, they were asked to read a textbook chapter about human vision and discuss their understanding of this topic (12 minutes).","1) visible gaze
2) no gaze","Using various natural language processing algorithms, we found that linguistic coordination (i.e., the extent to which students mimic each other in terms of their grammatical structure) did not predict the quality of student collaboration or learning gains. However, we found that the coherence of students’ discourse was significantly different across our experimental conditions; this measure was positively correlated with their learning gains. Finally, using various language metrics, we were able to roughly (i.e., using a mediansplit) predict learning gains with a 94.4% accuracy using Support Vector Machine. The accuracy dropped to 75% when we used our model on a validation set.",G=21; I=42,real-time,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,Unversity; age mean 23; male 14 female 28,"This paper showed NLP approaches offer substantial promise for understanding educational datasets and automating currently unwieldy and time-consuming hand analyses. The measures described above could easily be applied to other settings, such as forums or online discussions.",NS
17,17,,24,edwin,"RQ1: To use dual eye-tracking data to understand how well-functioning dyads of students interact
RQ2: To use cross-recurrence graphs to guide qualitative analyses
RQ3: A foray into studying the variety of ways that students use to maintain high levels of joint visual attention.",Model-building,The goal of this paper is to illustrate how well-coordinated groups establish and sustain joint visual attention by unpacking their different strategies and behaviors.,"The system used in this study, the TinkerLamp, is shown on Figure 2: it features small-scale shelves that students can manipulate to design a warehouse layout. A camera detects their location using fiducial markers and a projector enhances the simulation with an augmented reality layer. The activity lasted an hour and the goal for students was to uncover good design principles for organizing warehouses. The core of this reflection involves understanding the trade-off between the amount of merchandise that can be stored in a warehouse and how quickly one can bring an item to a loading dock (i.e., a lot of shelves make it difficult to efficiently load/unload items, while few shelves limit the size of available stock).","1) 3D condition
2) 2D condition","We found that highly coordinated dyads (as measured by dual eye-trackers) were not necessarily the best learning groups. Augmented cross-recurrence graphs revealed imbalances in students’ verbal contributions, which are characteristic of the free-rider effect where one student does all the work while his/her partner stays passive.",G=27; I=54,NS,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,Unversity; age mean 18.54,The implications of this work are that cross-recurrence graphs are highly valuable for distinguishing between productive and unproductive groups. But they should ideally be complemented with spatial and verbal information to provide a more refined representation of a group’s interactions.,NS
18,18,,25,edwin,How can expertise be estimated from simple multimodal features?,Model-building,The result of this processing is a set of simple features that could discriminate between experts and non-experts in groups of students solving mathematical problems.,"In each session, a group of 3 students worked together to solve a set of mathematics problems, each of which be- longed to a different difficulty level: easy, moderate, hard and very hard. The students of each group met and worked twice, in two separate sessions, to solve two distinct sets of problems. In each of these sessions, one of the students was assigned as the leader of the group in order to interact, on behalf of the other members, with a computer system that displayed the students the problems to solve and received the answers submitted.",NS,"The main finding is that several of those simple features, namely the percentage of time that the students use the calculator, the speed at which the student writes or draws and the percentage of time that the student mentions numbers or mathematical terms, are good discriminators between experts and non-experts students. Precision levels of 63% are obtained for individual problems and up to 80% when full sessions (aggregation of 16 problems) are analyzed.",G=12; I=18,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,High school,One important conclusion for the field of multimodal learning analytics is that good predictors were found in each one of the media analyzed.,NS
19,19,,26,edwin,How can a co-occurrent event mining framework from multiparty and multimodal interaction data be used to infer personality traits?,Model-building,"The goal is to find patterns between modalities and multiple people: segments of utterance, speech, gaze, head gestures and body gestures.",Winter survival task: rank twelve items important for surviving a airplane crash in harsh winter conditions.,NS,"Experimental results show that the model trained with co-occurrence features obtained higher accuracy than previously related work in 8 out of 10 traits. In addition, the cooccurrence features improve the accuracy from 2% up to 17%.",G=27; I=102,NS,lab,Model/metric; Algorithm,We need a [better] way of measuring / detecting / predicting something,NS,"Our feature representation captures the interplay between the nonverbal behavior of an individual and her/his interactions, and can be used for feature extraction for other types of conversation (e.g. dyadic interaction).",NS
20,20,,27,edwin,Whether expertise in mathematics can be detected by analyzing students’ rate and type of manual gestures?,Scientific hypothesis testing,The aim and unique contribution of the present research are to investigate whether rate and type of gestures are predictive of students’ domain expertise in mathematics.,"During each session, 16 geometry and algebra problems were presented, four apiece representing easy, moderate, hard, and very hard difficulty levels.",NS,"The results reveal several unique findings, including that math experts reduced their total rate of gesturing by 50%, compared with non-experts. They also dynamically increased their rate of gesturing on harder problems. Although experts reduced their rate of gesturing overall, they selectively produced 62% more iconic gestures. Iconic gestures are strategic because they assist with retaining spatial information in working memory, so that inferences can be extracted to support correct problem solving.",G=12; I=18,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,High school; age 15-17; male 9 female 9,"The present results on representation-level gesture patterns are convergent with recent findings on signal-level handwriting, while also contributing a causal understanding of how and why experts adapt their manual activity during problem solving.","The long-term aim and societal contribution of this research is to recognize naturally occurring multimodal behaviors and communication patterns that can be used to identify domain experts, to predict an individual’s level of domain expertise, and to understand why they are critical predictors."
21,21,,28,edwin,"RQ1: Can we distinguish more collaborative from less collaborative groups by the interwoven stream of students’ verbal and physical participation?
RQ2: Can we distinguish more collaborative from less collaborative groups by extracting patterns of interaction based on just students’ verbal participation?
RQ3: Can we distinguish more collaborative from less collaborative groups based on patterns involving traces of interaction of students with others’ objects?
RQ4: Can we distinguish more collaborative from less collaborative groups in terms of the actions that follow up the access to others’ knowledge structures?",Model-building,"In this paper, we explored the potential of an enriched tabletop to automatically and unobtrusively capture data from collaborative interactions.",Concept mapping learning activity,NS,"We discovered that the less collaborative groups had a predomination of patterns with physical interactions, high levels of physical concurrency and greater parallelism than the more collaborative groups. By contrast, the more collaborative groups had more verbal discussions in conjunction with physical actions, especially in the brainstorming phase. They also showed less concurrency in the physical dimension and less parallelism. One of the most interesting findings for the less collaborative groups was the detection of patterns where a learner spoke briefly without getting response from other students. We found that the more collaborative groups had some interaction with others’ objects in the brainstorming phase but in the linking phase, the less collaborative groups interacted more with others’ objects.",G=20; I=60,NS,lab,Model/metric; Tool/Application,We need a [better] way of measuring / detecting / predicting something,NS,The contribution of this paper is an approach for analyzing students’ interactions around an enriched interactive tabletop that is validated through an empirical study that shows its operationalization to extract frequent patterns of collaborative activity.,"We envisage that our work provides a foundation for creating a system with three components:
1) the data capture system to track and gather data of group activity;
2) the data analytics component, which is based on careful design of the alphabets that are a basis for producing group indicators via statistical and data mining techniques; and
3) the data presentation component that aims to present to teachers, researchers or students with visual information or knowledge about the collaborative process but which implementation goes beyond the scope of this paper."
22,22,,32,callie,"1) identifying the domain expert among groups of three students working cooperatively on mathematical problem solving tasks and
2) inferring, from group interaction during a problem solving task, whether the group answered the question correctly or incorrectly.",Model-building,To see whether simple non-verbal speech interaction features can predict expertise and performance.,"Algebra and geometry problems (2 separate sessions per group). Each session consisted of 16 problem solving tasks, in which the students worked cooperatively and mentored one another.",NS,"The proposed Bayesian approach to expert prediction proved quite effective, reaching accuracy levels of over 92% with as few as 6 dialogues of training data. Performance prediction was not quite as effective.",G=6; I=18,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,age 15-17,NS,NS
23,23,,33,callie,What measures from TUI logs and kinect sensors can best predict students' learning gains?,Model-building,"Our goal with this paper is twofold: first, we introduced methods to compute meaningful measures from logs generated by a tangible interface and Kinect data; second, we correlated those measures with students’ learning gains to find relevant predictors of learning.",EarExplorer:  Students are asked to connect the tangibles between the starting point and the ending point to let sound waves reach the auditory cortex,1) discover; 2) listen,"First, we showed that information retrieval techniques could be used on the system’s logs to predict learning gains as measured by pre and post-tests. Second, we explored how Kinect data can inform the way we understand “in-situ” interactions around a tabletop: we found that the raw amount of movement was not a relevant predictor for our purposes; however, we found that bimanual coordination was predictive of students’ leadership in a group. We found that we were able to predict students’ learning gains (i.e., being above or belong the median split) with very high accuracy (using all measures)",G=19; I=38,NS,lab,Model/metric; Algorithm,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,age mean 22.5; University,Implement learning algorithm to capture data as students are working on a task and make just-in-time predictions minute by minute,NS
24,24,,39,bert,"– RQ1: Are measures of physiological synchrony (PC, DA, IDM, SM) significantly correlated with collaboration quality, task performance and learning gains?
– RQ2a: By looking at line graphs of physiological synchrony, can we relate events of interest to peaks (sharp increase), oscillations (jolt) and valleys (sharp decrease)?
– RQ2b: Do these observations vary between a high-performing and a low-performing group?
– RQ3: Based on RQ2a and RQ2b, can we define new measures of physiological synchro- ny – i.e., are cycles between low and high synchronization related to our three outcome measures?",Metric-construct connection; Observe learning,electrodermal activity (EDA) was used to identify markers of productive collaboration,Programming a robot to solve a series of mazes,Speech awareness tool vs no awareness tool,PC to be positively associated with learning gains (r = 0.35) and DA with collaboration quality (r = 0.3). We computed the number of cycles between low and high synchronization. We found this measure to be significantly correlated with collaboration quality (r = 0.57) and learning gains (r = 0.47). ,G=42; I=84,real-time,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X; We need a [better] way of measuring / detecting / predicting something,University pool; age 19-51,"Those results are encouraging, especially in the context of developing real-time, just-in- time, personalized feedback to students. There is some preliminary evidence that high- frequency data can indeed improve collaboration",We can imagine leveraging these measures to develop dashboards for teachers and aware- ness tools for students (Buder 2011) – which has not been explored for physiological data.
25,25,,40,edwin,"1) Do teams exhibit multimodal regularity?
2) How do individual measures constitute regularity?
3) How do features of the task affect regularity?
4) How does regularity change over time?
5) Regularity as a predictor of team performance",Model-building,Our goal is to understand multimodal patterns that emerge and their relation with collaborative outcomes.,"We used Physics Playground for our problem solving environment. Game levels require participants to guide a green ball to a red balloon by drawing simple machines (i.e., ramps, levers, pendulums, and springboards) using the mouse.",NS,"We found that teams exhibit significant regularity above chance baselines. Regularity was unaffected by task factors. but had a quadratic relationship with session time in that it initially increased but then decreased as the session progressed. Importantly, teams that produce more varied behavioral patterns (irregularity) reported higher emotional valence and performed better on a subset of the problem solving tasks. Regularity did not predict arousal or subjective perceptions of the collaboration.","G=101, I=303",NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,Unversity; age 22; male 44 female 56,Measures of regularity could be used to aid machine learning models in predicting collaborative processes and outcomes.,"Our findings could be used to inform intelligent systems that monitor the quality of ongoing interactions and intervene when necessary. In particular, in the context of virtual interaction, regularity of low-level behaviors could be monitored in real-time in video conferencing software."
26,26,,41,bert,"1. JVA is associated with higher quality of collaboration; more specifically, JVA is associated with participants’ ability to sustain mutual understanding (Schneider & Pea, 2013).
2. The number of cycles of individual work (no-JVA) and collaborative interactions (JVA) is positively associated with the three outcome measures (collaboration, task performance, learning gains).",Metric-construct connection; Observe learning,"This paper provides three contributions: 1) I use an emerging methodology to capture joint visual attention in a co-located setting using mobile eye-trackers (Schneider & al., 2018); 2) I then replicate findings showing that levels of joint visual attention are positively correlated with collaboration quality; 3) finally, I present a new measure that captures cycles of collaborative / individual work, which is positively associated with learning gains (but not with collaboration quality).",Programming a robot to solve a series of mazes,Speech awareness tool vs no awareness tool,"joint visual attention is positively correlated with collaboration quality
measure that captures cycles of collaborative / individual work, which is positively associated with learning gains (but not with collaboration quality).",G=42; I=84,NS,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X; We need a [better] way of measuring / detecting / predicting something,University pool; age 19-51,"The final and main contribution of this paper is a new measure that captures cycles of collaboration and individual work in dyads. This measure provides a complementary lens into collaborative processes: I found JVA to be positively associated with collaboration quality, and this new measure with learning gains (as well as task performance and collaboration quality, depending on the threshold used). This suggests that an important feature of successful collaborative learning groups is to balance individual cognition with group work. ","this study shows that it is possible to develop new ways of capturing 21st century skills in hands- on tasks typical of makerspaces. Even with the limitations mentioned above, this work makes a first step in this direction and opens the way to more rigorously studying collaborative processes in open-ended learning environments using dual mobile eye-trackers."
27,27,,42,steph,"1. Can you predict the social context of players from their eye-movement data?
2. How much are these predictions affected by missing gaze data, is it possible to improve the predictions by taking into account the quality level? ",Model-building,We investigate the automatic detection (or recognition) of pair composition using dual gaze-based as well as action-based multimodal features.,Tetris,"1) Novice-novice
2) Novice-expert
3) Expert-novice
4) Expert-expert","We have first shown that it is possible to differentiate four
difference social contexts given gaze and action features.","G=59, I=118",NS,lab,Algorithm,We need a [better] way of measuring / detecting / predicting something,NS,The long term goal of our work is to design adaptive gaze awareness tools that take the pair composition into account.,NS
28,28,,43,callie,"1. Does acoustic-prosodic entrainment exist in in human-human collaborative learning dialogues?
2. Can acoustic-prosodic entrainment can help us model the interaction between the two students by detecting the presence or absence of rapport?",Metric-construct connection,"The first goal is to identify whether there is evidence of acoustic-prosodic entrainment in human-human collaborative learning dialogues. The second goal, given that acoustic-prosodic entrainment is present, is to investigate whether acoustic-prosodic entrainment can help us model the interaction between the two students by detecting the presence or absence of rapport, one social factor of the interaction.",Math questions via the formative Assessment with Computation Technologies (FACT) application,NS,"Wefind that entrainment does correlate to rapport; speakers appear to entrain primarily by matching their prosody on a turn-by-turn basis, and pitch is the most signicant acoustic-prosodic feature people entrain on when rapport is present.",G=8; I=16,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,University; basic knowledge of algebra and geometry,,The results here suggest that increased focus on the prosodic features of pitch and voice quality may present an opportunity for further understanding and building an emotional connection with a virtual agent.; we can build systems which can support and provide interventions when we detect entrainment or a lack of entrainment on these features.
29,29,,44,edwin,How remote collaboration by a worker and a remote collaborator will change if the collaborator’s eye fixations on a physical workspace are presented to the worker.,Design;Scientific hypothesis testing,"In particular, we are interested in how systems can facilitate remote collaboration when remote collaborators wish to manifest their intentions or instructions, such as what objects they are currently focusing on and how they want a worker to manipulate these objects.","Complete an assembly task by using the following steps. First, one of the participants in each pair assigned the role of a remote collaborator saw the as-built drawings of a target object. Then, the collaborator instructed the other participant (i.e., a worker) on how to assemble the target object by using blocks. Each participant took turns playing the collaborator and the worker for each condition, and the order of conditions was randomized to maintain the counterbalance (i.e., each pair conducted four sessions in total).","1) Gesture condition where a collaborator was allowed to use hand gestures and speech
2) Gesture + Eye condition where eye fixations are also available for the collaborator","1) Eye fixations can serve as a fast and precise pointer to objects of the collaborator’s interest. 2) Eyes and other modalities, such as hand gestures and speech, are used differently for object identification and manipulation. 3) Eyes are used for explicit instructions only when they are combined with speech. 4) The worker can predict some intentions of the collaborator such as his/her current interest and next instruction.",G=4; I=8,NS,lab,Tool/application,We want something that changes people’s behavior / Need to improve existing practices,University; age 20-30,"This application involves several important topics in CSCW, such as efficient monitoring and provision of instructions to multiple workers, long-term assistance with remote collaboration, and heterogeneous monitoring of workspaces by using smartphones as well as fixed POV and wearable cameras.",NS
30,30,,46,bert,"— H1: students will better memorize a warehouse’s layout if it is presented as a 3D tangible model.
— H2: when asked to build a warehouse, students will better be able to optimize its layout when using tangible shelves compared to paper ones.
— H3: students will learn more from the task when using tangible shelves.",Metric-construct connection; Observe learning,"The hypothesis behind our study is that tangible interfaces have unique perceptual affordances. We argue that providing a realistic representation of a problem can help students better understand and analyze a complex system. When using 2D paper shelves, we remove the “realistic” aspect of the system while keeping all other parameters mostly unchanged. This allowed us to study the impact of interacting with a small-scale warehouse versus interacting with an augmented diagram",Designing and optimize warehouse layouts,"Participants interacted with
1) 2D paper shelves
2) 3D physical shelves","We found that participants in the first group (i.e. who used 3D realistic shelves) better memorized a warehouse layout, built a more efficient model, and scored higher on a learning test. Additionally, students wore eye-tracking goggles while completing those tasks; preliminary results suggest that 3D interfaces increased joint visual attention, which was found to be a significant predictor for participants’ task performance and learning gains.","G=27, I=54",NS,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X; We need a [better] way of measuring / detecting / predicting something,Apprentices in a vocational training program; age mean 19.07,"our results also open the door to the development of new systems in the realm of vocational training education. There is a wide breath of spatial domains where TUIs could help students more easily grasp complex concepts, especially at the introductory level.",NS
31,31,,47,edwin,Recognizing and understanding social dimensions (the personality traits and social impressions) during small group interactions.,Model-building,Our research interest focuses on the prediction of personality traits and social impressions of participants during small group interactions.,Winter survival task: rank twelve items important for surviving a airplane crash in harsh winter conditions.,NS,Our experiments show that the combination of intra-personal and one vs all features can greatly improve the prediction accuracy of personality traits and social impressions. Prediction accuracy reaches 81.37% for the social impression named 'Rank of Dominance'.,"G=27, I=102",NS,lab,Model/metric; Algorithm,We need [better] way of measuring / detecting / predicting something,NS,"There are three main contributions in this paper. Firstly, we propose a new approach to analyse the relationships between social behaviour features and personality traits/social impressions by using a new feature categorisation, which claries the study of the influence of intra-personal and interpersonal features during social interaction. Secondly, by analysing and interpreting the prediction results, we draw interesting conclusions about the relationships between personality traits/social impressions and related social features Finally, we show that our novel categorisation of features improves the prediction accuracy of personality traits and social impressions.",NS
32,32,,48,bert,visual dominance ratio (VDR) can estimate dominance in multi-party group discussions where natural verbal exchanges occur ,Model-building,"we investigate a model for visually dominant be- haviour, grounded in findings from social psychology, using fully automatic audio and visual cues to assess whether the most dominant person can be reliably detected when there are varying degrees of agreement in human perceptions of dominance.",We used meeting data from the publicly available AMI meeting corpus [4] ,NS,Our findings suggest that fully au- tomated versions of these measures can estimate effectively the most dominant person in a meeting and can approxi- mate the dominance estimation performance when manual labels of visual attention are used.,G=91; I=?,NS,lab,Model/metric,"Understanding these constructs (e.g., dominance) in automatic systems is desirable for tasks such as for improving task-oriented group effectiveness or remote meeting scenarios ",NS,NS,NS
33,33,,49,steph,Can superficial measures of speech and user interactions of students measure collaboration? ,Model-building,We tested whether superficial measures of speech and user interactions of students would suffice for measuring collaboration.,mathematical task: Positioning a set of cards so that all cards in a row describe the same process,NS,"Using just the speech and tablet log data, several detectors were machine learned and achieved an overall accuracy of 96% (Kappa=0.92), which is higher than earlier attempts to use speech and log data for detecting collaboration",G=14; I=28,NS,lab,Model/metric,We need a [better] way of measuring /detecting / predicting something,University students,NS,NS
34,34,,50,edwin,Can interlocutor-modulated attention BLSTM (IM-aBLSTM) be used to predict personality traits in the ELEA corpus?,Model-building,"In this work, we present a network architecture of interlocutor-modulated attention BLSTM (IM-aBLSTM) that captures both the target speaker’s self vocal behaviors and his/her interactive behaviors with other group members jointly to improve the prediction of the target speaker’s personality traits.",Winter survival task: rank twelve items important for surviving a airplane crash in harsh winter conditions.,NS,"Our framework achieves a promising unweighted recall accuracy of 87.9% in ten different binary personality trait prediction tasks, which outperforms the best results previously reported on the same database by 10.4% absolute. Finally, by analyzing the interpersonal vocal behaviors in the region of high attention weights, we observe several distinct intra- and inter-personal vocal behavior patterns that vary as a function of personality traits.",G=28; I=112,NS,lab,Algorithm,We need a [better] way of measuring /detecting / predicting something,NS,This work presents a preliminary personality prediction result by modeling vocal behaviors between interlocutors via embedding interaction-based attention mechanism in a BLSTM.,"Our future work will focus on leveraging multiple behavior modalities to advance our algorithm in modeling the relationship between target speaker and interlocutors to improve regression correlations across these ten personality traits. Additionally, we would also investigate algorithmic frameworks that jointly models individual behaviors at the group level to predict the final group performance outcome as they engage in a variety of small group interaction contexts."
35,35,,52,steph,"How does group-level gaze-UI regularity, defined as the dynamics of teams’ collective attention aligned with changes in the UI, predict collaborative problem-solving processes and outcomes after accounting for individual-level gaze coupling with the UI and other covariates (e.g. verbosity)?","Design, model-building",We investigate teams’ multidimensional patterns of visual attention during a collaborative problemsolving task with an eye for leveraging insights to improve collaborative interfaces.,Minecraft hour of code,1) controlling screen 2) watching screen,"We quantified not only how a team member individually coupled his or her attention with UI changes, but also repeated patterns in the triad’s attention in context with UI changes.",G=37 I=111,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,University students,Our work is novel by providing a computational assessment of complex computer-mediated collaboration from the lens of triadic visual attention,The next step is to leverage these findings to develop intelligent collaborative user interfaces that monitor the unfolding collaboration and launching interventions to improve the collaborative process and outcomes.
36,36,,53,bert,The goals of this paper are 1) to explore a variety of computational techniques for analyzing the transcripts of student discussions; 2) to examine whether any of those measures sheds new light on our previous results; and 3) to test whether those metrics have any predictive power regarding learning outcomes. ,Observe learning,explore a variety of computational techniques for analyzing the transcripts of student discussions,Contrasting cases in neuroscience,"1) visible gaze
2) no gaze","we found that linguistic coordination (i.e., the extent to which students mimic each other in terms of their grammatical structure) did not predict the quality of student collaboration or learning gains. However, we found that a simple computational measure of student verbal coherence (i.e., the extent to which students build on each other’s ideas) was positively correlated with their learning gains. Additionally, this measure was significantly different across our experimental conditions: students who could see the gaze of their partner in real time were more likely to develop a coherent discussion. Finally, using various language metrics, we were able to roughly predict (i.e., using a median-split) learning gains with a 94.4% accuracy using Support Vector Machine. The accuracy dropped to 75% when we used our model on a validation set.",G=21; I=42,real-time,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,College students; average age 23,"By combining NLP measures with various kinds of sensors (eye-trackers, motion sensors, Galvanic Skin Response sensors) we can start to unpack a complex taxonomy of productive collaborative learning markers. The overarching goal is to then use those markers to construct “learning states” and map student trajectories using those states. One could then differentiate productive from unproductive trajectories and intervene to redirect students from the latter state to the former one.","we believe that the field of learning analytics (and more specifically our work) can contribute to education in several ways. First, finding positive predictors of learning can help us unpack student learning trajectories by detecting unproductive patterns; this information, in turn, can provide interesting feedback loops for both students and teachers to avoid dead-ends in their learning paths. Second, high-frequency sensors (such as eye-trackers) can provide an additional layer of complexity to nuance simple measures of learning: in this paper, we showed that combining JVA with coherence had the potential for uncovering productive and off-task behaviours. Finally, computational measures offer the prospect of speeding up the pace of educational research by automatically extracting constructs of interest: instead of painstakingly annotating hours of videos and reams of transcripts, we can start to graph the evolution of particular behaviours and use those graphs to isolate interesting learning moments. "
37,37,,54,steph,"Can an analysis of speaking activity features, visual attention features, and multimodal features that rely on the audiovisual synchrony be used to estimate an emergent leader?",Model-building,"In this work, we report the performance in the emergent leader inference using social attention automatically extracted from audio-visual features (e.g. looking at participants while speaking) and using a subset of the ELEA corpus. Additionally, we describe the annotations collected from external observers and we present the perception of the emergent leader from the external observers’ point of view",Survival task,NS,"Although the multimodal features are not the best descriptor of leadership, nor of dominance, they provide some information about the perceived leadership during the interaction, such that being the center of attention while speaking correlates with being perceived as the leader","G=19, I=85",NS,lab,"Dataset, Model/metric",We need a [better] way of measuring / detecting / predicting something,University; age mean 23.1; male 54 female 31,"As future work, the effect of other interesting automatically extracted features, including floor patterns and emotional states on estimating the leader in the group can be investigated.",NS
38,38,,56,bert,"RQ1: Are Coh-Metrix indices derived from transcripts of discourse between co-located partners related to the quality of their ollaboration and learning gains?
RQ2: Are Coh-Metrix indices different across experimental conditions?
RQ3: Are Coh-Metrix indices associated with MMLA measures (e.g., Joint visual attention, physiological synchrony, nonverbal behaviors) that were previously significantly correlated with collaboration quality?
RQ4: What Coh-Metrix indices are most meaningful for estimating a group’s collaboration?
RQ5: Can Coh-Metrix indices be used to train supervised machine learning algorithms to predict collaboration quality?",Observe learning,"Analyzing this discourse can give insights into how consensus is reached and can estimate the depth of their understanding of the problem. This study uses Coh-Metrix, a natural language processing tool that measures cohesion, to analyze participant discourse",novice programmers collaborated to use a block-based programming language to instruct a robot on how to solve a series of mazes,Speech awareness tool vs no awareness tool,"We significantly correlated thirty-five Coh-Metrix indices from the transcripts of dyads' discourse with collaboration, learning gains, and multimodal sensor values. We then fit a variety of machine learning classifiers to predict collaboration using the indices generated by Coh-Metrix as features. ",G=40; I=80,real-time,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,College students,"This research paves the way for real-time detection of (un)productive interactions from multimodal data, potentially facilitating the development of fail-soft real-time interventions to support collaborative learning. ",NS
39,39,,57,callie,"Does task/technology conditions (specifically, task demand and technology reliability) affects the level of group PC?
Is PC of a group is positively related to the performance of the group after controlling for effects of task/technology conditions?
Is the PC of a group related to how the passive user estimates the active user’s workload?
Is the PC of a group related to the shared perception about the trustworthiness of the technology among the group members.",Scientific hypothesis testing; Metric-construct connection,"This study aimed to understand the process of joint activity, using PC measures, in technologically complex environments that include active and passive users. This study also investigated how PC relates to task performance and the subjective experience of the group.",psycho-motor tasks in a modified version of the Multiattribute Task Battery (MATB) program,"1) normal condition
2) hard condition
3) low reliability condition",Results indicate that PC is related to group performance after controlling for task/technology conditions. PC is also correlated with shared perceptions of trust in technology among group members.,G=24; I=48,NS,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,College,This study has implications for understanding effective collaboration.,"Technology could play a supportive role in this regard by supporting passive users’ sensitivity to active users’ workload, such as when a teammate is deciding whether or not to interrupt another teammate"
40,40,,58,edwin,Which features of MMLA are good predictors of collaborative problem-solving in open-ended tasks in project-based learning?,Design,"One of the aims of the project is to develop learning analytics tools for hands- on, open-ended STEM and STEAM learning activities using physical computing.",Prototype an interactive toy,NS,"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning analytics system to be effectively used to identify collaboration in small groups of Engineering students.",G=12; I=36,NS,lab,Tool/application,We want something that changes people’s behavior / Need to improve existing practices,College; age mean 20; male 17 female 1,These results are significant for the CSCL community as a starting point to investigate further what features of MMLA can be used to support collaborative learning providing insights about the physical and embodied processes involved in hands-on learning and how.,NS
41,41,,60,edwin,Can the quality of groups’ collaborative learning processes be detected using touch data alone?,Metric-construct connection,"In this paper, we present a follow-up study conducted in a classroom field setting with high school students to find out if these touch interaction patterns continue to reflect collaborative learning processes beyond the original lab setting.",To design and build a website around a topic of their choosing,"1) Microsoft PixelSense SDK
2) Web browser-based",We found that two of the touch patterns in combination—Unrelated Touches and a modified version of Overlapping Sequences—were clearly associated with quality of collaboration in up to 84.2% of cases.,G=13; I=16,NS,ecological,Model/metric,We need a [better] way of measuring / detecting / predicting something,High School; female 10 male 6,NS,"As well as helping groups of students to improve their collaboration processes, the approach to detecting collaboration quality described in this paper could be used to help teachers to monitor small group work in their classroom."
42,42,,61,steph,"Do visualizations of participant participation influence critical constructs captured by traditional and multi-modal measurement techniques such as collaboration quality, task understanding and learning gains?",Scientific Hypothesis Testing,We explored 2 interventions to support their collaboration: a real-time visualization of their verbal contribution and a short verbal explanation of the benefits of collaboration for learning. ,Use block-based programming to navigate a simple robot through a series of mazes,"1) speech equity visualization (SEV) + informational collaboration intervention (ICI)
2) SEV
3) ICI
4) none","Analysis of researcher’s coding of participants’ collaboration quality found that while collaboration quality improved for both groups that received the informational intervention, there was no significant improvement of collaboration quality for the groups that received the visualization intervention and no significant advantage conferred to the group that experienced both interventions.",G=42; I=84,real-time,lab,Tool/application,We want something that changes people’s behavior / Need to improve existing practices,College and members of community; age mean 26.7; female 60%,"We collected a rich multi-modal dataset that can be used to build proxies for measuring effective collaborations. As a preliminary analysis, we found that various indicators captured by the Kinect sensor were correlated with participants’ quality of collaboration (e.g., amount of talking and movements)",NS
43,43,,62,steph,"How can the extreme values (i.e., the moments with extremely high or extremely low values) of eye-tracking data during collaobration inform us of the students’ learning gains?",Model-building,To propose two types of shifts from the traditional analyses of how visual focus of peers is related to collaborative outcomes. ,"1) collaborative concept-map building activity
2) problem-solving activity around fractions using a networked collaborative ITS",NS,In both datasets we found that students with lower outcomes had lower focus during the collaborative session,"1) G=24; I=48
2) G=14; I=28",NS,lab,Algorithm,We need a [better] way of measuring / detecting / predicting something,"1) NS
2) 4th and 5th grade",This paper contributes to the CSCL literature by providing an alternate method for analysing relationships between process and outcome data that complements existing methodology while also extending our understanding of the relationship between visual focus and collaborative outcomes.,NS
44,44,,63,iulian,"1) How do phase of working and type of interaction relate to simultaneous occurrence of arousal among group members? 2) What types of facial expression (positive, negative, or neutral) are observed when arousal occurs simultaneously among group members? 3) How does regulation of learning appear during interaction when arousal occurs simultaneously among group members?",Model-building,"They ""investigate whether physiological sensors in combination with facial expression recognition data can shed light on the quality of social interaction during collaborative learning""","“Design a perfect breakfast for a
marathon runner.”",NS,"The results show that simultaneous arousal episodes occurred throughout phases of collaborative learning and the learners presented the most negative facial expressions during the simultaneous arousal episodes. Most of the collaborative interaction during simultaneous arousal was low-level, and regulated learning was not observable. However, when the interaction was high-level, markers of regulated learning were present; when the interaction was confused, it included monitoring activities. This",G=16; I=48,NS,NS,Model/metric,We need a [better] way of measuring / detecting / predicting something,high school students; 27 females 21 males; (Mean age=17.4 years; SD=0.67),NS,"This study demonstrates that new technologies such as facial recognition software and physiological signals can be used unobtrusively in collaborative learning contexts to capture episodes that may reveal regulated learning or the need to activate it. In the future, this type of multichannel data should be used for validating contextual data, and students’ own interpretations of simultaneous arousal episodes would provide valuable insights for data interpretation"
45,45,,64,iulian,Can we develop prediction models of outputting the performance for multiple group meeting tasks that do not have a clear correct answer ?,Model-building,"This paper proposes an approach to develop models for predicting the performance for multiple group meeting tasks, where the model has no clear correct answer","Different types of planning tasks, part of MATRICS corpus",NS,The results find that a support vector regression model archived a 0.76 correlation in the discussion-task-dependent setting and 0.55 in the task-independent setting,G=30; I=40,NS,lab,Algorithm,We need a [better] way of measuring / detecting / predicting something,NS (see MATRICS corpus),NS,We need a [better] way of measuring / detecting / predicting something
46,46,,66,bert,"Does dual eye-tracking data, modeled with network analyses techniques, predict collaborative learning?",Model-building,applications of network analysis techniques to eye-tracking  data collected during collaborative learning activities to predict collaboration,analyzing contrasting cases,"1) visible gaze
2) no visible gaze","We found that among the eight dimensions of collaboration that we considered, 20 we were able to roughly predict (using a median-split) students’ quality of collaboration with 21 an accuracy between ~85 and 100 %. ",G=16; I=32,real-time,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,"College students; age mean 23; 28 females, 14 males","our graph visualizations help other researchers gain further insights into their own datasets. We believe that network visualizations can advantageously complement existing plots and graphs for initial data exploration, and that various social settings could benefit from the visualization developed in this paper (e.g., parent-infant interactions, diplomatic negotiations, psychotherapeutic dialogues, brainstorming sessions, or sales activities).","In formal learning environments, such measures could be computed in real time; teachers could employ such metrics of ‘collaboration sensing’ to target specific interventions while students are at work on a task. In informal networked learning, collaboration sensor metrics could trigger hints or provide other scaffolds for guiding collaborators to more productive coordination of their attention and action."
47,47,,69,steph,Which features of students' group work that can be automatically collected with our MMLA system are good predictors of students' project outcomes in open-ended learning activities with physical computing?,Observe learning,"In this paper, we investigated how MMLA data can be used to support project-based learning from a specially designed worktable environment, where small groups of students use new physical computing components to solve open-ended tasks.","3 open-ended design task:
1: prototype of an interactive toy
2: prototype a color sorting machine
3: build an autonomous automobile",NS,"The results illustrate that state-of-the-art computational techniques can be used to generate insights into the ""black box"" of learning in students' project-based activities. The features identified from the analysis show that distance between learners' hands and faces is a strong predictor of students' artefact quality, which can indicate the value of student collaboration.",G=6; I=18,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,Engineering University students ,"The work presented here is a significant contribution towards developing techniques to automatically identify the key aspects of students success in project-based learning environments, and to ultimately help teachers provide appropriate and timely support to students in these fundamental aspects. ",the techniques and types of data we presented here can be the first step towards the evidence-informed and effective implementation and evaluation of project-based learning at a scale.
48,48,,70,steph,"Can prosodic, voice quality and writing-based features be used to determine socially dominant and experts in a group? ",Model-building,"In this study, we investigate low level predictors from audio and writing modalities for the separation and identification of socially dominant leaders and experts within a study group.",Geometry and algebra problems,"1) Assigned leader
2) Regular participant",we found a number of significant predictors in the prosodic domain for the separation of the assigned leaders from other students and less predictors for the separation of experts and other students,"G=6, I=18",NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,high school students ,The automatic prediction then can be used to improve the objective evaluation of group learning tasks and optimize individualized student specific learning strategies.,NS
49,49,,71,steph,Which features of student group work are good predictors of team success in open-ended tasks with physical computing?,Model-building,The aim of this paper is to investigate how Multimodal Learning Analytics (MMLA) can be used to support project-based learning from a specially designed worktable environment where small groups of students (three students) use new physical computing components to solve open-ended tasks.,open ended task with physical computing,NS,We found that the planning and building stages of students learning activities are better predictors of their success than the reflection stage. Our results show that the Distance between Hands DBH hands and Distance between Learners DBL are key features to predict students’ performances in practice-based learning activities.,"G=6, I=18",NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,university students; age mean 20; female 1 male 17,NS,NS
50,50,,73,steph,"1. Is JVA associated with higher quality of collaboration; more specifically, is JVA associated with participants’ ability to sustain mutual understanding?
2. Is the number of cycles of individual work (no-JVA) and collaborative interactions (JVA) positively associated with the three outcome measures (collaboration, task performance, learning gains)?",Scientific hypothesis testing; Metric-construct connection,This paper is about going beyond capturing JVA and finding more precise indicators of collaborative learning.,program a robot to autonomously solve a series of increasingly complex mazes,"1) no intervention
2) visualization intervention
3) information intervention
4) visualization + information intervention",JVA is positively correlated with high quality collaborative interactions and cycles of collaborative/individual work,G=42; I=84,real-time,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,age mean 26.7; female 60%,"this study shows that it is possible to develop new ways of capturing 21st century skills in hands-on tasks typical of makerspaces. Even with the limitations mentioned above, this work makes a first step in this direction and opens the way to more rigorously studying collaborative processes in open-ended learning environments using dual mobile eye-trackers.",NS
51,51,,74,steph,What is the effect of tangibles on students' visual coordination? ,Metric-construct connection,The goal of this paper is to close this gap by providing highly granular data on users’ visual coordination around a TUI.,optimize the layout of a warehouse on a TUI,"1) working with 2D paper shelves
2) working with 3D tangible shelves","We found that 3D tangibles seemed to facilitate visual coordination between users, which in turn was associated with higher performances on an optimization task.","G=27, I=54",NS,lab; ecological,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,age mean: 18,we believe that the methodology and results described above are promising building blocks for studying visual coordination (and more generally collaborative learning) in small groups of students.,NS
52,52,,75,steph,Can we measure the quality of collaboration by analyzing participant movement and correlating a variety of measures with task performance and a coding scheme for assessing collaboration quality in dyads?,Model-building,We explore how unsupervised machine learning algorithms can find prototypical states from dyads of students when learning to program a robot.,program a robot to autonomously solve a series of increasingly complex mazes,"1) no intervention
2) visualization intervention
3) information intervention
4) visualization + information intervention""",We first show how certain movements and patterns of gestures correlate positively with collaboration and learning gains. We next use clustering algorithms to find prototypical body positions of participants and relate amount of time spent in certain postures with learning gains,G=42; I=84,real-time,lab,Model/metric ,We need a [better] way of measuring / detecting / predicting something,age mean 26.7; female 60%,"We found predictors for those dependent measures in a naturalistic, open-ended task that routinely takes place in makerspaces and engineering courses. While there are limitations to this work, our contribution paves the way to rich multimodal analyses of students’ collaboration. It also unlocks new opportunities to design innovative interventions to support social interactions in small groups",NS
53,53,,78,iulian,Does real-time mutual gaze15perception intervention help students achieve a higher quality of collaboration and higher learning gains ?,Design,See,"explore the effects of brain function, using contrasting cases","1) see other's gaze
2) don't see other's gaze",Real-time mutual gaze perception intervention helped students achieve a higher quality of collaboration and a higher learning gain.,"G=24, I=42",real-time,lab,Tool/application,We want something that changes people’s behavior / Need to improve existing practices,age mean 23; college students,"We argue that the use of eye-trackers is highly relevant for studying700collaborative learning, because it provides the opportunity to speed up the research process by701quickly computing metrics of interest and by providing real-time feedback to learners and702teachers.","We hypothesize that our688intervention may lead to similar benefits for students working on an interactive surface (as while689wearing eye-tracking goggles). Finally, our results have further implications for teachers’690practices; with training, we posit that gaze-awareness tools could teach students the value of691achieving joint attention in collaborative groups."
54,54,,80,iulian,1. Which PCI reflects better the collaborative will? 2.  Which PCI predicts the collaborative learning productand the dual learning gain the best,Metric-construct connection,"More  is  needed  to  knowabout socio-emotional and cognitive interaction pro-cesses, and new methodological solutions may offer what theearlier have not achieved. The measurement of collaborationin general is a challenge as such, as there are not enough in-dicators available so far",design nutritious breakfast,NS,"Regression analy-ses showed that out of the five PCIs, IDM related the mostto CW and was the best predictor of the CLP. Meanwhile,DA predicted DLG the best.","G=16, I=46",NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,age 16-19,"As  biosensors  are  becoming  more  and  more  common  ineveryday life,  we believe that they can increase the acces-sibility  to  a  data  source  with  potential  to  enrich  the  LAfield.   They  can  be  used  as  input  for  a  learning-orientedbiofeedback dashboard in a computer supported collabora-tive learning (CSCL) context. ",Further studies on the applications of EDA to the learn-ing  sciences  should  consider  the  laterality  effect.   Lateralmeasures  need  to  be  compared  and  correlated  to  learning.
55,55,,82,iulian,What is the relationship between the nonverbal and verbal features and three measures of successful interaction?,Metric-construct connection,We created an experiment to mimic complex voice search tasks in order to understand how affective signals can be used to improve information retrieval.,information seeking tasks,NS,"We found that those who were expressive in channels that were missing from the communication channel (e.g., facial actions and gaze) were rated as communicating poorly, being less helpful and understanding. Having a way of reinstating nonverbal cues into these interactions would improve the experience, even when the tasks are purely information seeking exercises.","G=22, I=44",NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,age 24-65; 24 females; 20 males,"Designing successful agents that interpret and express emotion probably requires visual cues, in addition to audio cues, even if the interactions are by voice alone.","Agents that lack these abilities are likely to be rated as less helpful in addition to being poorer communicators. However, testing this with an artificial agent remains as future work."
56,56,,83,iulian,"RQ1:  What  behavioral  patterns  (in  terms  of  the  team’s interaction, speech, and body movement)  emerge  during remote collaborative problem solving?•RQ2: How do these patterns predict subjective and objective outcomes? •RQ3:  What  is  the  advantage  of  multimodal  patterns  over unimodal primitives?",Model-building,we aim   to identifyinterpretable  patterns  in  teams’  verbal  and  nonverbal  behaviors that correlate with meaningful outcomes during remote CPS. ,collaborative programming,NS,"We  found  that idling with limited  speech  (i.e., silence or  backchannel  feedback  only) and without    movement    was    negatively    correlated    with    task performance and with participants’  subjective perceptions of the collaboration. However, being silent and focused during solution execution   was   positively   correlated   with task performance. Results   illustrate that in   some   cases,   multimodal   patterns improved the predictions and improved explanatory power overthe unimodal primitives.","G=116, I=348",NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,students age mean 20,Our   work provides   in-detailed   insights   on   interaction patterns and behaviors observed   in   the   large-scaled   datasets and has implications for the design of real-time interventions.,"uture  research  should  include  factors  such  as the team’s  demographic  composition,  personality  differences,  prior knowledge  effects,  and task-related  aspects  (i.e.,  establishing common   grounds,   setting   goals,   or   getting familiar with teammates) in order to study the incremental predictive validity of the patterns over these more stable factors. "
57,57,,84,iulian,Do physiological measures of synchrony relate to learning gains and collaboration? Does telling participants to balance their collaboration actually influence changes in physiological synchrony?,Metric-construct connection,"The goal of this project is to explore how various kinds of sensors, such as eye-trackers, motion  sensors, galvanic skin response wristbands, can capture proxies of 21st  skills during learning activities. ",robot programming,2x2 with two variables 1] presence of visual indication of speech balance;  2] presence of instruction about speech balance benefits on collaboration,Detected some correlations between physiological synchrony indicators and learning/collaboration,"G=42, I=84",real-time,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,age mean 26.7; female 60%,NS,"In terms of future work, we want to consider the more specific characteristics of EDA: tonic versus phasic changes."
58,58,,89,callie,"i) Can the Pose All perform better than existing visual activity based nonverbal features (NFs) (Head Body Act) in predicting emerging leaders (EL)?
ii) Can unsupervised feature learning (DBM) contribute to improve the most and the least EL identication performances of Pose All? If yes, is it performing better than pre-processed Head Body Act?
iii) Can Pose All contribute to improve the most and the least EL detection performances of already well-performing NFs, i.e. V FOA and Speak Act? If yes, is its contribution better than Head Body Act's?",Model-building,"In this work, the main question asked is if the proposed pose-based nonverbal features perform better than existing Visual Act based nonverbal features for the identication of the most and the least emerging leaders.",Survival task,NS,"Overall, when feature subsets were used, the pose-based NFs were not signicantly better or worse than other Visual Act based NFs, although Head Act was particularly good at the least EL detection.",G=16; I=64,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,44 females; 20 males; unacquainted; same gender within group,NS,NS
59,59,,90,callie,Can emergent leadership be automatically detected with non-verbal visual features derived from video?,Metric-construct connection,We propose novel nonverbal visual features which are extracted from head pose of the people in a meeting environment to detect emergent leadership.,Survival task,NS,The proposed nonverbal features performed well for detection of the most and the least ELs (70% of detection rate in average) when the majority of the dened nonverbal features were highly correlated with the results of the social psychology questionnaires. ,"G=16, I=64",NS,lab,Model/metric; Dataset,We need a [better] way of measuring / detecting / predicting something,44 females; 20 males; unacquainted; same gender within group,"in absence of audio sensors, the accurate detection of social interactions is still crucial.",NS
60,60,,92,callie,"can we distinguish high from low collaboration groups by identifying patterns of interaction, based on their interwoven verbal and touch actions?",Model-building,"We design an automatic approach to distinguish, discover and distil patterns of interaction that can be associated with groups’ strategies.",learn and draw concept map on what should be included in a balanced diet,NS,We demonstrate that our approach can be used to discover patterns that may be associated with strategies that differentiate high and low collaboration groups.,G=20; I=60,NS,lab,Model/metric ,We need a [better] way of measuring / detecting / predicting something,NS,NS,Our approach can serve as a basis for the implementation of a system that can automatically and unobtrusively capture verbal and physical activity at the tabletop in order to alert teachers of possible issues in small-groups activities.
61,61,,93,callie,"Is it possible to automatically infer whether a group of learners is engaged in a collaborative situation, from the application and audio traces of interaction with a reasonable level of accuracy?",Model-building,The goal of our work is to explore ways to exploit readily available data to determine the level and nature of collaboration.,"Job Shop Scheduling (JSS) task, where six jobs and resources must be scheduled and optimised",NS,"Results show up to 69.4% accuracy (depending on the classifier) and that the error rate for extreme misclassification (e.g. when a collaborative episode is classified as non-collaborative, or vice-versa) is less than 7.6%.",G=13; I=39,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,age 18-27; college students,We argue that this technique can be used to show the teacher and the learners an overview of the extent of their collaboration so they can become aware of it.,NS
62,62,,94,callie,"RQ1: Do the instances of collaborative interactions (as detected by Kinect sensors) provide meaningful and accurate information about students’ performance in the makerspace?
RQ2: Do the diversity of collaborative interactions (as detected by Kinect sensors) provide meaningful and accurate information about students’ performance in the makerspace?
RQ3: What can the transitions between instances of interactions inform instructors about student learning experiences in the makerspace?",Observe learning; Metric-construct connection,the goal of this paper is to examine the instances and diversity of student collaborative interactions within makerspaces using Kinect sensors.,Natural makerspace interactions,NS,"Findings indicate that letting students work on their own promotes the development of technical skills, while working together encourages students to spend more time in the makerspace.",G=1; I=16,NS,ecological,Model/metric ,We need a [better] understanding of how people behave / We need a deeper understanding of construct X; We need a [better] way of measuring / detecting / predicting something,graduate students,"As such, this paper aims to provide instructional support in makerspaces through the examination of the instances and diversity of student collaborative interactions using motion sensors.",findings suggest that multimodal sensors have a role to play in aiding instructors in harnessing the full potential of makerspaces and represent initial steps towards the development of a semi-automated teacher dashboard to provide instructional support for makerspaces.
63,63,,96,bert,"(RQ1) Is there any relationship between PS and metacognitive experiences (ie, estimates of effort, judgement of confidence, task interest, task difficulty and emotions)?
(RQ2) Is there any relationship between PS and perceived group performance?
(RQ3) Is there a relationship between PS and objective group performance?",Metric-construct connection,is physiological synchrony (PS) correlated with  Collaborative Problem Solving (CPS). ,Tailorshop simulation,NS,"The results show a positive relationship between continuous PS episodes and groups’ collective mental effort. No relationship was found between PS and judgement of confidence, task interest, task difficulty or emotional valence. The relationship between PS and group performance was also non- significant. ",G=19; I=57,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,higher education students (Mage = 27.8; SDAge = 5.43; ffemal e = 43; fmale = 34),"multimodal data analytics offer new paths to observe and quantify the temporal and situated nature of metacognitive experiences. Further, multi- modal data analytics might help to develop objective metrics to assess group CPS performance.",NS
64,64,,97,bert,"i) that pairs of people from the same team (true dyads) have higher synchrony than pairs of people taken from dif- ferent teams (pseudo dyads);
ii) that synchrony is positively correlated with the perceived quality of cooperation and liking.",Scientific hypothesis testing; Model-building,we investigate the coordination dynamics of multiple psychophysiological measures and their utility in capturing emotional dynamics in teams.,Origami building task,"1) positive/active in which participants were induced by a trained experimenter acting positive (valence) and active (arousal)
2)  negative/inactive in which the ex- perimenter acted negative (valence) and inactive (arousal)","• Members of newly formed teams synchronize on physiological measures.
• Synchrony of smiling is correlated with team cohesion.
• Synchrony of skin conductance is correlated with group tension and negative affect.
• Synchrony of smiling may be a predictor of a team's decision to adopt a new routine. • Synchrony decreases after a team's decision to adopt a new routine.",G=51; I=153,NS,lab,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,"university students; age from 18 to 58 years old (μ = 24.0, σ = 4.5) ","These new results hold promise that team emo- tions as interpersonal synchrony can shed light on the emotional pro- cesses occurring in teams as they perform their tasks. However, in order to establish such a dynamical view on team emotions as a comple- ment to more established approaches further studies using synchrony measures are needed.",NS
65,65,,99,bert,"We explored the hypothesis that synchronized neural activity across a group of students predicts (and possibly underpins) classroom engagement and social dynamics. When students feel connected or engaged with the material or each other, are their brains in fact ‘‘in sync’’ in a formal, quantifiable sense? ",Scientific hypothesis testing; Model-building,"Yet we know so little about how it supports dynamic group interactions that the study of real-world social exchanges has been dubbed the ‘‘dark matter of social neuroscience’’ [2]. Recently, various studies have begun to approach this question by comparing brain responses of multiple individuals during a vari- ety of (semi-naturalistic) tasks",Natural classroom interactions,NS,"
Students’ brain-to-brain group synchrony predicts classroom engagement
Students’ brain-to-brain group synchrony predicts classroom social dynamics",G=1; I=12,NS,ecological,Model/metric,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,"high school students (9 females and 3 males, age 17-18)","These findings suggest that brain-to-brain synchrony is a sensitive marker that can predict dynamic classroom interactions, and this relationship may be driven by shared attention within the group. ",NS
66,66,,101,bert,Can Multi-modal Social Signal Analysis Predict Agreement in Conversation Settings?,Model-building,we present a non-invasive ambient intelligence framework for the analysis of non-verbal communication ap- plied to conversational settings,victim-offender meditation scenario,NS,"Using different state-of-the-art classification approaches, our system achieve upon 75% of recognition predicting agree- ment among the parts involved in the conversations, using as ground truth the experts opinions.",G=26; I=52,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,N/A,NS,NS
67,67,,103,bert,Can physiological synchrnoy predict collaboration quality in a naturalistic setting? ,Scientific hypothesis testing; Metric-construct connection,"The aim of the present study is to explore potentials of using physiological synchrony to measure collaboration quality in real educational settings, based on consistently identified synchrony during interpersonal interaction by previous studies",Problem-solving task,NS,"The result showed that during groups discussions, high collaboration pairs produced significantly higher synchrony than low collaboration dyads (p = 0.010).",G=16; I=30,NS,ecological,Model/metric,We need a [better] way of measuring / detecting / predicting something,"college students (M = 21.61, SD = 2.43, 8 females)",Our findings provide evidence for the potential application of biosensors in the real-world classroom. We focus on the connection between the bio-signals and human behaviors on which we believe is the advantage of this interdisciplinary research area. This project also suggests that future researches in the same realm place attention to the scope of appropriate assumptions and research questions so that the laboratory-based experiments and naturalistic setting studies can be good complement for each other.,NS
68,68,,104,bert,"Visualizing the collaborator’s point of gaze on a shared screen has been explored as a promising way to alleviate some of these limitations by increasing shared awareness. However, prior studies on shared gaze have not considered the medium of communication and have only studied its effect on audio.",Design,"Remote collaborations are becoming ubiquitous, but, despite their many advantages, face unique challenges compared to collocated collaborations",Murder mysteries,"1) voice
2) voice + gaze
3) text
4) text + gaze","We find that for text, shared gaze improved task correctness and led collaborators to look at and talk more about shared content. Similar trends are found for gaze-augmented voice communication, but contrary to the slower performance in text, it also saw improvements in completion time as well as in cognitive workload",G=24; I=48,real-time,lab,Tool/application,We want something that changes people’s behavior / Need to improve existing practices,"undergraduate students; 28 females, 19 males, 1 non-binary; between 18 and 24 years old","Ultimately, developers of novel gaze-sharing tools and interaction techniques should take into account the capabilities of the communication medium to better support remote collaborators.",NS
69,69,,105,bert,"Do triads show group synchrony, reflecting a general average affiliation among its three members? Or do triads show distinct patterns of synchrony in their component dyads, potentially reflecting mutual affiliation only among those pairs? Does increased bodily synchrony between a pair predict that they will form an alliance in future interactions? ",Scientific hypothesis testing; Model-building,Does triadic interaction exist compared to random pairings?,"natural conversation
Prisoner's dilemna",NS,"We correlated this measure of synchrony with a diverse set of covariates related to the outcome of interactions. Triads showed higher maximum cross-correlation relative to a surrogate baseline, and ‘meta-synchrony’, in that composite dyads in a triad tended to show correlated structure.",G=35; I=105,NS,lab,Model/metric ,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,undegraduate students; N/A,NS,NS
70,70,,174,iulian,"Can collaboration quality be predicted from nonverbal featuers such as log data and prosodic audio featuers, to such a strong degree as human raters ?",Model-building,"it is a worthwhile endeavor to develop technologies that are able to automatically assess the quality of collaboration and in this way provide the teacher with information to improve their ability to guide, assist and scaffold student groups. This",Interpreting and sorting math distance/time graphs,NS,"When this project began, we did not think the induced detectors would be accurate because they used only lowlevel features that do not understand what the participants are saying nor what plans and goals the task involves. Against these low expectations, the results were surprisingly good, with accuracies between 87 and 96 percent",G=14; I=28,NS,lab,Model/metric,We need a [better] way of measuring / detecting / predicting something,graduate and undergraduate students,"If this method works, then it may become practical to equip a variety of software intended to be used collaborative with detectors which can be used in real world learning environments. ...  It is a worthwhile endeavor to develop technologies that are able to automatically assess the quality of collaboration and in this way provide the teacher with information to improve their ability to guide, assist and scaffold student groups",NS
71,71,,181,iulian,"Does remote lag affect collaborator interactions? Does it help them mitigate invisibility problems?
Do remote lags improve conversational efficiency by reducing unnecessary
questions/confirmations redundant instructions?
Do remote lags impose a greater mental load (e.g., effort, frustration) on users to gestures?",Design,"Collaborative distributed tabletop activities involving real objects are complicated by invisibility factors introduced into the workspace. In this paper, we propose a technique called “remote lag” to alleviate the problems caused by the invisibility of remote gestures.",LEGO layout in space,"1) No visualization of remote gestures
2) Visualization","Our results show that remote lags effectively alleviated the invisibility problems, resulting in fewer questions/confirmations and redundant instructions during collaboration. The technique also decreased the overall workload of workers as well as the temporal demands for both helpers and workers",G=20; I=80,real-time,lab,Tool/application,We want something that changes people’s behavior / Need to improve existing practices,"NS, but all females",Our next step is to explore the improvements raised here and carry out the technique in actual use.,NS
72,72,,182,iulian,Can speaking time and social gaze predict extravertedness? is classification time accuracy affected by size of time slices? does social context influence accuracy of prediction?,Model-building,"In order to predict the Extraversion personality trait, we exploit medium-grained behaviors enacted in group meetings, namely, speaking time and social attention (social gaze).",survival,NS,"The results of our work confirm many of our hypotheses: a) speaking time and (some forms of) social gaze are effective in automatically predicting Extraversion; b) classification accuracy is affected by the size of the time slices used for analysis, and c) to a large extent, the consideration of the social context does not add much to accuracy prediction, with an important exception concerning social gaze.",G=12; I=48,NS,lab,Model/metric,We need a [better] way of measuring /detecting / predicting something,NS,NS,NS
73,73,,503,steph,"What is the interaction between referential form, gaze, and spatial context in a collaborative naturalistic conversation elicitation task?  ",Observe learning,"This study aims to explore the interplay of gaze coordination, spatial context, and linguistic detail and form in the process of collaborative reference.",Naturalistic conversational elicitation task,1) seated side-by-side 2) seated across 3) mobile,"Our results detail measurable interactions between referential form, gaze, and spatial context and can be used to enable the development of more natural collaborative user interfaces.",G=33; I=66,NS,lab,Tool/application,We need a [better] understanding of how people behave / We need a deeper understanding of construct X,College,"Importantly, this study demonstrates the need for designers and builders of collaborative systems to acknowledge the systematic differences in collaborative reference between mobile and stationary users.","Probabilistic models based on findings like the ones in this paper could be used to assess topical shifts or attention shifts. Techniques such as these could help intelligent user interfaces to infer discourse focus, in turn helping systems to better track the status of a task or provide context-appropriate information. "
74,74,,506,iulian,Which/do audio features predict collaboration quality ?,Model-building,.,Solving math problems,NS,"Results reveal that both speech activity features and prosodic features are good predictors of collaboration quality, while their combination by means of fusion can considerably improve their collaboration prediction performance",G=47; I=141,NS,ecological,Model/metric,We need a [better] way of measuring / detecting / predicting something,"Middle school (6,7,8th grade US); age 11-13","The ultimate goal of the project is to produce knowledge about the feasibility of speech analytics and the creation of adaptive software that could help teachers
by identifying groups that need feedback in real time, as well as by helping teachers to better target their interventions.",NS
75,75,,,,"RQ1: What are the different forms of gestural engagement in the context of a paired, hands-on learning experience?
RQ2: What are effective learning practices in the engineering design context?",,"The current paper builds on these ideas by taking a deeper look at different forms of gestural engagement in the context of a paired, hands-on learning experience. Furthermore, it leverages a combination of unsupervised and supervised machine learning algorithms to help draw inferences about effective learning practices from the engineering design context. ","Two engineering design tasks: 1) first task asked students to use one sheet of printer paper to construct a structure that could support one or more engineering textbooks at least three inches above a table 2)  In the second task students had 10 minutes to use one paper plate, two feet of tape, three wooden sticks and four straws to build a structure that could support a mass of 0.5 lb. as high off the table as possible","The combination of machine learning and human inference helps elucidate the practices that seem to correlate with student learning. In particular, both engagement and disengagement seem to correlate with student learning, albeit in a somewhat nuanced fashion",,"G=27, I=54",,lab,,,University,,
76,76,,,,What measures from TUI logs and kinect sensors can best predict students' learning gains?,,"Our goal with this paper is twofold: first, we introduced methods to compute meaningful measures from logs generated by a tangible interface and Kinect data; second, we correlated those measures with students’ learning gains to find relevant predictors of learning.",EarExplorer:  Students are asked to connect the tangibles between the starting point and the ending point to let sound waves reach the auditory cortex,1) discover; 2) listen,"First, we showed that information retrieval techniques could be used on the system’s logs to predict learning gains as measured by pre and post-tests. Second, we explored how Kinect data can inform the way we understand “in-situ” interactions around a tabletop: we found that the raw amount of movement was not a relevant predictor for our purposes; however, we found that bimanual coordination was predictive of students’ leadership in a group. We found that we were able to predict students’ learning gains (i.e., being above or belong the median split) with very high accuracy (using all measures)",G=19; I=38,,lab,,,age mean 22.5; University,,
78,78,,,,"RQ1: Are there ways to characterize the effect of our intervention on student discourse?
RQ2: Is it possible to find markers of productive learning trajectories?
RQ3: Is it possible to find markers of constructive collaborations?",,"The goals ofthis paper are
1) to explore a variety of computational techniques for analyzing the transcripts ofstudent discussions;
2) to examine whether any of those measures sheds new light on ourprevious results; and
3) to test whether those metrics have any predictive power regardinglearning outcomes.",,"1) visible-gaze, students can see the gaze of their partner
2) no-gaze","eye-tracking data, Natural Language processing","G=22, 20, I=42",,lab,,,University students; age mean 23.0,University students; age mean 23.0,University students; age mean 23.0
79,79,,,,Does real-time mutual gaze perception intervention help students achieve a higher quality of collaboration and higher learning gains ?,,"88 freshmen engineering students completed online pretopic and posttopic emotions surveys for five engineering design activities. A subset of 14–18 participants, the focal point of this study, wore an EDA sensor while completing the surveys and participating in these sessions.","This course introduces students to the 3-Dcomputer-aided design (CAD) software, SolidEdge, through a series of five engineeringdesign method workshop topics created by an engineering instructor, allowing students toapply their 3-D CAD skills in the context of an engineering design problem.
",,"Preliminary findings suggest that EDA increased for individual and collaborative active learning activities compared to lectures. No significant changes in EDA were found between individual and collaborative active learning activities. Moderate negative correlations were found between EDA and negative topic emotions in the first engineering design activity but not across the rest. At the end of the semester, active learning activities showed higher effect sizes indicating a re-enforcement of students’ engagement in the engineering design method activities.",I=88,,lab,,,University students,University students,age mean 23; college students
80,80,,,,"1. How did the eye tracking measures (JVA and JME) relate to student performance?
2. How do JVA and JME relate to other indicators of collaboration, such as dialogue content and division of labor, and how do the interactions with student performance associate with JVA and JME?",,"Students were assigned to pairs based on their teachers’ pairings, and each pair was randomly assigned to one of four conditions, by crossing two factors; collaborative or individual instruction and problems geared towards conceptual knowledge or procedural knowledge.","Individual and collaborative versions of an ITS (Intelligent Tutoring Systems)
Problems geared towards conceptual knowledge or procedural knowledge
The problems targeting conceptual knowledgeincluded all three features, while procedural problems were supported through thecollaboration features of roles and unique information.",,"For the conceptual conditions, the students working individually (IC) requested significantly more hints than those working collaboratively (CC), while in the procedural conditions the students working individually (IP) asked for marginally more hints than the students working collaboratively (CP), t(27.8) = 2.00, p = .06.

While joint attention correlated with total test scores (i.e., conceptual and procedural items), there was no correlation between the amount of joint attention and the procedural test items, r = .14, p = .491. Surprisingly, for the procedural condition, there was a correlation between the joint attention and the conceptual test items, r =.35, p = .072, but this was not observed for the conceptual condition, r = .08, p =. 777.","G=42 , I=84",,lab,,,Elementary students (4th and 5th),Elementary students (4th and 5th),University students
81,81,,,,"H1. Students who share mutual gazes during the activity – for example, those teams with higher JVA values – obtain higher scores in their post-activity knowledge tests as well since they engage more in collaborative tasks",,"Understanding gender effects in collaboration dynamics and investigating best learning
","Students worked in teams to complete a muscle painting activity as part of their required laboratory activities. They were expected to identify and paint the major muscles of their body using one of the learning instruments (textbook, tablet or AR) and washable painting supplies.","Participants using instrumental tools with identical information
(study conditions)
1) textbook
2) interactive app on tablet
3) screen-based AR system","Experimental teams who interacted with 3D digital learning tools had a high frequency of JVA and better knowledge retention outcomes than those in the control group.
We also investigated the association of user study gender composition effects on JVA ratios and team test scores. We found no significant difference for JVA ratios or post-test scores among different teams with varied gender compositions.","G=30, I=60",,lab,,,Elementary students (4th and 5th),Elementary students (4th and 5th),University students
82,82,,,,"1. How does learners’ behavior (gaze and actions) affect their performance and the learning gain?
2. How does the gaze change in the di erent tasks in informal learning context?",,"Utilize DUET to understand how learners collaborate, engage with the content and learn with interactive displays ininformal educational settings",Participants were asked to go through a set of posters and interact with the application (both in collaborative and competitive ways).,game1) collaborative2) competitive,"The results show that collaboratively, pairs who have high gaze similarity have high learning outcomes.Individually, participants spending high proportions of time in acquiring the complementary information from images and textual parts of the learning material attain high learning outcomes.","G=18, I=36",,lab,,,University students,,
83,83,,,,This study hypothesizes that students would experience no self-reported mean changes in topic emotions from their pre-assessment scores for each engineering design topic and instructional format nor would electrodermal activities (EDA) associate to these topic emotions throughout the design activities.,,"The goal of this research is to improve our understanding of how students respond, via engagement, to their engineering design activities during class.","88 freshmen engineering students completed online pretopic and posttopic emotions surveys for five engineering design activities. A subset of 14–18 participants, the focal point of this study, wore an EDA sensor while completing the surveys and participating in these sessions.","This course introduces students to the 3-D
computer-aided design (CAD) software, SolidEdge, through a series of five engineering
design method workshop topics created by an engineering instructor, allowing students to
apply their 3-D CAD skills in the context of an engineering design problem.","Preliminary findings suggest that EDA increased for individual and collaborative active learning activities compared to lectures. No significant changes in EDA were found between individual and collaborative active learning activities. Moderate negative correlations were found between EDA and negative topic emotions in the first engineering design activity but not across the rest. At the end of the semester, active learning activities showed higher effect sizes indicating a re-enforcement of students’ engagement in the engineering design method activities.",I=88,,lab,,,University students,,
84,84,,,,"RQ1. How accurately do unimodal and multimodal non-verbal behavioral signals predict task performance?
RQ2: How do role-based, trait-based, and behavior-based approaches to combining multiparty features compare to an equal-weighting baseline?
RQ3: How do nonverbal and language models compare and does combining the two improve performance?
RQ4. How accurately can we prospectively predict task performance? ",,"We investigated the accuracy of machine-learned models trained on facial expressions, acoustic-prosodics, eye gaze, and task context information, computed one-minute prior to the end of a game level, at predicting success at solving that level.","We used Physics Playground as our problem-solving environment. Physics Playground is a two-dimensional educational game that aims to teach students basic Newtonian physics concepts through gameplay. Students complete levels by using mouse input to draw simple machines (ramps, levers, pendulums, and springboards) that guide a green ball to a red balloon.",1) At home surveys and short tutorials 2) In lab procedure,"AUROCs for unimodal models that equally weighted features from the three teammates ranged from .54 to .67, whereas a combination of gaze, face, and task context features, achieved an AUROC of .73. The various multiparty weighting strategies did not outperform an equal-weighting baseline. However, our best nonverbal model (AUROC = .73) outperformed a language-based model (AUROC = .67), and there were some advantages to combining the two (AUROC = .75). Finally, models aimed at prospectively predicting performance on a minute-by-minute basis from the start of the level achieved a lower, but still above-chance, AUROC of .60.","G=101, I=303",,lab,,,University students,,
85,85,,,,"RQ1 (Can temporal analysis expose relationships between variables that are not visible in overall frequency analyses?)
RQ2 (Does multimodal data provide more accurate predictions from those gained by single data streams for collaborative learning?)
RQ3 (What multimodal combinations are more beneficial than others?",,"In this paper, we investigate how both the use of multimodal data and moving from averages and counts to temporal aspects in a collaborative setting provides a better prediction of learning gains.",problem solving,"For the experiment, the dyads engaged in an equivalent fractions problem-solving activity using a networked collaborative ITS, which allowed them to synchronously work in a shared problem space where they could see each other’s actions while sitting at their own computers.",,"I =25, G = 12",,lab,,,middle school students,,
86,86,,,,"By analyzing different data streams from elementary school student’s collaborations, we can better understand what differentiates successful dyads from those that are less successful and how best to support these interactions in a CSCL environment for this age group.",,"We focus on how to best scaffold sense-making behaviors, as collaborationis often successful when these learning behaviors are elicited.","Students were assigned to pairs based on their teachers’ pairings, and each pair was randomly assigned to one of four conditions, by crossing two factors; collaborative or individual instruction and problems geared towards conceptual knowledge or procedural knowledge.","Individual and collaborative versions of an ITS (Intelligent Tutoring Systems)Problems geared towards conceptual knowledge or procedural knowledgeThe problems targeting conceptual knowledge
included all three features, while procedural problems were supported through the
collaboration features of roles and unique information.","For the conceptual conditions, the students working individually (IC) requested significantly more hints than those working collaboratively (CC), while in the procedural conditions the students working individually (IP) asked for marginally more hints than the students working collaboratively (CP), t(27.8) = 2.00, p = .06.While joint attention correlated with total test scores (i.e., conceptual and procedural items), there was no correlation between the amount of joint attention and the procedural test items, r = .14, p = .491. Surprisingly, for the procedural condition, there was a correlation between the joint attention and the conceptual test items, r =.35, p = .072, but this was not observed for the conceptual condition, r = .08, p =. 777.","G=42 , I=84",,ecological,,,Elementary students (4th and 5th),,
87,87,,,,Do extreme values from gaze episodes predict the quality of collaboratively produced concept maps better than central trends?,,We apply EVT to eye-tracking data collected during online collaborative problem solving with the aim of predicting the quality of collaboration.,"Upon their arrival in the laboratory, the participants signed a consent form. Then they took an individual pre-test on the basics of neuronal transmission. Then the participants individually watched two videos about “resting membrane potential.” Next, they created a collaborative concept-map using IHMC CMap tools.3 Finally, they took an individual post-test.",Participants worked as pairs when making the collaborative concept-map(10-12 minutes) in the laboratory.,,"G=33, I=66",,lab,,,University students,,
88,88,,,,"RQ1. Are 4th and 5th grade students able to show learning gains from a short period of instruction with a collaboration-enabled ITS?
RQ2. Is the development of conceptual knowledge especially facilitated when collaborators work on conceptually-oriented learning materials, compared to procedurally-oriented materials?
RQ3: Is joint visual attention related to increases in learning?",,"To investigate this effect, we leverage and expand upon a new methodology, dual eye-tracking, to understand how collaborators’ joint attention may impact learning in a collaboration-enabled Intelligent Tutoring System for fractions. We present results from a study in which 28 pairs of 4th and 5th grade students completed a set of either conceptually- or procedurally-oriented instructional activities in a school setting.","They were assigned to dyads based on their teachers’ pairings, and each dyad was randomly assigned to one of four conditions, created by crossing two factors; whether learning was collaborative or individual, and whether the learning materials were geared towards acquiring conceptual knowledge or procedural knowledge. As the present hypotheses are only concerned with the collaborative conditions, the sample of interest here are the 28 students in the collaborative/conceptual and 28 students in the collaborative/procedural conditions.","1) collaborative/conceptual condition: problems focus on understanding underlying principles of fraction equivalence, and how individual components (e.g., numerators, denominators) are interrelated2) collaborative/procedural condition: focused on scaffolding student problem solving as they create and compare equivalent fractions","Results indicate that students collaborating exhibited learning gains for conceptual knowledge, but not for procedural knowledge, and that more joint attention was related to learning gains.","1) G= 14, I= 282) G= 14, I= 28",,lab,,,Elementary students (4th and 5th),,
89,89,deleted90,,,"H1: task level and task unit level: How does the level of understanding relate to the prevalence of different gaze episodes?
H2: task unit level: How do the types of gaze episodes relate to the types of dialogue episodes?
H3: task unit level and operation level: How do different dialogue episodes relate to the different gaze transitions?",,"The present dual eye-tracking study examines the relationship between gaze, speech and performance in spatially distributed (remote) pair programming.","In the experiment, pairs of subjects had to solve two types of pair programming tasks. The task consisted of describing the rules of a game implemented as a Java program. ",,"In a nutshell, we showed that there is a relationship between gaze and dialogue indicators at different time scales. These relations help us understand the cognition that underlies program comprehension as well as the collaboration that underlies pair programming. ",G=16， I = 32,,lab,,,University students,,
90,91,,Looking To Understand: The Coupling Between Speakers’ and Listeners’ Eye Movements and Its Relationship to Discourse Comprehension,kavie,"H1. How speakers and listen- ers deployed their attention within a visual “common ground""?",,to investigate the temporal coupling between conversants’ eye movements and to examine whether this coupling is helpful to the success of the discourse.,One set of participants (speakers) talked spontaneously about a television show whose characters were displayed in front of them; and the listeners who look at the same display listen to the audio recordings of the speakers,,"1. The eye movements of speakers and listeners are linked;
2. How closely a listener is following a speaker's gaze predicts how well the lister will answer comprehension questions
","4 speakers, 36 listeners",,lab,,,university students,,
91,92,,On the Same Wavelength: Exploring Team Neurosynchrony in Undergraduate Dyads Solving a Cyberlearning Problem With Collaborative Script,"kavie, zoe","H1: Use of a social script will result in improved individual learning (individual knowledge acquisition) and team learning (collaborative problem-solving performance and efficiency).
H2: Dyads using a social script will exhibit higher alpha-band phase-locking values, indicating enhanced team neurosynchrony, compared to epistemic script dyads.",,1. Our study explored which collaboration script – epistemic or social – would result in increased alphaband across-brain synchrony and behavioral manifestations of improved learning during an authentic collaborative problem-solving task.,Teams use a cyberlearning environment to define the problem to be solved on the provided descrition and solve the problem using relevant resources and strategies,,"1) Analyses revealed greater alpha-band phase-locking values between the central and parietal electrodes of dyad members in the epistemic script condition
2) Mean alpha phase-locking values were positively correlated with collaborative problem solving performance and negatively correlated with time spent on the problem solving process, suggesting that epistemic scripts were more effective scaffolds of collaborative problem solving compared to social scripts in this study.",G=40; I=80,,lab,,,"University: age range 18-24; 70 male, 70 female",,
92,93,,Automatically Recognizing Facial Indicators of Frustration: A Learning-centric Analysis,kavie,"H1: Does the affective interpretation by one participant coincide with reported affective states of the other participant?
H2: Do one participant's bodily expressions coincide with reported affect of either participant?",,"1. To investigate the extent to which some dimensions of affect are implicitly expressed through computer-mediated textual dialogue
2. To see how the perceptions of one participant correspond to the posture and gesture expressed by the other participant, even when those multimodal features were not observable through the textual dialogue channel","Students participated for course credit in an introductory engineering course, ",,"1. Tutors implicitly perceived students’ focused attention, physical demand, and frustration.
2. Bodily expressions of posture and gesture correlated with student cognitive-affective states that were perceived by tutors through the implicit affective channel.
3. Posture and gesture complement each other in multimodal predictive models of student cognitive-affective states, explaining greater variance than either modality alone.",I = 42,,lab,,,students,,
93,94,,"The relationships between learner variables, tool-usage behaviour and performance",kavie,"H1: How does tool-usage behaviour differ between students (i.e., frequency of tool use and proportional time spent on tools) in an open learning environment?
H2: Do different patterns result in performance differences?
H3: Can different patterns be related to the variation of learner
variables (prior knowledge and goal orientation)?
",,"1. In order to better understand the impact of learner variables on tool use, the current study investigates the relationships between learner variables, the quantitative aspects of tool-usage behaviour and its out- come (i.e., performance).
2. More specifically, the focus is on how the variation in tool use is related to prior knowledge and goal orientation and how this variation affects performance. ",Participants use a computer-based learning program - STUWAWA that engages them in ecological decision making. They were asked to collect evidence to build up their arguments from three perspectives,,"1. Variation in tool-usage behaviour is related to mastery goal orientation.
2. Distinguished tool-usage behaviour could result in significantly different tool- usage outcomes.  ",I = 58,,lab,,,"97% female, university students",,
94,95,,Effects of Knowledge Interdependence with the Partner on Visual and Action Transactivity in Collaborative Concept Mapping,kavie,"H1: To what extent do participants visually refer to their partner’s prior knowledge (the partner’s individual concept map) while building the collaborative concept map (visual transactivity)?
H2: To what extent do participants manipulate their partner’s contributions (intersubjective uptake acts) in the collaborative map (action transactivity)?
H3: To what extent does transactivity at both the visual and action levels influence (a) individual learning and also (b) equivalence between learning partners regarding the extent of their outcome knowledge
H4: To what extent does a computer-supported script designed to create knowledge interdependence among co-learners influence both individual acquisition of knowledge and outcome knowledge equivalence?
H5: To what extent does knowledge interdependence influence (a) the amount of time participants spent looking at their partner’s individual concept map (visual transactivity) and (b) the number of uptake acts during the building of the collaborative concept map (action transactivity)?",,"1. We wanted to deepen our knowledge of how students – working in dyads – construct a common concept map in order to graphically represent their shared understanding of a particular learning topic (the functioning of the neuron).
2. More specifically, we focused on transactivity in collaborative concept mapping. ",Participants worked in dyads to build a concept map collaboratively,"1) same information
2) complementary condition","1. found that the degree to which participants co-manipulate the same objects in the collaborative map (action transactivity) is higher when they discussed identical (rather than complementary) information.
2. Results from eye-gaze data showed that participants who shared complementary information transitioned more frequently between their own map and their partner’s map; eye-movement transitions between own and peer maps were also negatively correlated with learning outcomes.",G=29; I=58,,lab,,,"47 men, 11 women, university students",,
95,96,,On the Same Wavelength: Exploring Team Neurosynchrony in Undergraduate Dyads Solving a Cyberlearning Problem With Collaborative Script,zoe,"H1: Use of a social script will result in improved individ-ual learning (individual knowledge acquisition) and teamlearning (collaborative problem-solving performanceand efficiency).
H2: Dyads using a social script will exhibit higheralpha-band phase-locking values, indicating enhancedteam neurosynchrony, compared to epistemic scriptdyads",,"Thisstudy explored the synchrony of mutually interacting brains,or team neurosynchrony, during cyber-enabled collabora-tive problem solving.",,,,"I = 80, G=40",,lab,,,140 undergraduate students at alarge public university in the southeastern United States.All participants werebetween 18 and 24 years old.,,
96,97,,The NISPI framework: Analysing collaborative problem-solving from students' physical interactions,zoe,The overarching research aim of this study is to investigate the four aspects of CPS via nonverbal indexes of students' physical engagement.,,The overarching research aim of this study is to investigate the four aspects of CPS via nonverbal indexes of students' physical engagement.,building a working prototype using an Arduino-based physical computing kit,NS,The results show that the NISPI framework can be used to judge students' CPS competence levels accurately based on their non-verbal behaviour data. ,I = 45,,lab,,,"which nine are in the first year of their secondary education (aged 11e12 years) from a girls-only secondary school in the UK, and the other thirty-six are Engineering students at a European Uni- versity, with an average age of 20 years old, three female and thirty three male. ",,
97,98,,Many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities,zoe,"What do learners’ visible behavior profles reveal about learning in a collaborative
 open-ended learning activity?",,"The goal of this paper is to build a multi-modal understanding of learning vs non-learning
 as it happens in a collaborative open-ended activity","an open-ended learning
 task around interactive tabletops with a robot mediator",NS," The amount of speech interaction and the overlap of
 speech between a pair of learners are behavior patterns that strongly distinguish between
 learning and non-learning pairs. Delving deeper, fndings suggest that overlapping speech
 between learners can indicate engagement that is conducive to learning.","I= 64, G= 32",,lab,,,9-12 years,,
98,99,,"Supervised machine learning in multimodal learning analytics
for estimating success in project-based learning",zoe,Which features of student group work are good predictors of team success in open-ended tasks with physical computing? T,,"This paper investigates the use of diverse sensors, including computer vision, user-generated content, and data from the learning objects (physical computing components), to record high-fidelity synchro- nised multimodal recordings of small groups of learners interacting. ",,NS,"The results illustrate that state-of-the-art computational techniques can be used to generate insights into the ""black box"" of learning in students' project-based activities. The features identified from the analysis show that distance between learners' hands and faces is a strong predictor of students' artefact quality, which can indicate the value of student collab- oration. ","I = 18, G =6",,ecological,,,university,,
99,100,,Modelling Collaborative Problem-solving Competence with Transparent Learning Analytics: Is Video Data Enough?,zoe,"1) What automated metrics from video data can be used to predict students’ speaking, making, listening and watching behaviours during collaborative learning activities?
2) To what extent can video data analytics accurately predict learners’ CPS competence?",,Our ultimate goal is to generate transparent and explainable models that predict learners’ CPS competences from video data.,Design tasks (engineering),NS,"Although the decision trees illustrate that the automated metrics from video data can be used to predict CPS competence better than baseline, the accuracy of the decision trees need to be improved. The accuracy of the transparent models we built are inferior to previous LA work in the field that involves multimodal data ","I = 45, G=15 ",,lab,,,"18 engineering students (17 male, 1 female). Their average age was 20 years old",,
100,101,,An Integrated Observing Technic for Collaborative Learning: The Multimodal Learning Analytics Based on the Video Coding and EEG Data Mining,zoe,,, it is an integrated observation technic to represent it at both macro and micro levels through video coding and electroencephalogram (EEG) data mining,,NS,"6 participants, 2 groups","I = 6, G= 2",,ecological,,,"3 females and 6 males, who were third grade engineering students in one normal university.",,
101,102,,Inter-brain synchrony in teams predicts collective performance,zoe, examine the relationship between group identification and inter-brain synchrony in explaining collective per- formance,,This design allowed us to examine the relationship between group identification and inter-brain synchrony in explaining collective per- formance. ,Problem-solving tasks,NS,These results suggest that inter-brain synchrony can be informative in understanding collective performance among teams where self-report measures may fail to capture behavior.,"I= 174, G= 44",,ecological,,,university,,
102,103,,,kavie,"H1: On average, significantly more time is spent gazing at the individual one listens or speaks to, than at others
H2: On average, significantly more time is spent gazing at each person when addressing a group of three, than at others when addressing a single individual
H3: On average, time spent gazing at each individual when addressing a group of three is significantly more than one third of the time spent gazing at a single addressed individual
H4: On average, significantly more time is spent gazing at the individual one listens to, than at the individual one speaks to",,"1. In this paper, we present an experiment aimed at evaluating whether gaze directional cues of users could be used for establishing who is talking to whom","Subjects participated in four 8-minute sessions: one in which we recorded where they looked using a desk-mounted eye tracker, and three in which they were conversational partners only. ",,"1. Results indicate that when someone is listening or speaking to individuals, there is indeed a high probability that the person looked at is the person listened (p=88%) or spoken to (p=77%).
2. We conclude that gaze is an excellent predictor of conversational attention in multiparty conversations.",I=24,,lab,,,"19 male, 5 female, university students",,
103,104,,,kavie,H1: If seeing one person's visual focus of attention (represented as an eyegaze cursor) while debugging software (displayed as text on a screen) can be helpful to another person doing the same task. ,,"1. We set out to demonstrate that having a visual representation of one person's eye gaze can improve another's performance in a visuo-spatial domain, namely, debugging computer programs.",Programmers visited lab and found bugs in programs,,"1. Another person's eye gaze, produced instrumentally (as opposed to intentionally, like pointing with a mouse), can be a useful cue in problem solving.
2. This finding supports the potential of eye gaze as a valuable cue for collaborative interaction in a visuo-spatial task conducted at a distance.",I=10,,lab,,,expert software engineers,,
104,105,,,kavie,"H1: To what extent does collaborative search produce meaningful benefits over soli- tary search?
H2: How do people coordinate during collaborative search?
H3: What communication medium best mediates time-critical collaborative search?",,"1. In this study, we use the grounding framework and a visual search task to study the behavioral coordination underlying collaboration.","Participants searched for an O among Qs oriented at 0°, 90°, 180°, and 270°.","1) Shared-Gaze (SG)
2) Shared-Voice (SV)
3) SG+V
4) No-Communication (NC)
5) One-person (1P)","1. People can successfully communicate and coordinate their searching labor using shared gaze alone.
2. Strikingly, shared gaze search was even faster than shared-gaze-plus-voice search; speaking incurred substantial coordina- tion costs.
3. We conclude that shared gaze affords a highly efficient method of coordinating par- allel activity in a time-critical spatial task.",G=40; I=20,,lab,,,graduate students,,
105,106,,,kavie,"H1: We hypothesize that if we can show the remote helper the desired view of the worker’s environment in any specific instance of the task automatically, it will free the helper from having to control the camera.","In this research, we are interested in the relationship between spoken utterances and gaze. Our goal is to predict focus of attention from keywords extracted from the dialogue during a collative physical task, i.e., Talk -to-Look.",,The helper and worker collaborated to construct a series of puzzles.,"1) Differentiability of the puzzle pieces (solid colors vs. shaded)
2) Complexity of the puzzle (5, 10, 15)","The results further suggest that a helper’s desired focus of attention can be predicted based on task properties, his/her partner’s actions, and message properties.",G=12; I=24,,lab,,,college and graduate students,,
106,107,,,kavie,H1: We hypothesized that DOVE’s drawing component would improve communication over a scene camera alone because it allows helpers to display a full range of pointing and representational gestures.,,"In this research, our goal is to develop technologies to support communication through speech and gesture during collaborative physical tasks.",build a large toy robot,"1) Scene camera only: helper could view the output of the IP camera focused on the workers task environment, but could not manipulate the video feed;
2) DOVE + manual erasure of gestures: helper could draw on the video feed but had to manually erase their gestures; and
3) DOVE + automatic erasure of gestures: helpers could draw on the video feed and the gestures faded out after 3 seconds.","As hypothesized, performance was significantly faster with the DOVE system than with the network IP camera alone (see Figure 11). Performance was also faster with the auto-erase function enabled than when helpers had to manually erase their gestures.",G=14; I=28,,lab,,,,,
107,108,,,kavie,"H1: Speaking time will be distributed more equally in sessions with feedback than in sessions without feedback. Concretely,
participants who under-participate without feedback will
participate more in the presence of feedback and participants who over-participate without feedback will participate less when
feedback is presented.
H2: Speakers’ visual attention will be distributed more equally
among listeners when feedback is present than without feedback.
H3: Visual attention from listeners for the speaker will be higher in sessions with feedback.
H4: Participants’ satisfaction about group communication and
performance will be higher in the presence of feedback.",,"In this paper we focus on social dynamics and we are interestedin investigating whether we can influence the social dynamics ofa meeting by providing feedback to the meeting participants.","Each group participated in two discussionsessions in which the members had to reach agreement on aparticular topic.","1) feedback about speaking time and visual attention
was provided in the form of a visualization
2) in the other condition nofeedback was provided","It was found that the presence of the feedback about speaking
time influenced the behaviour of the participants in such a way
that it made over-participators to behave less dominant and
under-participators to become more active. Feedback on eye gaze
behaviour did not affect participants’ gazing behaviour (both for
listeners and for speakers) during the meeting.","G=19; I=76
G=2; I=6",,lab,,,,,
108,109,,A Look Is Worth a Thousand Words: FullGaze Awareness in Video-MediatedConversation,zoe,The hypothesis that full gaze awareness can perform the function of check andalign games nonverbally,"Full gaze awareness,defined here as knowing what someone is looking at, might be ex-pected to be a powerful communicative resource when the conversation concerns someobject of common interest in the environment. This article sets out to demonstrate thispossibility in the context of video-mediated communication.",hypothesis that full gaze awareness can perform the function of check andalign games nonverbally,E learning ,"1)GA DisplayVideo
2)Tunnel Only
3) Audio Only","1.  Making full gaze awareness possible considerably reduces the amount oftalk and, even more notably, the degree to which participants need to verballycheck their own and the other person’s understanding of what has been said.2.  Any advantage provided by a view of the face (facial expression and mu-tual gaze) is smaller than the advantage provided by full gaze awarenessand not detectable in this experiment.",G=24; I=48,,lab,,,,,
109,110,,Recognizing communicative facial expressions for discovering interpersonal emotions in group meetings,zoe,"Our goal is to automatically discover the interpersonal emotions that
evolve over time in meetings, e.g. how each person feels
about the others, or who affectively influences the others
the most. A",,"This paper proposes a novel facial expression recognizer and
describes its application to group meeting analysis. ",group meeting ,none,A four-person meeting captured by an omnidirectional video system is used to confirm the effectiveness of the proposed method and the potential of our approach for deep understanding of human relationships developed through communications,"G= 1, I =4",,lab,,,,,
110,113,,Coordinating spatial referencing using shared gaze,zoe ,how emotion evolves during learning process and how emotion feedback could be used to improve learning experiences.,this study explored how emotion evolves during learning process and how emotion feedback could be used to improve learning experiences.,,," SG, shared gaze; SGV, shared gaze plus speech;
SV, shared voice; NC, no communication","These results suggest that sharing gaze can be more efficient than speaking when people
collaborate on tasks requiring the rapid communication of spatial information","I =32, G =16",,lab,University ,,,,
111,114,,Affective e-Learning: Using “Emotional” Data to Improve Learning in Pervasive Learning Environment,zoe ,this study explored how emotion evolves during learning process and how emotion feedback could be used to improve learning experiences.,,this study explored how emotion evolves during learning process and how emotion feedback could be used to improve learning experiences.,online learning ,NS,Experiments indicated the superiority of emotion aware over non-emotion-aware with a performance increase of 91%.,"G = 1, I =2 (online tutoring)",,university ,,,,,
112,118,Deleted 115-117,Multimodal Analysis of Group Attitudes Towards Meeting Management,zoe,We hypothesize that linguistic information is useful when trying to detect complex attitudes that may only subtly manifest themselves in group situations.,,"We present experimental results on the task of automatically predicting group members’ attitudes about management of their meeting,
based on linguistic and acoustic features derived from the meeting
recordings and transcripts. T",group meeting ,none,"A key finding is that features of linguistic content by themselves yield poor
prediction performance on this task, but the best results are found
by combining acoustic and linguistic features in a multimodal prediction model. ","G= 3, I =12",,,,,,,
113,119,,"Toward Open-Microphone Engagementfor Multiparty Interactions",kavie,"1. We hypothesized that speakers would use substantially
higher amplitude when addressing a computer compared
with human peers
2. We hypothesized that substantial,
abrupt, and bi-directional amplitude shifts would occur
across adjacent utterance boundaries representing such
changes in interlocutor.
3. We hypothesized that amplitudedifferences would be largest when lexical markers wereabsent, and attenuated when present.
4. We hypothesized large individualdifferences among speakers in the adoption of commandstylespeech, and also a higher ratio of such input directedto computers.",,"Given the dynamic way people useamplitude to capture and direct an addressee’s attention, in thisstudy we focus on amplitude as a potential marker of intendedinterlocutor during computer-assisted group interactions.","solve basic geometry andalgebra problems,","1. low
2. moderate
3. very-high","Results
revealed that amplitude was 3.25dB higher when users
addressed a computer rather than human peer when no lexical
marker of intended interlocutor was present, and 2.4dB higher
for all data. ",G=6; I=18,,lab,,,,,
114,120,,Evaluation of user gestures in multi-touch interaction: a case study in pair-programming,kavie,"1. is multi-touch better than the desktop for some traditional application?
2. can gesticulation be used as a suitable signal of natural interaction justifying  the chain that more gestures provoke a more natural, and, thus, better interaction?",,"Is there any practicaladvantage (e.g., in terms of efficient problem solving) when usinga natural interface?",review snippets of C code,"1. Desktop
2. Multi-touch","Weshow how the adoption of a multi-touch user interface fosters asignificant, observable and measurable, increase of nonverbalcommunication in general and of gestures in particular, that inturn appears related to the overall performance of the users in thetask of algorithm understanding and debugging.",G=22; I=44,,lab,,,,,
115,121,,"Towards Adapting Fantasy, Curiosity and Challenge in
Multimodal Dialogue Systems for Preschoolers",kavie,"We investigate how fantasy, curiosity and challenge con-
tribute to the user experience in multimodal dialogue com-
puter games for preschool children.",,,"The tasks selectedwere (the target age group for each task is shown in paren-thesis): animal recognition (ages 3-4), shape recognition(ages 4-5), quantity comparison (ages 3-4), number recog-nition (ages 5-6) and addition (ages 5-6).",,"Results show high speech usage and high curiosity levels in the application correlate well with task completion, showing that preschoolers become more engaged when multimodal interfaces are speech enabled and contain curiosity elements.",I=9,,lab,,,,,
116,122,,"Multimodal recognition of personality traits in social interactions
Authors
",zoe,This paper targets the automatic detection of personality traits in a meeting environment by means of audio and visual features; information about the relational context is captured by means of acoustic features designed to that purpose. T,,This paper targets the automatic detection of personality traits in a meeting environment by means of audio and visual features; information about the relational context is captured by means of acoustic features designed to that purpose. T,group problem solving in a meeting,NS,"The outcomes improve considerably over existing results, provide evidence about the feasibility of the multimodal analysis of personality, the role of social context, and pave the way to further studies addressing different features setups and/or targeting different personality traits.","G=1, I = 4",,lab,,,,,
117,123,,Modeling the Personality of Participants During Group Interactions,zoe,"target the automatic prediction of two personality traits, Extraversion and Locus of Control, in a meeting scenario using visual and acoustic features",," In this paper we target the automatic prediction of two personality traits, Extraversion and Locus of Control, in a meeting scenario using visual and acoustic features",group problem solving in a meeting,NS,"We designed our task as a regression one where the goal
is to predict the personality traits‘ scores obtained by the meeting participants.
Support Vector Regression is applied to thin slices of behavior, in the form of
1-minute sequences.","G=1, I = 4",,lab,,,,,
118,124,,"Automatic prediction of individual performance from"" thin slices"" of social behavior",zoe,"This paper targets the automatic detection of individual
performances in group tasks by means of short sequences, ‘‘thin
slices’’, of nonverbal behavior. ",,"This paper addresses the prediction of individual performance in
group tasks by means of short sequences of nonverbal behavior
(‘‘thin slices’’)",Mission Survival Task - group problem solving ,NS,"Few social signals (a subset of Emphasis and general visual
activity features) have turned out to be enough to take accuracy
close to 0.5 and provide a statistically significant improvement
over the trivial classifier.","G=1, I = 4",,lab,,,,,
119,125,,Combining audio and video to predict helpers' focus of attention in multiparty remote collaboration on physical tasks,kavie,"1. explore the possibility of predicting the helper's FOA from speech
2. explore the possibility of predicting the helper's FOA from video",,"In this research , our goal is to develop a multimedia system that supports multipart y remote collaboration s on physical tasks in which a helper (expert) assists multiple workers ( novices).",collaborative circuit assembly task,,The overall prediction accuracies are 79.52% using audio only and 81.79% using audio and video combined.,G=12; I=36,,lab,,,,,
120,126,,Predicting remote versus collocated group interactions using nonverbal cues,kavie,1. We hypothesize that the difference in the dynamics between collocated and remote meetings is significant and measurable using speech activity based nonverbal cues.,,"This paper addresses two problems: Firstly, the problem of classifying remote and collocated small-group working meetings, and secondly, the problem of identifying the remote participant, using in both cases nonverbal behavioral cues. Such classifiers can be used to improve the design of remote collaboration technologies to make remote interactions as effective as possible to collocated interactions.",,,"Our results on a publicly available dataset - the Augmented Multi-Party Interaction with Distance Access (AMIDA) corpus - show that such an approach is promising, although more controlled settings and more data are needed to explore the addressed problems further.",G=9,,ecological,,,,,
121,127,,Reciprocal attentive communication in remote meeting with a humanoid robot,kavie,"1. By using the robot’s head motion, the remote user’s attention condition, such as listening or not listening, can be perceived more easily than by using video conferencing because in video conferencing the remote user’s focus of attention information is constrained by the 2-D display.
2. The remote user’s gaze direction would not match that of the local objects.
3. When using a robot, the remote user’s attention is expressed by the robot’s head; that is, the robot’s attention matches that of the local objects.",,,,"1. face-to-face
2. robot
3. video",Our experimental result shows that a tangible robot avatar provides more effective reciprocal attention against video communication.,G=3; I=6,,lab,,,,,
122,128,,Estimating focus of attention based on gaze and sound,zoe,"we are interested in
modeling focus of attention in a meeting situation. W",," In work presented here, we are interested in
modeling focus of attention in a meeting situation. We have
developed a system capable of estimating participants’ focus of attention from multiple cues. We employ an omnidirectional camera to simultaneously track participants’ faces
around a meeting table and use neural networks to estimate
their head poses. In addition, we use microphones to detect
who is speaking. T",group meeting,NS,"Using this approach, we have achieved 74 % accuracy in detecting the
participants’ focus of attention on three recorded meetings.","G=4, I = 12 ",,lab,,,,,
123,129,,Gaze quality assisted automatic recognition of social contexts in collaborative Tetris,zoe,"The first question concerns the possibility of predicting
the social context of the players from their eye-movement
data, possibly combined with their actions. In this respect,
we want to compare the predictive power of using different
features, such as only gaze of one player, dual gazes or a
combination of gazes and actions.
The second question is how much these predictions are
affected by missing gaze data and more precisely, is it possible to improve the predictions by taking into account the
quality level.",,In this paper we have studied the automatic detection of social context (pair composition) in collaborative,collaborative Tetris game,NS,"We describe several methods for the improvement of
detection (or recognition) and experimentally demonstrate
their effectiveness, especially in the situations when the collected gaze data are noisy","G=  59, I =118
",,lab,,,,,
124,130,,Tracking the Multi Person Wandering Visual Focus of Attention,zoe,"To solve the WVFOA problem, we propose a multi-person tracking approach based on a hybrid Dynamic Bayesian Network that simultaneously infers the number of people in the
scene, their body and head locations, and their head pose,
in a joint state-space formulation that is amenable for person interaction modeling",,"To solve the WVFOA problem, we propose a multi-person tracking approach based on a hybrid Dynamic Bayesian Network that simultaneously infers the number of people in the
scene, their body and head locations, and their head pose,
in a joint state-space formulation that is amenable for person interaction modeling",a group of people looking at advertisment ,NS,"Our model was rigorously evaluated for tracking and its ability to recognize when people look at an outdoor advertisement using
a realistic data set.","G=9, I =22",,ecological,,,,,
125,131,,Putting the Pieces Together: Multimodal Analysis of Social Attention in Meetings,zoe,"H1: Attention is mostly given to the person sitting
right in front of the observer. This hypothesis derives
from an observation made in [14].
• H2: There exists a direct relationship between the verbal behavior of a person and the amount of attention
he receives. This hypothesis derives from [10].
• H3: Use of eye-gaze in conjunction with head-pose improves accuracy of automated social attention estimation. This hypothesis directly derives from the above
discussion.",,"This paper presents a multimodal framework employing eyegaze, head-pose and speech cues to explain observed social
attention patterns in meeting scenes.",group problem solving ,NS,Experimental results show that combining eye-gaze and head-pose estimates decreases error in social attention estimation by over 26%.,"G= 1, I =4 ",,lab,,,,,
126,132,,Modeling focus of attention for meeting indexing,zoe,"The research issues include to identify: 1)
who/what is the source of the message, 2) who or what is
the target and ob ject of the message (focus of attention), 3)
what is the content of the message in the presence of jamming noise.",,"In this paper, we present an approach to detect who is looking at whom during a meeting",meeting,NS,". The system has achieved an accuracy rate of
up to 93 % in detecting focus of attention on test sequences
taken from meetings. We have used focus of attention as an
index in a multimedia meeting browser.","G=1, I = 4",,lab,,,,,
127,133,,Modeling focus of attention for meeting indexing based on multiple cues,zoe,The research issues include to identify: 1) who/what is the source of the message; 2) who or what is the target and object of the message (focus of attention); 3) what is the content of the message in the presence of jamming noise.,,We have evaluated the system using the data from three recorded meetings. The acoustic information has provided 8% relative error reduction on average compared to only using one modality. The focus of attention model can be used as an index for a multimedia meeting record. It can also be used for analyzing a meeting.,meeting,NS,"By using both head
pose and sound, focus of attention could be detected in 76% of
the frames in recorded meetings","G=3, I= 12",,lab,,,,,
128,134,,Mediated attention with multimodal augmented reality,kavie,"H1: participants have a lower error rate in the condition using the audiovisual augmentations compared to the condition without them
H2: similarly that they exhibit a shorter reaction time in the condition with both augmentations.",,,performed gaze game,1.“highlighting on” condition where both the visual and the auditory augmentations were provided  2. “highlighting off” condition where neither visual nor auditory augmentation was given,We were able to show an improvement for both dependent variables as well as positive feedback for the visual augmentation in the questionnaire.,G=11; I=22,,lab,,,,,
129,135,,Implicit user-adaptive system engagement in speech and pen interfaces,kavie,"1. People will use both higher speech amplitude and higher pen pressure levels to mark a computer rather than human as an intended addressee.
2. Reliable system engagement can be achieved above chance levels based on implicit energy cues in users’ communication, with speech amplitude yielding higher reliabilities than pen pressure
3. Over a session, people will increase their amplitude and pressure when addressing the computer, decrease them when addressing a human, and expand their energy differential between computer and human─which will yield improvement in overall system reliability.
4. Following computer failures to engage, people will forcefully increase both speech amplitude and pen pressure as an adaptation to repair the error. Basically, they will accentuate their naturally-occurring behavior.",,The primary objectives of the present study involved empirical research and prototyping of implicit user-adaptive interfaces involving speech and pen input for collaborative use in field settings.,solve basic algebra and geometry problems,,"Results revealed that users spontaneously, reliably, and substantially adapted these forms of communicative energy to designate and repair an intended interlocutor in a computer-mediated group setting. This behavior was harnessed to achieve system engagement accuracies of 75-86%, with accuracies highest using speech amplitude. However, students had limited awareness of their own adaptations. Finally, while continually using these implicit engagement techniques, students maintained their performance level at solving complex mathematics problems throughout a one-hour session.",G=12; I=24,,lab,,,,,
130,136,,Gaze-communicative behavior of stuffed-toy robot with joint attention and eye contact based on ambient gaze-tracking,kavie,"1. This research is motivated by the need toclarify both the simple effectiveness of human-robot eye contact/joint attention and the interaction of these behaviors.",,"In this paper, we propose and verify a gaze-communicative stuffed-toy robot system based on a gradual gaze-communication model. The model consists of 1) joint attention action for indirectly getting the user’s interest and 2) eye-contact reaction for directly eliciting the user’s favorable feeling.",,,"From both subjective evaluations and observations of the user’s gaze in the demonstration experiments, we found that i) joint attention draws the user’s interest along with the user-guessed interest of the robot, ii) “eye contact” brings the user a favorable feeling for the robot, and iii) this feeling is enhanced when “eye contact” is used in combination with “joint attention.” These results support the approach of our embodied gazecommunication model.",I=22,,lab,,,,,
131,137,,Multi-party focus of attention recognition in meetings from head pose and multimodal contextual cues,zoe,This paper presents investigations on visual focus of attention (VFOA) recognition in meetings from audio-visual perceptual cues.,,"This paper presented a multi-person focus of attention tracking system that combine two technologies, namely person tracking and head orientation estimation. T","lecture in a meeting room:In this paper, we propose and verify a gaze-communicative stuffed-toy robot system based on a gradual gaze-communication model. The model consists of 1) joint attention action for indirectly getting the user’s interest and 2) eye-contact reaction for directly eliciting the user’s favorable feeling.",NS,"The model is rigorously evaluated on a publicly available dataset of 4 real meetings of 23min on average, showing an overall 10% relative performance increase w.r.t. the independent recognition case.","G= 1, I =4 ",,ecological,,,,,
132,138,,Multimodal Real-Time Focus of Attention Estimation in SmartRooms,zoe,This paper presents a real-time operating system for multi-person focus of attention (FoA) estimation in an inFigure 1. Focus of attention estimation of a group of people may allow the system to identify this gathering as a lecture where somebody is distracted looking through the window and somebody else is checking his email at the computer. door scenario equipped with multiple cameras and far-field microphones,,This paper presents an overview of our work on real-time multimodal tracking focus of attention of multiple persons in a SmartRoom scenario,lecture in a room ,NS,Experiments conducted over annotated databases yield quantitative results proving the effectiveness of the presented approach,G=2; I=8,,lab,,,,,
133,139,,,kavie,"1. Task performance. We predict that performance will be best in the side-by-side condition, because the quality of the shared visual context is maximized, and poorest in the audio-only condition, due to the lack of shared visual context. Performance in the video condition should be intermediate, because the video technology supports some but not all of the benefits of actual physical co-presence.
2. Conversational grounding. Ease of conversational grounding, as indicated by message length, number of conversational turns, and use of deictic expressions, should be easiest in the side-by-side condition, and hardest in the audio condition.
3. Helper expertise. We anticipate that the effects of media condition on task performance and conversational grounding will be mediated by the expertise of the helper.",,In this paper we aim to clarify the role of visual information in one type of computer-supported cooperative work—collaborative repair of complex devices.,repair of complex devices,"1. audio-video
2. audio-only
3. side-by-side","Resultsshow that the dyads complete the task more quickly andaccurately when helpers are co-located than when they areconnected via an audio link. However, they didn’t achievesimilar efficiency gains when they communicated throughan audio/video link. These results demonstrate the valueof a shared visual work space, but raise questions aboutthe adequacy of current video communication technologyfor implementing it.",I=25,,lab,,,,,
134,140,,,kavie,"1. We hypothesize that both the headcamera with eye tracking system and the scene camera system would improve performance over an audio-only link, because the visual cues provided by the systems improve situational awareness and conversational grounding
2. Combined scene plus head-mounted camera system would improve performance over either camera alone, because each provides a complementary set of visual cues.",,"In the current study, we build on the findings by Kraut et al. by comparing the value, alone and in combination, of two different video systems: (a) a head-mounted video system with additional eye-tracking capability and (b) a scene camera that provides a wider view of the work area.",robot assembly,"1. side-by-side
2. audio-only
3. head-mounted camera
4. scene camera
5. scene plus head cameras.","Task completion times were shortest inthe side-by-side condition, and shorter with the scenecamera than in the audio-only condition. Participants ratedtheir work quality highest when side-by-side, intermediatewith the scene camera, and worst in the audio-only andhead-camera conditions. Similarly, helpers self-ratedability to assist workers and pairs communicationefficiency were highest in the side-by-side condition, butsignificantly higher with the scene camera than in theaudio-only condition. The results demonstrate the value ofa shared view of the work environment for remotecollaboration on physical tasks.",G=38; I=76,,lab,,,,,
135,141,,,kavie,"1. by using our system users can spend more time and attention on their primary task, as compared to standard video conferencing systems.
2. the system requires less time and physical effort to stay aware of a remote partner’s attention",,"In this paper, we present an embodied remote presence system designed to support design activities that involve a physical artifact by enhancing peripheral awareness.",debugging session,"1. system condition
2. video condition","The analysis of completion time for guessing attention area shows that participants took more time to guess in the video condition (MD = 2.41s, SD=0.97) than in our system condition (MD = 1.44s, SD=0.46). Users rotate their head less to look at the monitor to determinethe attention area, and can observe it through their peripheral vision",I=13,,lab,,,,,
136,142,,EEG in classroom: EMD features to detect situational interest of students during learning,zoe,this study investigates the possibility of detecting situational interest using Electroencephalogram (EEG) in classroom.,,this study investigates the possibility of detecting situational interest using Electroencephalogram (EEG) in classroom.,lecture  ,NS,"While SVM achieved high accuracy of 93.3% and 87.5% for two data sets using features from the four EEG channels, KNN classifier achieved high accuracy of 87.5% and 86.7% in the same datasets using single EEG channel.","G=1, I=43",,ecological,first year university ,,,,
137,143,,,kavie,"1. Can eye gaze be used in the resolution of temporary ambiguity?
2. What is the time course by which it is integrated with linguistic processing?
3. Does eye gaze serve as an automatic orienting cue, or as a flexible cue?",,"In two experiments, we explored the time course and flexibility with which speakers’ eye gaze can be used to disambiguate referring expressions in spontaneous dialog.",referential communication,"1. near competitor
2. far competitor
3. no competitor","When mirrored displays held far competitors, matchers used directors’ eye gaze to identify targets before the linguistic point of disambiguation. Reversed displays caused substantial competition, yet matchers still identified targets before the linguistic point of disambiguation, showing an ability to rapidly re-map directors’ eye gaze. Our findings indicate eye gaze is a powerful and flexible disambiguating cue in referential communication","G=24; I=48
G=12; I=24",,lab,,,,,
138,144,,,kavie,"1. We hypothesize that MM will encourage meeting participants to modify their behavior to speak more.
2. We posit that participants using MM will be more interactive.
3. We hypothesize that MM, as an ambient and personal display, will not be disruptive to the purpose of the meeting.",,Our system encourages effective group dynamics that may lead to higher performance and satisfaction. We envision MM to be deployed in real-world organizations to improve interactions across various group collaboration contexts.,brainstorming and problem-solving,,,G=37; I=148,,lab,,,,,
139,145,,,kavie,"1. We hypothesise that dominance in
group meetings correlates with high dynamic levels of human
activity, which can be represented by relatively primitive
features extracted from audio and/or video sensor data.",,"The aim of our work is to identify which features are well correlated with the most dominant person in a meeting. We use speaking length and energy, as suggested by [2]. In addition, we also extract features from video since non-verbal cues can also affect perceptions of dominance [9].",design remote control device,,"Our work indicates that, for the 34 meeting segments in which the most dominant person was reliably decoded by all annotators, our investigated audio cues were able to classify the most dominant person with relatively good accuracy. Our compressed-domain video features performed less well but still provided some discrimination, when the motion vectors and residual bit-rate measures were combined.",I=4,,ecological,,,,,
140,146,,Visual focus of attention estimation from head pose posterior probability distributions,zoe,We address the problem of recognizing the visual focus of attention (VFOA) of meeting participants from their head pose and contextual cues. ,,"In this paper, we address the VFOA recognition problem from head pose information. Rather than relying on an estimated head poses defined by a pan and tilt angle, as done in all previous studies on the topic [2], [3], we propose to rely on the posterior probability density function (pdf) of the different head poses given the data to represent the head pose information embedded in the image data.",meeting with lecture ,,Numerical experiments on a public database of 4 meetings of 22 min on average show that this change of representation allows for a 5.4% gain with respect to the standard approach using head pose as observation.,G= 4; I= 16,,ecological,,,,,
141,147,,Towards High-Level Human Activity Recognition through Computer Vision and Temporal Logic,zoe,"we develop state of the art real-time
computer vision systems to track and identify users, and estimate their
visual focus of attention and gesture activity. We also monitor the users’
speech activity in real time.",,"This paper presents a system
that fuses and interprets the outputs of several computer vision components as well as speech recognition to obtain a high-level understanding
of the perceived scene.","group activity in a room/ meeting, presentation, individual work etc.",,"In this paper, we presented our progress towards a framework for high-level human activity recognition.",G=1; I=3,,ecological,,,,,
142,148,,Biosignals reflect pair-dynamics in collaborative work: EDA and ECG study of pair-programming in a classroom environment,zoe,"RQ1 Can we extract SPC from HRV variables in a natural protocol?
RQ2 Can windowed HRV be substituted by fast biosignals for examining SPC in a natural protocol?
RQ3 Can the physiological signals with high temporal resolution, found to reflect SPC, be associated with task related emotional valence and engagement?
",,"In this paper we study novel means to characterize collaboration using SPC measured via noisy signals from ANS, in uncontrolled natural settings.",paied programming ,," Results suggest that (a) we can isolate cognitive processes (mental workload) from confounding environmental effects, and (b) electrodermal signals show role-specific but correlated affective response profiles. ","G= 14, I=28",,ecological,,,,,
143,149,,"Toward Automated Detection of Phase Changes in Team Collaboration
",zoe," how the BERT NLP model performs in a teamcommunicationcategorization task, in comparison to ground truth measures.",,,collaborative problem solving,"1. experimenter-participants
2. participants only","Results suggest  BERT’s  capabilitiesat  phase  change  detectionare promisingfor  experienced  teams,  though  further  iteration  is needed on the methods in the current study","G=4, I =8",,lab,,,,,
144,150,,Modeling People's Focus of Attention,zoe,we present an approach to model focus of attention of participants in a meeting via hidden Markov models (HMM),,The ob jective of this research is to track the focus of attention of participants in a meeting.,,,"The system has achieved
an accuracy rate of up to 93 % in detecting focus of
attention on test sequences taken from meetings.","G=1, I =4",,ecological,,,,,
145,151,,,kavie,"1. if compliance in electrodermal activity EDA , heart rate or breathing in two-person teams was predictive of team performance or coordination in a continuous tracking task simulating teleoperation.",,The overall goal of the present research on physiological compliance was to develop an objective means to assess team work andor team coordination that could be used to guide human factors design and the evaluation of sociotechnical systems.,simulate telemanipulation of heavy objects under microgravity condition,"1. visual contact
2. no visual contact",Heart cross correlation showed the strongest predictive relationships. These results provide evidence that physiological compliance among team members may benefit team performance.,G=16; I=32,,lab,,,,,
146,152,,,kavie,"Based on different rules of combination of metrics including talking time, Gini coefficient of verbal participation, number of touches, and the gini coefficient of the quantity of touches, different modes of group collaboration can be modeled. ",,Our approach exploits the digital and audio footprints of the users' actions at collocated settings to automatically build a model of symmetry of activity.,create concept map,,Our evaluation demonstrates that both amount and symmetry of verbal and physical participation are good indicators of collaborative and non-collaborative moments.,G=19; I=57,,ecological,,,,,
147,153,,,kavie,"1. We will establish a computational model for the estimation of conversational dominance based on automatically obtained audio and visual data, and we will propose a fully automatic dominance estimation method.",,This paper proposes a method for estimating the conversational dominance of participants in group interactions.,discuss and decide where participants would like to go out to enjoy a weekend,,"By exploiting the speech and gaze data as estimation parameters, we created a regression model to estimate conversational dominance, and the multiple correlation coefficient of this model was 0.85.",G=3; I=12,,lab,,,,,
