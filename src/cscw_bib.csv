"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"9JHRQB6R","journalArticle","2014","Montague, Enid; Xu, Jie; Chiou, Erin","Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters","IEEE Transactions on Human-Machine Systems","","2168-2291, 2168-2305","10.1109/THMS.2014.2325859","http://ieeexplore.ieee.org/document/6837486/","The aim of this study is to examine the utility of physiological compliance (PC) to understand shared experience in a multiuser technological environment involving active and passive users. Common ground is critical for effective collaboration and important for multiuser technological systems that include passive users since this kind of user typically does not have control over the technology being used. An experiment was conducted with 48 participants who worked in two-person groups in a multitask environment under varied task and technology conditions. Indicators of PC were measured from participants’ cardiovascular and electrodermal activities. The relationship between these PC indicators and collaboration outcomes, such as performance and subjective perception of the system, was explored. Results indicate that PC is related to group performance after controlling for task/technology conditions. PC is also correlated with shared perceptions of trust in technology among group members. PC is a useful tool for monitoring group processes and, thus, can be valuable for the design of collaborative systems. This study has implications for understanding effective collaboration.","2014-10","2020-11-01 17:39:04","2020-11-01 17:39:04","2020-11-01 17:39:04","614-624","","5","44","","IEEE Trans. Human-Mach. Syst.","Shared Experiences of Technology and Trust","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/XZ9T9G6H/Montague et al. - 2014 - Shared Experiences of Technology and Trust An Exp.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"92GJFNHL","bookSection","2020","Chng, Edwin; Seyam, Mohamed Raouf; Yao, William; Schneider, Bertrand","Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs","Artificial Intelligence in Education","978-3-030-52236-0 978-3-030-52237-7","","","http://link.springer.com/10.1007/978-3-030-52237-7_10","Open-ended learning environments such as makerspaces present a unique challenge for instructors. While it is expected that students are given free rein to work on their projects, facilitators have to strike a difﬁcult balance between micromanaging them and letting the community support itself. In this paper, we explore how Kinect sensors can continuously monitor students’ collaborative interactions so that instructors can gain a more comprehensive view of the social dynamics of the space. We employ heatmaps to examine the diversity of student collaborative interactions and Markov transition probabilities to explore the transitions between instances of collaborative interactions. Findings indicate that letting students work on their own promotes the development of technical skills, while working together encourages students to spend more time in the makerspace. This conﬁrms the intuition that successful projects in makerspaces necessitate both individual and group efforts. Furthermore, such aggregation and display of information can aid instructors in uncovering the state of student learning in makerspaces. Identifying the instances and diversity of collaborative interactions affords instructors an early opportunity to identify struggling students and having these data in a near real-time manner opens new doors in terms of making (un)productive behaviors salient, both for teachers and students. We discuss how this work represents a ﬁrst step toward using intelligent systems to support student learning in makerspaces.","2020","2020-11-01 17:36:36","2020-11-01 17:36:36","2020-11-01 17:36:36","118-128","","","12163","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-52237-7_10","","/Users/htk/Zotero/storage/7DC8MJ4V/Chng et al. - 2020 - Using Motion Sensors to Understand Collaborative I.pdf","","","","Bittencourt, Ig Ibert; Cukurova, Mutlu; Muldner, Kasia; Luckin, Rose; Millán, Eva","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UTD8FH2K","bookSection","2011","Martinez, Roberto; Wallace, James R.; Kay, Judy; Yacef, Kalina","Modelling and Identifying Collaborative Situations in a Collocated Multi-display Groupware Setting","Artificial Intelligence in Education","978-3-642-21868-2 978-3-642-21869-9","","","http://link.springer.com/10.1007/978-3-642-21869-9_27","Detecting the presence or absence of collaboration during group work is important for providing help and feedback during sessions. We propose an approach which automatically distinguishes between the times when a co-located group of learners, using a problem solving computer-based environment, is engaged in collaborative, non-collaborative or somewhat collaborative behaviour. We exploit the available data, audio and application log traces, to automatically infer useful aspects of the group collaboration and propose a set of features to code them. We then use a set of classifiers and evaluate whether their results accurately match the observations made on videorecordings. Results show up to 69.4% accuracy (depending on the classifier) and that the error rate for extreme misclassification (e.g. when a collaborative episode is classified as non-collaborative, or vice-versa) is less than 7.6%. We argue that this technique can be used to show the teacher and the learners an overview of the extent of their collaboration so they can become aware of it.","2011","2020-11-01 17:36:12","2020-11-01 17:36:12","2020-11-01 17:36:12","196-204","","","6738","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21869-9_27","","/Users/htk/Zotero/storage/C6BTV9UX/Martinez et al. - 2011 - Modelling and Identifying Collaborative Situations.pdf","","","","Biswas, Gautam; Bull, Susan; Kay, Judy; Mitrovic, Antonija","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A49T63VT","bookSection","2013","Martinez-Maldonado, Roberto; Kay, Judy; Yacef, Kalina","An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop","Artificial Intelligence in Education","978-3-642-39111-8 978-3-642-39112-5","","","http://link.springer.com/10.1007/978-3-642-39112-5_11","Learning to collaborate is important. But how does one learn to collaborate face-to-face? What are the actions and strategies to follow for a group of students who start a task? We analyse aspects of students' collaboration when working around a multi-touch tabletop enriched with sensors for identifying users, their actions and their verbal interactions. We provide a technological infrastructure to help understand how highly collaborative groups work compared to less collaborative ones. The contributions of this paper are (1) an automatic approach to distinguish, discover and distil salient common patterns of interaction within groups, by mining the logs of students’ tabletop touches and detected speech; and (2) the instantiation of this approach in a particular study. We use three data mining techniques: a classification model, sequence mining, and hierarchical clustering. We validated our approach in a study of 20 triads building solutions to a posed question at an interactive tabletop. We demonstrate that our approach can be used to discover patterns that may be associated with strategies that differentiate high and low collaboration groups.","2013","2020-11-01 17:36:10","2020-11-01 17:36:10","2020-11-01 17:36:10","101-110","","","7926","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39112-5_11","","/Users/htk/Zotero/storage/SCRS6TDX/Martinez-Maldonado et al. - 2013 - An Automatic Approach for Mining Patterns of Colla.pdf","","","","Lane, H. Chad; Yacef, Kalina; Mostow, Jack; Pavlik, Philip","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CVM8J4JU","conferencePaper","2016","Beyan, Cigdem; Carissimi, Nicolò; Capozzi, Francesca; Vascon, Sebastiano; Bustreo, Matteo; Pierro, Antonio; Becchio, Cristina; Murino, Vittorio","Detecting emergent leader in a meeting environment using nonverbal visual features only","Proceedings of the 18th ACM International Conference on Multimodal Interaction - ICMI 2016","978-1-4503-4556-9","","10.1145/2993148.2993175","http://dl.acm.org/citation.cfm?doid=2993148.2993175","In this paper, we propose an eﬀective method for emergent leader detection in meeting environments which is based on nonverbal visual features. Identifying emergent leader is an important issue for organizations. It is also a wellinvestigated topic in social psychology while a relatively new problem in social signal processing (SSP). The eﬀectiveness of nonverbal features have been shown by many previous SSP studies. In general, the nonverbal video-based features were not more eﬀective compared to audio-based features although, their fusion generally improved the overall performance. However, in absence of audio sensors, the accurate detection of social interactions is still crucial. Motivating from that, we propose novel, automatically extracted, nonverbal features to identify the emergent leadership. The extracted nonverbal features were based on automatically estimated visual focus of attention which is based on head pose. The evaluation of the proposed method and the deﬁned features were realized using a new dataset which is ﬁrstly introduced in this paper including its design, collection and annotation. The eﬀectiveness of the features and the method were also compared with many state of the art features and methods.","2016","2020-11-01 17:34:57","2020-11-01 17:34:58","2020-11-01 17:34:57","317-324","","","","","","","","","","","ACM Press","Tokyo, Japan","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/GW36MCSL/Beyan et al. - 2016 - Detecting emergent leader in a meeting environment.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 18th ACM International Conference","","","","","","","","","","","","","","",""
"P4652GKG","conferencePaper","2017","Beyan, Cigdem; Katsageorgiou, Vasiliki-Maria; Murino, Vittorio","Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose","Proceedings of the 2017 ACM on Multimedia Conference - MM '17","978-1-4503-4906-2","","10.1145/3123266.3123404","http://dl.acm.org/citation.cfm?doid=3123266.3123404","Detecting leadership while understanding the underlying behavior is an important research topic particularly for social and organizational psychology, and has started to get attention from social signal processing research community as well. It is known that, visual activity is a useful cue to investigate the social interactions, even though previously applied nonverbal features based on head/body actions were not performing well enough for identiﬁcation of emergent leaders (ELs) in small group meetings. Starting from these premises, in this study, we propose an eﬀective method that uses 2D body pose based nonverbal features to represent the visual activity of a person. Our results suggest that, i) overall, the proposed nonverbal features derived from body pose perform better than existing visual activity based features, ii) it is possible to improve classiﬁcation results by applying unsupervised feature learning as a preprocessing step, and iii) the proposed nonverbal features are able to advance the EL identiﬁcation performances of other types of nonverbal features when they are used together.","2017","2020-11-01 17:32:57","2020-11-01 17:32:57","2020-11-01 17:32:57","1425-1433","","","","","","Moving as a Leader","","","","","ACM Press","Mountain View, California, USA","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/D9J8BT53/Beyan et al. - 2017 - Moving as a Leader Detecting Emergent Leadership .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 2017 ACM","","","","","","","","","","","","","","",""
"Q7GTPXNI","journalArticle","2010","Terken, Jacques; Sturm, Janienke","Multimodal support for social dynamics in co-located meetings","Personal and Ubiquitous Computing","","","","","In this paper, we present a system that employsperceptual technologies (i.e. technologies that perceive thecontext through sensors such as cameras and microphone)to  provide  feedback  about  people’s  behaviour  in  smallgroup meetings. The system measures aspects of behaviourthat  are  relevant  to  the  social  dynamics  of  the  meeting,speaking  time  and  gaze  behaviour,  and  provides  visualfeedback   about   these   aspects   to   the   meeting   partici-pants through a peripheral display. We describe the systemproperties and the perceptual components. Also, we presenta study aimed at evaluating the effect of such a system onmeeting  behaviour.  Groups  of  participants,  amounting  to82 participants in all, discussed topics of general interest.Analysis  of  the  data  of  58  participants  showed  that  feed-back influenced the behaviour of the participants in such away  that it made over-participators speak less and under-participators speak more. Analysis of the micro-patterns ofsix participants indicated that feedback on gaze behaviourhad little effect on the interaction dynamics. We concludethat perceptual technologies can be used to build servicesthat  may  help  people  to  improve  their  meeting  skills  andwe  consider  some  ways  in  which  such  systems  may  bedeployed in meetings","2010","2020-11-01 16:57:51","2020-11-01 17:08:17","","703–714","","8","14","","","","","","","","","","","","","","","Google Scholar","","Publisher: Springer","","/Users/htk/Zotero/storage/QYQCCD4L/Terken and Sturm - 2010 - Multimodal support for social dynamics in co-locat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z9EBQHBX","conferencePaper","2011","Yamashita, Naomi; Kaji, Katsuhiko; Kuzuoka, Hideaki; Hirata, Keiji","Improving visibility of remote gestures in distributed tabletop collaboration","Proceedings of the ACM 2011 conference on Computer supported cooperative work","","","","","Collaborative distributed tabletop activities involving real objects are complicated by invisibility factors introduced into the workspace. In this paper, we propose a technique called ""remote lag"" to alleviate the problems caused by the invisibility of remote gestures. The technique provides people with instant playback of remote gestures to recover from the missed context of coordination. To examine the effects of the proposed technique, we studied four-person groups who engaged in two mentoring tasks using physical objects with and without remote lags. Our results show that remote lags effectively alleviated the invisibility problems, resulting in fewer questions/confirmations and redundant instructions during collaboration. The technique also decreased the overall workload of workers as well as the temporal demands for both helpers and workers.","2011","2020-08-03 23:03:46","2020-11-01 17:06:48","","95–104","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/UBZ74U86/Yamashita et al. - 2011 - Improving visibility of remote gestures in distrib.pdf; /Users/htk/Zotero/storage/L4H5PS6Y/1958824.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D5QAYY2S","conferencePaper","2018","Dich, Yong; Reilly, Joseph; Schneider, Bertrand","Using physiological synchrony as an indicator of collaboration quality, task performance and learning","International Conference on Artificial Intelligence in Education","","","","","Over the last decade, there has been a renewed interest in capturing 21st century skills using new data collection tools. In this paper, we leverage an existing dataset where multimodal sensors (mobile eye- trackers, motion sensors, galvanic skin response wristbands) were used to identify markers of productive collaborations. The data came from 42 pairs (N = 84) of participants who had no coding experience. They were asked to program a robot to solve a variety of mazes. We explored four different measures of physiological synchrony: Signal Matching (SM), Instantaneous Derivative Matching (IDM), Directional Agreement (DA) and Pearson’s Correlation (PC). Overall, we found PC to be positively associated with learning gains and DA with collaboration quality. We compare those results with prior studies and discuss implications for measuring collaborative process through physiological sensors.","2018","2020-11-01 16:56:53","2020-11-01 17:05:45","","98–110","","","","","","","","","","","Springer","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/NHY7FYJX/978-3-319-93843-1_8.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WX6DWT8B","conferencePaper","2020","Vrzakova, Hana; Amon, Mary Jean; Stewart, Angela; Duran, Nicholas D.; D'Mello, Sidney K.","Focused or Stuck Together: Multimodal Patterns Reveal Triads’ Performance in Collaborative Problem Solving","Proceedings of the Tenth International Conference on Learning Analytics & Knowledge","","","","","Collaborative problem solving (CPS) in virtual environments is an increasingly important context of 21st century learning. However, our understanding of this complex and dynamic phenomenon is still limited. Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS. We analyze two datasets where 116 triads collaboratively engaged in a challenging visual programming task using video conferencing software. We investigate how UI-interactions, behavioral primitives, and multimodal patterns were associated with teams' subjective and objective performance outcomes. We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants' subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives. We discuss how the findings can inform the design of real-time interventions for remote CPS.","2020","2020-11-01 16:56:24","2020-11-01 17:04:55","","295–304","","","","","","Focused or stuck together","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/4WBTMLD9/3375462.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7HCT5LM7","conferencePaper","2017","McDuff, Daniel; Thomas, Paul; Czerwinski, Mary; Craswell, Nick","Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results","Proceedings of the 19th ACM International Conference on Multimodal Interaction","","","","","Intelligent agents have the potential to help with many tasks. Information seeking and voice-enabled search assistants are becoming very common. However, there remain questions as to the extent by which these agents should sense and respond to emotional signals. We designed a set of information seeking tasks and recruited participants to complete them using a human intermediary. In total we collected data from 22 pairs of individuals, each completing five search tasks. The participants could communicate only using voice, over a VoIP service. Using automated methods we extracted facial action, voice prosody and linguistic features from the audio-visual recordings. We analyzed the characteristics of these interactions that correlated with successful communication and understanding between the pairs. We found that those who were expressive in channels that were missing from the communication channel (e.g., facial actions and gaze) were rated as communicating poorly, being less helpful and understanding. Having a way of reinstating nonverbal cues into these interactions would improve the experience, even when the tasks are purely information seeking exercises. The dataset used for this analysis contains over 15 hours of video, audio and transcripts and reported ratings. It is publicly available for researchers at: http://aka.ms/MISCv1.","2017","2020-11-01 16:55:57","2020-11-01 17:03:45","","456–463","","","","","","Multimodal analysis of vocal collaborative search","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/GV3UJHR9/3136755.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7UB2DMLS","journalArticle","2018","Samrose, Samiha; Zhao, Ru; White, Jeffery; Li, Vivian; Nova, Luis; Lu, Yichen; Ali, Mohammad Rafayet; Hoque, Mohammed Ehsan","Coco: Collaboration coach for understanding team dynamics during video conferencing","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","","","We present and discuss a fully-automated collaboration system, CoCo, that allows multiple participants to video chat and receive feedback through custom video conferencing software. After a conferencing session, a virtual feedback assistant provides insights on the conversation to participants. CoCo automatically pulls audial and visual data during conversations and analyzes the extracted streams for affective features, including smiles, engagement, attention, as well as speech overlap and turn-taking. We validated CoCo with 39 participants split into 10 groups. Participants played two back-to-back team-building games, Lost at Sea and Survival on the Moon, with the system providing feedback between the two. With feedback, we found a statistically significant change in balanced participation---that is, everyone spoke for an equal amount of time. There was also statistically significant improvement in participants' self-evaluations of conversational skills awareness, including how often they let others speak, as well as of teammates' conversational skills. The entire framework is available at https://github.com/ROC-HCI/CollaborationCoach_PostFeedback.","2018","2020-11-01 16:55:30","2020-11-01 17:02:04","","1–24","","4","1","","","Coco","","","","","","","","","","","","Google Scholar","","Publisher: ACM New York, NY, USA","","/Users/htk/Zotero/storage/KRBBSCI2/Samrose et al. - 2018 - Coco Collaboration coach for understanding team d.pdf; /Users/htk/Zotero/storage/Z9ZUMLX3/3161186.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HT96ATFA","conferencePaper","2016","Pijeira-Díaz, Héctor J.; Drachsler, Hendrik; Järvelä, Sanna; Kirschner, Paul A.","Investigating collaborative learning success with physiological coupling indices based on electrodermal activity","Proceedings of the sixth international conference on learning analytics & knowledge","","","","","Collaborative learning is considered a critical 21st century skill. Much is known about its contribution to learning, but still investigating a process of collaboration remains a challenge. This paper approaches the investigation on collaborative learning from a psychophysiological perspective. An experiment was set up to explore whether biosensors can play a role in analysing collaborative learning. On the one hand, we identified five physiological coupling indices (PCIs) found in the literature: 1) Signal Matching (SM), 2) Instantaneous Derivative Matching (IDM), 3) Directional Agreement (DA), 4) Pearson's correlation coefficient (PCC) and the 5) Fisher's z-transform (FZT) of the PCC. On the other hand, three collaborative learning measurements were used: 1) collaborative will (CW), 2) collaborative learning product (CLP) and 3) dual learning gain (DLG). Regression analyses showed that out of the five PCIs, IDM related the most to CW and was the best predictor of the CLP. Meanwhile, DA predicted DLG the best. These results play a role in determining informative collaboration measures for designing a learning analytics, biofeedback dashboard.","2016","2020-11-01 16:55:05","2020-11-01 17:01:12","","64–73","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/TJH8JQ9F/Pijeira-Díaz et al. - 2016 - Investigating collaborative learning success with .pdf; /Users/htk/Zotero/storage/GRCR9HZ2/2883851.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5JY5D3FK","journalArticle","2018","Cukurova, Mutlu; Luckin, Rose; Millán, Eva; Mavrikis, Manolis","The NISPI framework: Analysing collaborative problem-solving from students' physical interactions","Computers & Education","","","","","Collaborative problem-solving (CPS) is a fundamental skill for success in modern societies, and part of many common constructivist teaching approaches. However, its effective implementation and evaluation in both digital and physical learning environments are challenging for educators. This paper presents an original method for identifying differences in students' CPS behaviours when they are taking part in face-to-face practice-based learning (PBL). The dataset is based on high school and university students' hand position and head direction data, which can be automated deploying existing multimodal learning analytics systems. The framework uses Nonverbal Indexes of Students' Physical Interactivity (NISPI) to interpret the key parameters of students' CPS competence. The results show that the NISPI framework can be used to judge students' CPS competence levels accurately based on their non-verbal behaviour data. The findings have significant implications for design, research and development of educational technology.","2018","2020-11-01 16:54:44","2020-11-01 17:00:13","","93–109","","","116","","","The NISPI framework","","","","","","","","","","","","Google Scholar","","Publisher: Elsevier","","/Users/htk/Zotero/storage/T4XRVCUM/Cukurova et al. - 2018 - The NISPI framework Analysing collaborative probl.pdf; /Users/htk/Zotero/storage/WYPLME4U/S0360131517301938.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4EPCUNGB","journalArticle","2013","Schneider, Bertrand; Pea, Roy","Real-time mutual gaze perception enhances collaborative learning and collaboration quality","International Journal of Computer-supported collaborative learning","","","","","In this paper we present the results of an eye-tracking study on collaborative problem-solving dyads. Dyads remotely collaborated to learn from contrasting cases involving basic concepts about how the human brain processes visual information. In one condition, dyads saw the eye gazes of their partner on the screen; in a control group, they did not have access to this information. Results indicated that this real-time mutual gaze perception intervention helped students achieve a higher quality of collaboration and a higher learning gain. Implications for supporting group collaboration are discussed.","2013","2020-11-01 16:54:20","2020-11-01 16:59:18","","375–397","","4","8","","","","","","","","","","","","","","","Google Scholar","","Publisher: Springer","","/Users/htk/Zotero/storage/IN9SVHAH/s11412-013-9181-4.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SUF336HX","conferencePaper","2019","Miura, Go; Okada, Shogo","Task-independent Multimodal Prediction of Group Performance Based on Product Dimensions","2019 International Conference on Multimodal Interaction","","","","","This paper proposes an approach to develop models for predicting the performance for multiple group meeting tasks, where the model has no clear correct answer. This paper adopts ”product dimensions” [Hackman et al. 1967] (PD) which is proposed as a set of dimensions for describing the general properties of written passages that are generated by a group, as a metric measuring group output. This study enhanced the group discussion corpus called the MATRICS corpus including multiple discussion sessions by annotating the performance metric of PD. We extract group-level linguistic features including vocabulary level features using a word embedding technique, topic segmentation techniques, and functional features with dialog act and parts of speech on the word level. We also extracted nonverbal features from the speech turn, prosody, and head movement. With a corpus including multiple discussion data and an annotation of the group performance, we conduct two types of experiments thorough regression modeling to predict the PD. The first experiment is to evaluate the task-dependent prediction accuracy, in the situation that the samples obtained from the same discussion task are included in both the training and testing. The second experiments is to evaluate the task-independent prediction accuracy, in the situation that the type of discussion task is different between the training samples and testing samples. In this situation, regression models are developed to infer the performance in an unknown discussion task. The experimental results show that a support vector regression model archived a 0.76 correlation in the discussion-task-dependent setting and 0.55 in the task-independent setting.","2019","2020-11-01 16:52:16","2020-11-01 16:52:55","","264–273","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/5KHD8824/3340555.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VZXB6NE","journalArticle","2019","Malmberg, Jonna; Järvelä, Sanna; Holappa, Jukka; Haataja, Eetu; Huang, Xiaohua; Siipo, Antti","Going beyond what is visible: What multichannel data can reveal about interaction in the context of collaborative learning?","Computers in Human Behavior","","","","","Progress in the development of technology has provided data-capturing devices that make it possible to identify detailed processes in collaborative learning. This study utilized multichannel data, namely physiological data, video observations, and facial recognition data, to explore what they can reveal about types of interaction and regulation of learning during different phases of collaborative learning progress. Participants were five groups of three members each, selected for further study from an initial set of 48 students. The collaborative task was to design a healthy breakfast for an athlete. Empatica sensors were used to capture episodes of simultaneous arousal, and video observations were used to contextualize working phases and types of interaction. Facial expression data were created by post-processing video-recorded data. The results show that simultaneous arousal episodes occurred throughout phases of collaborative learning and the learners presented the most negative facial expressions during the simultaneous arousal episodes. Most of the collaborative interaction during simultaneous arousal was low-level, and regulated learning was not observable. However, when the interaction was high-level, markers of regulated learning were present; when the interaction was confused, it included monitoring activities. This study represents an advance in testing new methods for the objective measurement of social interaction and regulated learning in collaborative","2019","2020-08-24 22:17:12","2020-11-01 16:51:29","","235–245","","","96","","","Going beyond what is visible","","","","","","","","","","","","Google Scholar","","Publisher: Elsevier","","/Users/htk/Zotero/storage/BZ254G5A/Malmberg et al. - 2019 - Going beyond what is visible What multichannel da.pdf; /Users/htk/Zotero/storage/DVTEGF3C/S0747563218303078.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ETD7G9J","conferencePaper","2014","Grafsgaard, Joseph F.; Wiggins, Joseph B.; Vail, Alexandria Katarina; Boyer, Kristy Elizabeth; Wiebe, Eric N.; Lester, James C.","The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring","Proceedings of the 16th International Conference on Multimodal Interaction","","","","","","2014","2020-10-29 06:39:42","2020-10-29 06:42:12","","42-49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ATIFCZQV","conferencePaper","2017","Viswanathan, Sree Aurovindh; Vanlehn, Kurt","High accuracy detection of collaboration from log data and superficial speech features","International Society of the Learning Sciences","","","","","","2017","2020-10-29 06:33:42","2020-10-29 06:34:34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQ54E9J5","journalArticle","2013","Sanchez-Cortes, Dairazalia; Aran, Oya; Jayagopi, Dinesh Babu; Mast, Marianne Schmid; Gatica-Perez, Daniel","Emergent leaders through looking and speaking: from audio-visual data to multimodal recognition","Journal on Multimodal User Interfaces","","","","","","2013","2020-10-29 06:03:15","2020-10-29 06:30:29","","39-53","","7","1-2","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C4NNITIR","conferencePaper","2018","Starr, Emma L.; Reilly, Joseph M.; Schneider, Bertrand","Toward Using Multi-Modal Learning Analytics to Support and Measure Collaboration in Co-Located Dyads","International Society of the Learning Sciences, Inc.[ISLS","","","","","","2018","2020-10-29 06:01:38","2020-10-29 06:03:08","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"564SESTJ","conferencePaper","2019","Sharma, Kshitij; Olsen, Jennifer","An Alternate Statistical Lens to Look at Collaboration Data: Extreme Value Theory","International Conference on Computer Supported Collaborative Learning","","","","","","2019","2020-10-29 05:59:25","2020-10-29 06:01:19","","400-407","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAUJRBGA","conferencePaper","2012","Scherer, Stefan; Weibel, Nadir; Morency, Louis-Philippe; Oviatt, Sharon","Multimodal prediction of expertise and leadership in learning groups","Proceedings of the 1st International Workshop on Multimodal Learning Analytics","","","","","","2012","2020-10-29 05:55:02","2020-10-29 05:57:07","","1-8","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PAZFZXVD","conferencePaper","2017","Spikol, Daniel; Ruffaldi, Emanuele; Landolfi, Lorenzo; Cukurova, Mutlu","Estimation of success in collaborative learning based on multimodal learning analytics features","2017 IEEE 17th International Conference on Advanced Learning Technologies (ICALT)","","","","","","2017","2020-10-29 05:52:29","2020-10-29 05:54:25","","269-273","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XRXVG5VP","conferencePaper","2019","Schneider, Bertrand","Unpacking collaborative learning processes during hands-on activities using mobile eye-trackers","International Conference on Computer Supported Collaborative Learning","","","","","","2019","2020-10-29 05:49:18","2020-10-29 05:52:04","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"89UESW8M","conferencePaper","2015","Schneider, Bertrand; Sharma, Kshitij; Cuendet, Sébastien; Zufferey, Guillaume; Dillenbourg, Pierre; Pea, Roy D.","3D tangibles facilitate joint visual attention in dyads","International Society of the Learning Sciences, Inc.[ISLS]","","","","","","2015","2020-10-29 05:45:39","2020-10-29 05:48:23","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U6T9FBRI","conferencePaper","2018","Reilly, Joseph M.; Ravenell, Milan; Schneider, Bertrand","Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics","International Educational Data Mining Society","","","","","","2018","2020-10-29 05:41:11","2020-10-29 05:44:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGR775UK","journalArticle","2013","Martinez-Maldonado, Roberto; Dimitriadis, Yannis; Martinez-Monés, Alejandra; Kay, Judy; Yacef, Kalina","Capturing and analyzing verbal and physical collaborative learning interactions at an enriched interactive tabletop","International Journal of Computer-Supported Collaborative Learning","","1556-1607, 1556-1615","10.1007/s11412-013-9184-1","http://link.springer.com/10.1007/s11412-013-9184-1","Interactive tabletops can be used to provide new ways to support face-to-face collaborative learning. A little explored and somewhat hidden potential of these devices is that they can be used to enhance teachers’ awareness of students’ progress by exploiting captured traces of interaction. These data can make key aspects of collaboration visible and can highlight possible problems. In this paper, we explored the potential of an enriched tabletop to automatically and unobtrusively capture data from collaborative interactions. By analyzing that data, there was the potential to discover trends in students’ activity. These can help researchers, and eventually teachers, to become aware of the strategies followed by groups. We explored whether it was possible to differentiate groups, in terms of the extent of collaboration, by identifying the interwoven patterns of students’ speech and their physical actions on the interactive surface. The analysis was validated on a sample of 60 students, working in triads in a concept mapping learning activity. The contribution of this paper is an approach for analyzing students’ interactions around an enriched interactive tabletop that is validated through an empirical study that shows its operationalization to extract frequent patterns of collaborative activity.","2013-12","2020-08-29 09:11:10","2020-09-07 12:43:24","2020-08-29 09:11:10","455-485","","4","8","","Intern. J. Comput.-Support. Collab. Learn.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/YCFHAUAJ/Martinez-Maldonado et al. - 2013 - Capturing and analyzing verbal and physical collab.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N56SQ3YW","journalArticle","2016","Schneider, Bertrand; Sharma, Kshitij; Cuendet, Sébastien; Zufferey, Guillaume; Dillenbourg, Pierre; Pea, Roy","Detecting Collaborative Dynamics Using Mobile Eye-Trackers","International Society of the Learning Sciences","","","","","Prior work has successfully described how low and high-performing dyads of students differ in terms of their visual synchronization (e.g., Barron, 2000; Jermann, Mullins, Nuessli & Dillenbourg, 2011). But there is far less work analyzing the diversity of ways that successful groups of students use to achieve visual coordination. The goal of this paper is to illustrate how well-coordinated groups establish and sustain joint visual attention by unpacking their different strategies and behaviors. Our data was collected in a dual eyetracking setup where dyads of students (N=54) had to interact with a Tangible User Interface (TUI). We selected two groups of students displaying high levels of joint visual attention and compared them using cross-recurrence graphs displaying moments of joint attention from the eye-tracking data, speech data, and by qualitatively analyzing videos generated for that purpose. We found that greater insights can be found by augmenting cross-recurrence graphs with spatial and verbal data, and that high levels of joint visual attention can hide a free-rider effect (Salomon & Globerson, 1989). We conclude by discussing implications for automatically analyzing students’ interactions using dual eye-trackers.","2016","2020-08-29 08:45:09","2020-09-07 12:21:44","","8","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/htk/Zotero/storage/VY7SIKEL/Schneider et al. - Detecting Collaborative Dynamics Using Mobile Eye-.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4WM8L375","conferencePaper","2018","Olsen, J.; Sharma, K.; Aleven, V.; Rummel, N.","Combining gaze, dialogue, and action from a collaborative intelligent tutoring system to inform student learning processes","","","","","","In a computer supported collaborative learning environment, students have both interactions with each other as well as the technology that is guiding their learning, which can influence how the students construct their knowledge. Often in technology enhanced learning situations, information from the system provides discrete data points that can be used to infer learning without providing much information on the knowledge construction. On the other hand, analysis of student dialogues can be time consuming and subjective. In this paper, we propose combining log data, student dialogue, and gaze analysis to provide a clearer picture of how students construct knowledge collaboratively while working with an intelligent tutoring system. We found that students' gaze similarity is negatively correlated with levels of abstraction in speech and that students have higher gaze similarity surrounding feedback provided by the tutor. These results show that the gaze data can be used as a proxy for dialogue in a collaborative learning context.","2018","2020-08-29 08:41:43","2020-09-07 12:14:13","","","","","","","","","","","","","","","","","","","","","","","","/Users/htk/Zotero/storage/F8AKA7DY/Olsen-2018-Combining.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLS 2018 Proceedings","","","","","","","","","","","","","","",""
"26RPWXMQ","conferencePaper","2014","Schneider, Bertrand; Pea, Roy","The Effect of Mutual Gaze Perception on Students’ Verbal Coordination","","","","","","In a previous study, we found that real-time mutual gaze perception (i.e., being able to see the gaze of your partner in real time on a computer screen while solving a learning task) had a positive effect on students’ collaboration and learning [8]. The goals of this paper are to: 1) explore a variety of computational techniques for analyzing the transcripts of students’ discussions; 2) examine whether any of those measures sheds new light on our previous results; and 3) test whether those metrics have any predictive power regarding learning outcomes. Using various natural language processing algorithms, we found that linguistic coordination (i.e., the extent to which students mimic each other in terms of their grammatical structure) did not predict the quality of student collaboration or learning gains. However, we found that the coherence of students’ discourse was significantly different across our experimental conditions; this measure was positively correlated with their learning gains. Finally, using various language metrics, we were able to roughly (i.e., using a mediansplit) predict learning gains with a 94.4% accuracy using Support Vector Machine. The accuracy dropped to 75% when we used our model on a validation set. We conclude by discussing the benefits of using computational techniques on educational datasets.","2014","2020-08-29 08:38:52","2020-09-07 12:05:31","","7","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/htk/Zotero/storage/HKR9PNPT/Schneider and Pea - 2014 - The Effect of Mutual Gaze Perception on Students’ .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","7th International Conference on Educational Data Mining","","","","","","","","","","","","","","",""
"3EVLDBDB","conferencePaper","2011","Martinez, R; Yacef, K; Kay, J","Analysing frequent sequential patterns of collaborative learning activity around an interactive tabletop","","","","","","Electronic traces of activity have the potential to be an invaluable source to understand the strategies followed by groups of learners working collaboratively around a tabletop. However, in tabletop and other co-located learning settings, high amounts of unconstrained actions can be performed by different students simultaneously. This paper introduces a data mining approach that exploits the log traces of a problem-solving tabletop application to extract patterns of activity in order to shed light on the strategies followed by groups of learners. The objective of the data mining task is to discover which frequent sequences of actions differentiate high achieving from low achieving groups. An important challenge is to interpret the raw log traces, taking the user identification into account, and pre-process this data to make it suitable for mining and discovering meaningful patterns of interaction. We explore two methods for mining sequential patterns. We compare these two methods by evaluating the information that they each discover about the strategies followed by the high and low achieving groups. Our key contributions include the design of an approach to find frequent sequential patterns from multiuser co-located settings, the evaluation of the two methods, and the analysis of the results obtained from the sequential pattern mining.","2011","2020-08-29 08:34:08","2020-09-07 12:02:30","","10","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/htk/Zotero/storage/VSZP26GZ/Martinez et al. - Analysing frequent sequential patterns of collabor.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","4th International Conference on Educational Data Mining","","","","","","","","","","","","","","",""
"MMX4VLU4","conferencePaper","2019","Sriramulu, Abishek; Lin, Jionghao; Oviatt, Sharon","Dynamic Adaptive Gesturing Predicts Domain Expertise in Mathematics","2019 International Conference on Multimodal Interaction","978-1-4503-6860-5","","10.1145/3340555.3353726","https://dl.acm.org/doi/10.1145/3340555.3353726","Embodied Cognition theorists believe that mathematics thinking is embodied in physical activity, like gesturing while explaining math solutions. This research asks the question whether expertise in mathematics can be detected by analyzing students’ rate and type of manual gestures. The results reveal several unique findings, including that math experts reduced their total rate of gesturing by 50%, compared with non-experts. They also dynamically increased their rate of gesturing on harder problems. Although experts reduced their rate of gesturing overall, they selectively produced 62% more iconic gestures. Iconic gestures are strategic because they assist with retaining spatial information in working memory, so that inferences can be extracted to support correct problem solving. The present results on representation-level gesture patterns are convergent with recent findings on signal-level handwriting, while also contributing a causal understanding of how and why experts adapt their manual activity during problem solving.","2019-10-14","2020-08-29 09:05:50","2020-08-29 09:05:50","2020-08-29 09:05:50","105-113","","","","","","","","","","","ACM","Suzhou China","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/K7V288A8/Sriramulu et al. - 2019 - Dynamic Adaptive Gesturing Predicts Domain Experti.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICMI '19: INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION","","","","","","","","","","","","","","",""
"E8KUMDPM","conferencePaper","2015","Okada, Shogo; Aran, Oya; Gatica-Perez, Daniel","Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery","Proceedings of the 2015 ACM on International Conference on Multimodal Interaction - ICMI '15","978-1-4503-3912-4","","10.1145/2818346.2820757","http://dl.acm.org/citation.cfm?doid=2818346.2820757","This paper proposes a novel feature extraction framework from mutli-party multimodal conversation for inference of personality traits and emergent leadership. The proposed framework represents multi modal features as the combination of each participant’s nonverbal activity and group activity. This feature representation enables to compare the nonverbal patterns extracted from the participants of different groups in a metric space. It captures how the target member outputs nonverbal behavior observed in a group (e.g. the member speaks while all members move their body), and can be available for any kind of multiparty conversation task. Frequent cooccurrent events are discovered using graph clustering from multimodal sequences. The proposed framework is applied for the ELEA corpus which is an audio visual dataset collected from group meetings. We evaluate the framework for binary classiﬁcation task of 10 personality traits. Experimental results show that the model trained with co-occurrence features obtained higher accuracy than previously related work in 8 out of 10 traits. In addition, the cooccurrence features improve the accuracy from 2% up to 17%.","2015","2020-08-29 09:02:54","2020-08-29 09:02:54","2020-08-29 09:02:54","15-22","","","","","","","","","","","ACM Press","Seattle, Washington, USA","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/TUCSVUCF/Okada et al. - 2015 - Personality Trait Classification via Co-Occurrent .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 2015 ACM","","","","","","","","","","","","","","",""
"AGRCD7YI","conferencePaper","2013","Ochoa, Xavier; Chiluiza, Katherine; Méndez, Gonzalo; Luzardo, Gonzalo; Guamán, Bruno; Castells, James","Expertise estimation based on simple multimodal features","Proceedings of the 15th ACM on International conference on multimodal interaction - ICMI '13","978-1-4503-2129-7","","10.1145/2522848.2533789","http://dl.acm.org/citation.cfm?doid=2522848.2533789","Multimodal Learning Analytics is a ﬁeld that studies how to process learning data from dissimilar sources in order to automatically ﬁnd useful information to give feedback to the learning process. This work processes video, audio and pen strokes information included in the Math Data Corpus, a set of multimodal resources provided to the participants of the Second International Workshop on Multimodal Learning Analytics. The result of this processing is a set of simple features that could discriminate between experts and non-experts in groups of students solving mathematical problems. The main ﬁnding is that several of those simple features, namely the percentage of time that the students use the calculator, the speed at which the student writes or draws and the percentage of time that the student mentions numbers or mathematical terms, are good discriminators between experts and non-experts students. Precision levels of 63% are obtained for individual problems and up to 80% when full sessions (aggregation of 16 problems) are analyzed. While the results are speciﬁc for the recorded settings, the methodology used to obtain and analyze the features could be used to create discriminations models for other contexts.","2013","2020-08-29 09:00:15","2020-08-29 09:00:16","2020-08-29 09:00:15","583-590","","","","","","","","","","","ACM Press","Sydney, Australia","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/8RVQ6UBQ/Ochoa et al. - 2013 - Expertise estimation based on simple multimodal fe.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 15th ACM","","","","","","","","","","","","","","",""
"I8VCUMDF","journalArticle","2014","Schneider, Bertrand; Pea, Roy","Toward collaboration sensing","International Journal of Computer-Supported Collaborative Learning","","1556-1615","10.1007/s11412-014-9202-y","https://doi.org/10.1007/s11412-014-9202-y","We describe preliminary applications of network analysis techniques to eye-tracking data collected during a collaborative learning activity. This paper makes three contributions: first, we visualize collaborative eye-tracking data as networks, where the nodes of the graph represent fixations and edges represent saccades. We found that those representations can serve as starting points for formulating research questions and hypotheses about collaborative processes. Second, network metrics can be computed to interpret the properties of the graph and find proxies for the quality of students’ collaboration. We found that different characteristics of our graphs correlated with different aspects of students’ collaboration (for instance, the extent to which students reached consensus was associated with the average size of the strongly connected components of the graphs). Third, we used those characteristics to predict the quality of students’ collaboration by feeding those features into a machine-learning algorithm. We found that among the eight dimensions of collaboration that we considered, we were able to roughly predict (using a median-split) students’ quality of collaboration with an accuracy between ~85 and 100 %. We conclude by discussing implications for developing “collaboration-sensing” tools, and comment on implementing this approach for formal learning environments.","2014-12-01","2020-08-25 09:46:48","2020-08-25 09:46:48","2020-08-25 09:46:48","371-395","","4","9","","Intern. J. Comput.-Support. Collab. Learn.","","","","","","","","en","","","","","Springer Link","","","","/Users/htk/Zotero/storage/DT8ADIGI/Schneider and Pea - 2014 - Toward collaboration sensing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCD5GFRL","conferencePaper","2015","Schlösser, Christian; Schlieker-Steens, Philipp; Kienle, Andrea; Harrer, Andreas","Using Real-Time Gaze Based Awareness Methods to Enhance Collaboration","Collaboration and Technology","978-3-319-22747-4","","10.1007/978-3-319-22747-4_2","","Using eye-tracking in applications can be used to identify which areas are looked at by their users. In collaborative software this information can be transmitted to partners in real-time to provide an additional information channel. This paper compares different types of real-time gaze data visualizations. For this purpose, a study with three groups is conducted, who have to solve a collaborative puzzle. In every group the gaze data from each participant is recorded and visualized in a different way depending on the specific group condition. The aim is to evaluate a new context-based visualization to be able to make use of the known advantages of coordinate-based gaze data visualization outside of the domain of What-You-See-Is-What-I-See (WYSIWIS) interfaces.","2015","2020-08-25 09:23:39","2020-08-25 09:23:39","","19-27","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/htk/Zotero/storage/VHTGDJ6E/Schlösser et al. - 2015 - Using Real-Time Gaze Based Awareness Methods to En.pdf","","","Additional Information Channel; Gaze Data; Real-time Gaze; Shared Gaze; Specific Status Groups","Baloian, Nelson; Zorian, Yervant; Taslakian, Perouz; Shoukouryan, Samvel","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"84QZCW3V","journalArticle","2010","Winne, Philip H.","Improving measurements of self-regulated learning","Educational psychologist","","","","","","2010","2020-08-24 21:01:29","2020-08-24 21:01:30","","267–276","","4","45","","","","","","","","","","","","","","","Google Scholar","","Publisher: Taylor & Francis","","/Users/htk/Zotero/storage/56J2254P/00461520.2010.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5LBSZ57A","conferencePaper","2017","Spikol, Daniel; Ruffaldi, Emanuele; Cukurova, Mutlu","Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning","","","","","","Collaborative learning activities are a key part of education and are part of many common teaching approaches including problem-based learning, inquiry-based learning, and project-based learning. However, in open-ended collaborative small group work where learners make unique solutions to tasks that involve robotics, electronics, programming, and design artefacts evidence on the effectiveness of using these learning activities are hard to find. The paper argues that multimodal learning analytics (MMLA) can offer novel methods that can generate unique information about what happens when students are engaged in collaborative, project-based learning activities. Through the use of multimodal learning analytics platform, we collected various streams of data, processed and extracted multimodal interactions to answer the following question: which features of MMLA are good predictors of collaborative problem-solving in open-ended tasks in project-based learning? Manual entered scores of CPS were regressed using machine-learning methods. The answer to the question provides potential ways to automatically identify aspects of collaboration in projectbased learning.","2017","2020-08-24 08:56:39","2020-08-24 10:27:19","","8","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/htk/Zotero/storage/74FEQKFA/Spikol et al. - 2017 - Using Multimodal Learning Analytics to Identify As.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CSCL","","","","","","","","","","","","","","",""
"PPL3GZY2","conferencePaper","2016","Evans, Abigail C; Wobbrock, Jacob O; Davis, Katie","Modeling Collaboration Patterns on an Interactive Tabletop in a Classroom Setting","Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing - CSCW '16","978-1-4503-3592-8","","10.1145/2818048.2819972","http://dl.acm.org/citation.cfm?doid=2818048.2819972","Interaction logs generated by educational software can provide valuable insights into the collaborative learning process and identify opportunities for technology to provide adaptive assistance. Modeling collaborative learning processes at tabletop computers is challenging, as the computer is only able to log a portion of the collaboration, namely the touch events on the table. Our previous lab study with adults showed that patterns in a group’s touch interactions with a tabletop computer can reveal the quality of aspects of their collaborative process. We extend this understanding of the relationship between touch interactions and the collaborative process to adolescent learners in a field setting and demonstrate that the touch patterns reflect the quality of collaboration more broadly than previously thought, with accuracies up to 84.2%. We also present an approach to using the touch patterns to model the quality of collaboration in real-time.","2016","2020-08-24 10:27:19","2020-08-24 10:27:19","2020-08-24 10:27:19","858-869","","","","","","","","","","","ACM Press","San Francisco, California, USA","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/6KGZLEKT/Evans et al. - 2016 - Modeling Collaboration Patterns on an Interactive .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 19th ACM Conference","","","","","","","","","","","","","","",""
"3J5QG8FH","conferencePaper","2013","Schneider, Bertrand; Pea, Roy","Using Eye-Tracking Technology to Support Visual Coordination in Collaborative Problem-Solving Groups","CSCL 2013 Proceedings","","","","","In this paper we present the results of an eye-tracking study on collaborative problem-solving dyads. Dyads remotely worked on contrasting cases to study how the human brain processes visual information. In one condition, dyads saw the gaze of their partner on the screen; in a control group, they did not have access to this information. Results indicate that this real-time mutual gaze perception intervention helped students achieve a higher quality of collaboration and a higher learning gain. Implications for supporting group collaboration are discussed.","2013","2020-08-06 07:32:20","2020-08-06 08:09:17","","8","","","1","","","","","","","","","","en","","","","","Zotero","","","","/Users/htk/Zotero/storage/5PXBTARB/Schneider and Pea - 2013 - Using Eye-Tracking Technology to Support Visual Co.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PYY4DEAH","journalArticle","2015","Schneider, Bertrand; Pea, Roy","Does Seeing One Another’s Gaze Affect Group Dialogue? A Computational Approach","Journal of Learning Analytics","","1929-7750","10.18608/jla.2015.22.9","https://learning-analytics.info/index.php/JLA/article/view/4260","","2015-12-07","2020-08-04 07:32:24","2020-08-04 07:32:24","2020-08-04 07:32:24","107-133","","2","2","","Learning Analytics","Does Seeing One Another’s Gaze Affect Group Dialogue?","","","","","","","en","Copyright (c) 2015 Journal of Learning Analytics","","","","learning-analytics.info","","Number: 2","","/Users/htk/Zotero/storage/DHQ8D64T/Schneider and Pea - 2015 - Does Seeing One Another’s Gaze Affect Group Dialog.pdf; /Users/htk/Zotero/storage/JHCVHYWP/4260.html","","","CSCL","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GYCMKH55","conferencePaper","2019","Vrzakova, Hana; Amon, Mary Jean; Stewart, Angela EB; D'Mello, Sidney K.","Dynamics of visual attention in multiparty collaborative problem solving using multidimensional recurrence quantification analysis","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","","","","","","2019","2020-08-03 23:05:50","2020-08-03 23:05:50","","1–14","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/49NXEZU7/Vrzakova et al. - 2019 - Dynamics of visual attention in multiparty collabo.pdf; /Users/htk/Zotero/storage/4IYA9ACE/3290605.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LIVGN7JV","bookSection","2018","Xie, Bin; Reilly, Joseph M.; Dich, Yong Li; Schneider, Bertrand","Augmenting Qualitative Analyses of Collaborative Learning Groups Through Multi-Modal Sensing","","","","","","","2018","2020-08-03 23:05:31","2020-08-03 23:05:31","","","","","","","","","","","","","International Society of the Learning Sciences, Inc.[ISLS].","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/7E9GYZSE/910.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CFNYW3IJ","conferencePaper","2019","Muller, Philipp Matthias; Bulling, Andreas","Emergent Leadership Detection Across Datasets","2019 International Conference on Multimodal Interaction","","","","","","2019","2020-08-03 23:04:58","2020-08-03 23:04:58","","274–278","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/MXHXEUWK/Muller and Bulling - 2019 - Emergent Leadership Detection Across Datasets.pdf; /Users/htk/Zotero/storage/I87HBY4P/3340555.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LMQILYXB","conferencePaper","2016","Yoon, Dongwook; Chen, Nicholas; Randles, Bernie; Cheatle, Amy; Löckenhoff, Corinna E.; Jackson, Steven J.; Sellen, Abigail; Guimbretière, François","RichReview++ Deployment of a Collaborative Multi-modal Annotation System for Instructor Feedback and Peer Discussion","Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing","","","","","","2016","2020-08-03 23:02:59","2020-08-03 23:02:59","","195–205","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/MD2ZXL2H/Yoon et al. - 2016 - RichReview++ Deployment of a Collaborative Multi-m.pdf; /Users/htk/Zotero/storage/ZZKHC9UC/2818048.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"293DE5WH","conferencePaper","2018","D'Angelo, Sarah; Gergle, Darren","An eye for design: gaze visualizations for remote collaborative work","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","","","","","","2018","2020-08-03 23:01:36","2020-08-03 23:01:36","","1–12","","","","","","An eye for design","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/ZKLGYBN6/D'Angelo and Gergle - 2018 - An eye for design gaze visualizations for remote .pdf; /Users/htk/Zotero/storage/87Z3CNA3/3173574.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IARLSNWT","conferencePaper","2010","Li, Weifeng; Nüssli, Marc-Antoine; Jermann, Patrick","Gaze quality assisted automatic recognition of social contexts in collaborative Tetris","International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction","","","","","","2010","2020-08-03 23:01:05","2020-08-03 23:01:05","","1–8","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/XD5W2TZV/1891903.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X4NSWCLZ","journalArticle","2018","Schneider, Bertrand; Sharma, Kshitij; Cuendet, Sebastien; Zufferey, Guillaume; Dillenbourg, Pierre; Pea, Roy","Leveraging mobile eye-trackers to capture joint visual attention in co-located collaborative learning","International Journal of Computer-Supported Collaborative Learning","","","","","","2018","2020-08-03 23:00:28","2020-08-03 23:00:28","","241–261","","3","13","","","","","","","","","","","","","","","Google Scholar","","Publisher: Springer","","/Users/htk/Zotero/storage/4BH6VG5E/s11412-018-9281-2.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K42FU58F","bookSection","2015","Sharma, Kshitij; Caballero, Daniela; Verma, Himanshu; Jermann, Patrick; Dillenbourg, Pierre","Looking AT versus looking THROUGH: A dual eye-tracking study in MOOC context","","","","","","","2015","2020-08-03 23:00:04","2020-08-03 23:00:04","","","","","","","","Looking AT versus looking THROUGH","","","","","International Society of the Learning Sciences, Inc.[ISLS].","","","","","","","Google Scholar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJ9INWAR","conferencePaper","2011","Gergle, Darren; Clark, Alan T.","See What I’m Saying? Using Dyadic Mobile Eye Tracking to Study Collaborative Reference","Proceedings of the ACM 2011 conference on Computer supported cooperative work","","","","","","2011","2020-08-03 22:58:57","2020-08-03 22:58:57","","435–444","","","","","","See what I'm saying?","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/45YSJWNK/Gergle and Clark - 2011 - See what I'm saying Using dyadic mobile eye track.pdf; /Users/htk/Zotero/storage/DP9JV5N2/1958824.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EGNR5GCM","conferencePaper","2014","Grafsgaard, Joseph F.; Wiggins, Joseph B.; Vail, Alexandria Katarina; Boyer, Kristy Elizabeth; Wiebe, Eric N.; Lester, James C.","The additive value of multimodal features for predicting engagement, frustration, and learning during tutoring","Proceedings of the 16th International Conference on Multimodal Interaction","","","","","","2014","2020-08-03 22:57:02","2020-08-03 22:57:02","","42–49","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/Z949PVLM/Grafsgaard et al. - 2014 - The additive value of multimodal features for pred.pdf; /Users/htk/Zotero/storage/JYTKTHEE/2663204.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P24ZH4ED","conferencePaper","2016","D'Angelo, Sarah; Gergle, Darren","Gazed and confused: Understanding and designing shared gaze for remote collaboration","Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","","","","","","2016","2020-08-03 22:56:37","2020-08-03 22:56:37","","2492–2496","","","","","","Gazed and confused","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/W5AA9L9U/D'Angelo and Gergle - 2016 - Gazed and confused Understanding and designing sh.pdf; /Users/htk/Zotero/storage/LENZMS9Y/2858036.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G68JBG6M","conferencePaper","2016","Okada, Shogo; Ohtake, Yoshihiko; Nakano, Yukiko I.; Hayashi, Yuki; Huang, Hung-Hsuan; Takase, Yutaka; Nitta, Katsumi","Estimating communication skills using dialogue acts and nonverbal features in multiple discussion datasets","Proceedings of the 18th ACM International Conference on Multimodal Interaction","","","","","","2016","2020-08-03 22:55:26","2020-08-03 22:55:26","","169–176","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/7PWEDP3W/2993148.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQVKD4PS","conferencePaper","2012","Jayagopi, Dineshbabu; Sanchez-Cortes, Dairazalia; Otsuka, Kazuhiro; Yamato, Junji; Gatica-Perez, Daniel","Linking speaking and looking behavior patterns with group composition, perception, and performance","Proceedings of the 14th ACM international conference on Multimodal interaction","","","","","This paper addresses the task of mining typical behavioral patterns from small group face-to-face interactions and linking them to social-psychological group variables. Towards this goal, we define group speaking and looking cues by aggregating automatically extracted cues at the individual and dyadic levels. Then, we define a bag of nonverbal patterns (Bag-of-NVPs) to discretize the group cues. The topics learnt using the Latent Dirichlet Allocation (LDA) topic model are then interpreted by studying the correlations with group variables such as group composition, group interpersonal perception, and group performance. Our results show that both group behavior cues and topics have significant correlations with (and predictive information for) all the above variables. For our study, we use interactions with unacquainted members i.e. newly formed groups.","2012","2020-08-03 22:52:54","2020-08-03 22:53:49","","433–440","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/V9H8VE9V/Jayagopi et al. - 2012 - Linking speaking and looking behavior patterns wit.pdf; /Users/htk/Zotero/storage/UWX8E4CI/2388676.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V2EPYQDE","conferencePaper","2015","Nakano, Yukiko I.; Nihonyanagi, Sakiko; Takase, Yutaka; Hayashi, Yuki; Okada, Shogo","Predicting participation styles using co-occurrence patterns of nonverbal behaviors in collaborative learning","Proceedings of the 2015 ACM on International Conference on Multimodal Interaction","","","","","","2015","2020-08-03 22:51:46","2020-08-03 22:51:46","","91–98","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/32KRX66E/2818346.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTJG4MRL","conferencePaper","2018","Lin, Yun-Shao; Lee, Chi-Chun","Using Interlocutor-Modulated Attention BLSTM to Predict Personality Traits in Small Group Interaction","Proceedings of the 2018 on International Conference on Multimodal Interaction - ICMI '18","978-1-4503-5692-3","","10.1145/3242969.3243001","http://dl.acm.org/citation.cfm?doid=3242969.3243001","Small group interaction occurs often in workplace and education settings. Its dynamic progression is an essential factor in dictating the final group performance outcomes. The personality of each individual within the group is reflected in his/her interpersonal behaviors with other members of the group as they engage in these task-oriented interactions. In this work, we propose an interlocutor-modulated attention BSLTM (IM-aBLSTM) architecture that models an individual’s vocal behaviors during small group interactions in order to automatically infer his/her personality traits. The interlocutor-modulated attention mechanism jointly optimize the relevant interpersonal vocal behaviors of other members of group during interactions. In specifics, we evaluate our proposed IM-aBLSTM in one of the largest small group interaction database, the ELEA corpus. Our framework achieves a promising unweighted recall accuracy of 87.9% in ten different binary personality trait prediction tasks, which outperforms the best results previously reported on the same database by 10.4% absolute. Finally, by analyzing the interpersonal vocal behaviors in the region of high attention weights, we observe several distinct intra- and inter-personal vocal behavior patterns that vary as a function of personality traits.","2018","2020-08-03 13:25:23","2020-08-03 13:28:42","2020-08-03 13:25:23","163-169","","","","","","","","","","","ACM Press","Boulder, CO, USA","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/3T3YZZ9H/Lin and Lee - 2018 - Using Interlocutor-Modulated Attention BLSTM to Pr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2018 on International Conference on Multimodal Interaction - ICMI '18","","","","","","","","","","","","","","",""
"89FK7HQ7","conferencePaper","2010","Lepri, Bruno; Subramanian, Ramanathan; Kalimeri, Kyriaki; Staiano, Jacopo; Pianesi, Fabio; Sebe, Nicu","Employing social gaze and speaking activity for automatic determination of the extraversion trait","International conference on multimodal interfaces and the workshop on machine learning for multimodal interaction","","","","","In order to predict the Extraversion personality trait, we exploit medium-grained behaviors enacted in group meetings, namely, speaking time and social attention (social gaze). The latter will be further distinguished into attention given to the group members and attention received from them. The results of our work confirm many of our hypotheses: a) speaking time and (some forms of) social gaze are effective in automatically predicting Extraversion; b) classification accuracy is affected by the size of the time slices used for analysis, and c) to a large extent, the consideration of the social context does not add much to accuracy prediction, with an important exception concerning social gaze.","2010","2020-07-28 13:24:27","2020-07-28 13:24:37","","1–8","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/VG9GUCWQ/Lepri et al. - 2010 - Employing social gaze and speaking activity for au.pdf; /Users/htk/Zotero/storage/KW67G3WQ/1891903.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9R6ADCGK","conferencePaper","2008","Hung, Hayley; Jayagopi, Dinesh Babu; Ba, Sileye; Odobez, Jean-Marc; Gatica-Perez, Daniel","Investigating automatic dominance estimation in groups from visual attention and speaking activity","Proceedings of the 10th international conference on Multimodal interfaces","978-1-60558-198-9","","10.1145/1452392.1452441","http://doi.org/10.1145/1452392.1452441","We study the automation of the visual dominance ratio (VDR); a classic measure of displayed dominance in social psychology literature, which combines both gaze and speaking activity cues. The VDR is modified to estimate dominance in multi-party group discussions where natural verbal exchanges are possible and other visual targets such as a table and slide screen are present. Our findings suggest that fully automated versions of these measures can estimate effectively the most dominant person in a meeting and can match the dominance estimation performance when manual labels of visual attention are used.","2008-10-20","2020-07-27 09:20:18","2020-07-27 09:20:18","2020-07-27","233–236","","","","","","","ICMI '08","","","","Association for Computing Machinery","Chania, Crete, Greece","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/M3WF8REX/Hung et al. - 2008 - Investigating automatic dominance estimation in gr.pdf","","","audio-visual feature extraction; dominance modeling; meetings; visual focus of attention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CWV689GH","conferencePaper","2016","Fang, Sheng; Achard, Catherine; Dubuisson, Séverine","Personality classification and behaviour interpretation: an approach based on feature categories","Proceedings of the 18th ACM International Conference on Multimodal Interaction - ICMI 2016","978-1-4503-4556-9","","10.1145/2993148.2993201","http://dl.acm.org/citation.cfm?doid=2993148.2993201","This paper focuses on recognizing and understanding social dimensions (the personality traits and social impressions) during small group interactions. We extract a set of audio and visual features, which are divided into three categories: intra-personal features (i.e. related to only one participant), dyadic features (i.e. related to a pair of participants) and one vs all features (i.e. related to one participant versus the other members of the group). First, we predict the personality traits (PT) and social impressions (SI) by using these three feature categories. Then, we analyse the interplay between groups of features and the personality traits/social impressions of the interacting participants. The prediction is done by using Support Vector Machine and Ridge Regression which allows to determine the most dominant features for each social dimension. Our experiments show that the combination of intra-personal and one vs all features can greatly improve the prediction accuracy of personality traits and social impressions. Prediction accuracy reaches 81.37% for the social impression named ’Rank of Dominance’. Finally, we draw some interesting conclusions about the relationship between personality traits/social impressions and social features.","2016","2020-07-27 08:18:50","2020-07-27 08:18:50","2020-07-27 08:18:50","225-232","","","","","","Personality classification and behaviour interpretation","","","","","ACM Press","Tokyo, Japan","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/PQQRBCWP/Fang et al. - 2016 - Personality classification and behaviour interpret.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 18th ACM International Conference","","","","","","","","","","","","","","",""
"CHEGFJ6D","conferencePaper","2014","Lubold, Nichola; Pon-Barry, Heather","Acoustic-Prosodic Entrainment and Rapport in Collaborative Learning Dialogues","Proceedings of the 2014 ACM workshop on Multimodal Learning Analytics Workshop and Grand Challenge - MLA '14","978-1-4503-0488-7","","10.1145/2666633.2666635","http://dl.acm.org/citation.cfm?doid=2666633.2666635","In spoken dialogue analysis, the speech signal is a rich source of information. We explore in this paper how low level features of the speech signal, such as pitch, loudness, and speaking rate, can inform a model of student interaction in collaborative learning dialogues. For instance, can we observe the way that two people’s manners of speaking change over time to model something like rapport? By detecting interaction qualities such as rapport, we can better support collaborative interactions, which have been shown to be highly conducive to learning. For this, we focus on one particular phenomenon of spoken conversation, known as acoustic-prosodic entrainment, where dialogue partners become more similar to each other in their pitch, loudness, or speaking rate during the course of a conversation. We examine whether acoustic-prosodic entrainment is present in a novel corpus of collaborative learning dialogues, how people appear to entrain, to what degree, and report on the acoustic-prosodic features which people entrain on the most. We then investigate whether entrainment can facilitate detection of rapport, a social quality of the interaction. We ﬁnd that entrainment does correlate to rapport; speakers appear to entrain primarily by matching their prosody on a turn-by-turn basis, and pitch is the most signiﬁcant acoustic-prosodic feature people entrain on when rapport is present.","2014","2020-07-21 13:11:24","2020-07-21 13:11:24","2020-07-21 13:11:24","5-12","","","","","","","","","","","ACM Press","Istanbul, Turkey","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/XPTKEJW3/Lubold and Pon-Barry - 2014 - Acoustic-Prosodic Entrainment and Rapport in Colla.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 2014 ACM workshop","","","","","","","","","","","","","","",""
"EQPJ5L9B","conferencePaper","2018","Stewart, Angela E.B.; Keirn, Zachary A.; D'Mello, Sidney K.","Multimodal Modeling of Coordination and Coregulation Patterns in Speech Rate during Triadic Collaborative Problem Solving","Proceedings of the 2018 on International Conference on Multimodal Interaction  - ICMI '18","978-1-4503-5692-3","","10.1145/3242969.3242989","http://dl.acm.org/citation.cfm?doid=3242969.3242989","We model coordination and coregulation patterns in 33 triads engaged in collaboratively solving a challenging computer programming task for approximately 20 minutes. Our goal is to prospectively model speech rate (words/sec) – an important signal of turn taking and active participation – of one teammate (A or B or C) from time lagged nonverbal signals (speech rate and acoustic-prosodic features) of the other two (i.e., A + B → C; A + C → B; B + C → A) and task-related context features. We trained feed-forward neural networks (FFNNs) and long shortterm memory recurrent neural networks (LSTMs) using grouplevel nested cross-validation. LSTMs outperformed FFNNs and a chance baseline and could predict speech rate up to 6s into the future. A multimodal combination of speech rate, acousticprosodic, and task context features outperformed unimodal and bimodal signals. The extent to which the models could predict an individual’s speech rate was positively related to that individual’s scores on a subsequent posttest, suggesting a link between coordination/coregulation and collaborative learning outcomes. We discuss applications of the models for real-time systems that monitor the collaborative process and intervene to promote positive collaborative outcomes.","2018","2020-07-21 13:08:48","2020-07-21 13:08:48","2020-07-21 13:08:48","21-30","","","","","","","","","","","ACM Press","Boulder, CO, USA","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/QY4487K3/Stewart et al. - 2018 - Multimodal Modeling of Coordination and Coregulati.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 2018","","","","","","","","","","","","","","",""
"6SK85LMG","journalArticle","2016","Schneider, Bertrand; Sharma, Kshitij; Cuendet, Sébastien; Zufferey, Guillaume; Dillenbourg, Pierre; Pea, Roy","Using Mobile Eye-Trackers to Unpack the Perceptual Benefits of a Tangible User Interface for Collaborative Learning","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/3012009","http://doi.org/10.1145/3012009","In this study, we investigated the way users memorize, analyze, collaborate, and learn new concepts on a Tangible User Interface (TUI). Twenty-seven pairs of apprentices in logistics (N = 54) interacted with an interactive simulation of a warehouse. Their task was to discover efficient design principles for building storehouses. In a between-subjects experimental design, half of the participants used 3D physical shelves, whereas the other half used 2D paper shelves. This manipulation allowed us to control for the “representational effect” of 3D tangibles: the first group saw the warehouse as a small-scale model with realistic shelves, whereas the second group had access to a more abstract layout with rectangular pieces of paper. Both groups interacted with the system in the same way. We found that participants in the first group (i.e., who used 3D realistic shelves) better memorized a warehouse layout, built a more efficient model, and scored higher on a learning test. Additionally, students wore eye-tracking goggles while completing those tasks; preliminary results suggest that 3D interfaces increased joint visual attention, which was found to be a significant predictor for participants’ task performance and learning gains. Implications for designing TUIs in collaborative settings are discussed.","2016-12-09","2020-07-21 07:54:46","2020-07-21 07:54:46","2020-07-21 07:54:46","39:1–39:23","","6","23","","ACM Trans. Comput.-Hum. Interact.","","","","","","","","","","","","","December 2016","","","","/Users/htk/Zotero/storage/9KV4VPU6/Schneider et al. - 2016 - Using Mobile Eye-Trackers to Unpack the Perceptual.pdf","","","collaborative learning; eye-tracking; Tangible interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LATY86BK","conferencePaper","2016","Higuch, Keita; Yonetani, Ryo; Sato, Yoichi","Can Eye Help You?: Effects of Visualizing Eye Fixations on Remote Collaboration Scenarios for Physical Tasks","Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","978-1-4503-3362-7","","10.1145/2858036.2858438","https://dl.acm.org/doi/10.1145/2858036.2858438","In this work, we investigate how remote collaboration between a local worker and a remote collaborator will change if eye fixations of the collaborator are presented to the worker. We track the collaborator’s points of gaze on a monitor screen displaying a physical workspace and visualize them onto the space by a projector or through an optical see-through headmounted display. Through a series of user studies, we have found the followings: 1) Eye fixations can serve as a fast and precise pointer to objects of the collaborator’s interest. 2) Eyes and other modalities, such as hand gestures and speech, are used differently for object identification and manipulation. 3) Eyes are used for explicit instructions only when they are combined with speech. 4) The worker can predict some intentions of the collaborator such as his/her current interest and next instruction.","2016","2020-07-20 08:00:32","2020-07-20 08:07:18","2020-07-20 08:00:32","5180-5190","","","","","","Can Eye Help You?","","","","","ACM","San Jose California USA","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/8J392LWX/Higuch et al. - 2016 - Can Eye Help You Effects of Visualizing Eye Fixa.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CHI'16: CHI Conference on Human Factors in Computing Systems","","","","","","","","","","","","","","",""
"7C9A8ILX","conferencePaper","2018","Bhattacharya, Indrani; Foley, Michael; Zhang, Ni; Zhang, Tongtao; Ku, Christine; Mine, Cameron; Ji, Heng; Riedl, Christoph; Welles, Brooke Foucault; Radke, Richard J.","A multimodal-sensor-enabled room for unobtrusive group meeting analysis","Proceedings of the 20th ACM International Conference on Multimodal Interaction","","","","","Group meetings can suffer from serious problems that undermineperformance, including bias, “groupthink"", fear of speaking, andunfocused discussion. To better understand these issues, proposeinterventions, and thus improve team performance, we need tostudy human dynamics in group meetings. However, this processcurrently heavily depends on manual coding and video cameras.Manual coding is tedious, inaccurate, and subjective, while activevideo cameras can affect the natural behavior of meeting partici-pants. Here, we present a smart meeting room that combines mi-crophones and unobtrusive ceiling-mounted Time-of-Flight (ToF)sensors to understand group dynamics in team meetings. We auto-matically process the multimodal sensor outputs with signal, image,and natural language processing algorithms to estimate participanthead pose, visual focus of attention (VFOA), non-verbal speechpatterns, and discussion content. We derive metrics from theseautomatic estimates and correlate them with user-reported rank-ings of emergent group leaders and major contributors to produceaccurate predictors. We validate our algorithms and report resultson a new dataset of lunar survival tasks of 36 individuals across 10groups collected in the multimodal-sensor-enabled smart room.","2018","2020-06-30 13:57:39","2020-06-30 13:58:07","","347–355","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/I8RXHRBY/Bhattacharya et al. - 2018 - A multimodal-sensor-enabled room for unobtrusive g.pdf; /Users/htk/Zotero/storage/5WBXL9S4/3242969.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CLFY4AE3","conferencePaper","2017","D'Angelo, Sarah; Begel, Andrew","Improving communication between pair programmers using shared gaze awareness","Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","","","","","Remote collaboration can be more difficult than collocatedcollaboration for a number of reasons, including the inabil-ity to easily determine what your collaborator is looking at.This impedes a pair’s ability to efficiently communicate abouton-screen locations and makes synchronous coordination diffi-cult. We designed a novel gaze visualization for remote pairprogrammers which shows where in the code their partner iscurrently looking, and changes color when they are looking atthe same thing. Our design is unobtrusive, and transparentlydepicts the imprecision inherent in eye tracking technology.We evaluated our design with an experiment in which pairprogrammers worked remotely on code refactoring tasks. Ourresults show that with the visualization, pairs spent a greaterproportion of their time concurrently looking at the same codelocations. Pairs communicated using a larger ratio of implicitto explicit references, and were faster and more successful atresponding to those references.","2017","2020-06-30 13:56:06","2020-06-30 13:57:06","","6245–6290","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/CU4VH434/D'Angelo and Begel - 2017 - Improving communication between pair programmers u.pdf; /Users/htk/Zotero/storage/49DKRF3X/3025453.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGIUQ5JX","conferencePaper","2016","Bassiou, Nikoletta; Tsiartas, Andreas; Smith, Jennifer; Bratt, Harry; Richey, Colleen; Shriberg, Elizabeth; D'Angelo, Cynthia; Alozie, Nonye","Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration","INTERSPEECH","","","","","This work investigates whether nonlexical informationfrom speech can automatically predict the quality of small-group collaborations. Audio was collected from students as theycollaborated in groups of three to solve math problems. Expertsin education hand-annotated 30-second time windows for col-laboration quality. Speech activity features, computed at thegroup level, and spectral, temporal and prosodic features, ex-tracted at the speaker level, were explored. Fusion on featureswas also performed after transforming the later ones from thespeaker to the group level. Machine learning approaches usingSupport Vector Machines and Random Forests show that fea-ture fusion yields the best classification performance. The cor-responding unweighted averageF1measure on a 4-class pre-diction task ranges between40%and50%, much higher thanchance (12%). Speech activity features alone are also strongpredictors of collaboration quality achieving anF1measure thatranges between35%and43%. Spectral, temporal and prosodicfeatures alone achieve the lowest classification performance, butstill higher than chance, and exhibit considerable contribution tospeech activity feature performance as validated by the fusionresults. These novel findings illustrate that the approach understudy seems promising for monitoring of group dynamics andattractive in many collaboration activity settings where privacyis desired.","2016","2020-06-30 13:37:35","2020-06-30 13:38:09","","888–892","","","","","","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/74C7CH64/Bassiou et al. - 2016 - Privacy-Preserving Speech Analytics for Automatic .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BM8JBHT6","journalArticle","2017","Viswanathan, Sree Aurovindh; VanLehn, Kurt","Using the tablet gestures and speech of pairs of students to classify their collaboration","IEEE Transactions on Learning Technologies","","","","","ffective collaboration between student peers is not spontaneous. A system that can measure collaboration in real-timemay be useful, as it could alert an instructor to pairs that need help in collaborating effectively. We tested whether superficial measuresof speech and user interface actions would suffice for measuring collaboration. Pairs of students solved complex math problems whiledata were collected in the form of verbal interaction and user action logs from the students’ tablets. We distinguished four classificationsof interactivity: collaboration, cooperation, high asymmetric contribution and low asymmetric contribution. Human coders used richerdata (several video streams) to choose one of these codes for each episode. Thousands of features were extracted computationallyfrom the log and audio data. Machine learning was used to induce a detector that also assigned a code to each episode as a functionof these features. Detectors for combinations of codes were induced as well. The best detector’s overall accuracy was 96 percent(kappa¼0.92) compared to human coding. This high level of agreement suggests that superficial features of speech and log data dosuffice for measuring collaboration. However, these results should be viewed as preliminary because the particular task may havemade it relatively easy to distinguish collaboration from cooperation.","2017","2020-06-30 13:35:33","2020-06-30 13:36:38","","230–242","","2","11","","","","","","","","","","","","","","","Google Scholar","","Publisher: IEEE","","/Users/htk/Zotero/storage/3Z6DYXI7/Viswanathan and VanLehn - 2017 - Using the tablet gestures and speech of pairs of s.pdf; /Users/htk/Zotero/storage/6YNE9TD3/7927754.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HAYG5WSP","conferencePaper","2019","Eloy, Lucca; Stewart, Angela E.B.; Amon, Mary J.; Reinhardt, Caroline; Michaels, Amanda; Chen, Sun; Shute, Valerie; Duran, Nicholas D.; D'Mello, Sidney K.","Modeling Team-level Multimodal Dynamics during Multiparty Collaboration","2019 International Conference on Multimodal Interaction","978-1-4503-6860-5","","10.1145/3340555.3353748","https://dl.acm.org/doi/10.1145/3340555.3353748","We adopt a multimodal approach to investigating team interactions in the context of remote collaborative problem solving (CPS). Our goal is to understand multimodal patterns that emerge and their relation with collaborative outcomes. We measured speech rate, body movement, and galvanic skin response from 101 triads (303 participants) who used video conferencing software to collaboratively solve challenging levels in an educational physics game. We use multi-dimensional recurrence quantification analysis (MdRQA) to quantify patterns of team-level regularity, or repeated patterns of activity in these three modalities. We  found that teams exhibit significant regularity above chance baselines. Regularity was unaffected by task factors. but had a quadratic relationship with session time in that it initially increased but then decreased as the session progressed. Importantly, teams that produce more varied behavioral patterns (irregularity) reported higher emotional valence and performed better on a subset of the problem solving tasks. Regularity did not predict arousal or subjective perceptions of the collaboration. We discuss implications of our findings for the design of systems that aim to improve collaborative outcomes by monitoring the ongoing collaboration and intervening accordingly.","2019-10-14","2020-06-29 07:52:55","2020-06-29 07:58:14","2020-06-29 07:52:55","244-258","","","","","","","","","","","ACM","Suzhou China","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/LMCJFJ76/Eloy et al. - 2019 - Modeling Team-level Multimodal Dynamics during Mul.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICMI '19: INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION","","","","","","","","","","","","","","",""
"CGWXXWVN","conferencePaper","2017","Chikersal, Prerna; Tomprou, Maria; Kim, Young Ji; Woolley, Anita Williams; Dabbish, Laura","Deep Structures of Collaboration: Physiological Correlates of Collective Intelligence and Group Satisfaction","Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing","978-1-4503-4335-0","","10.1145/2998181.2998250","https://dl.acm.org/doi/10.1145/2998181.2998250","Collective intelligence (CI), a group’s capacity to perform a wide variety of tasks, is a key factor in successful collaboration. Group composition, particularly diversity and member social perceptiveness, are consistent predictors of CI, but we have limited knowledge about the mechanisms underlying their effects. To address this gap, we examine how physiological synchrony, as an indicator of coordination and rapport, relates to CI in computermediated teams, and if synchrony might serve as a mechanism explaining the effect of group composition on CI. We present results from a laboratory experiment where 60 dyads completed the Test of Collective Intelligence (TCI) together online and rated their group satisfaction, while wearing physiological sensors. We find that synchrony in facial expressions (indicative of shared experience) was associated with CI and synchrony in electrodermal activity (indicative of shared arousal) with group satisfaction. Furthermore, various forms of synchrony mediated the effect of member diversity and social perceptiveness on CI and group satisfaction. Our results have important implications for online collaborations and distributed teams.","2017-02-25","2020-06-29 07:46:47","2020-06-29 07:46:47","2020-06-29 07:46:47","873-888","","","","","","Deep Structures of Collaboration","","","","","ACM","Portland Oregon USA","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/VEBJTX7L/Chikersal et al. - 2017 - Deep Structures of Collaboration Physiological Co.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CSCW '17: Computer Supported Cooperative Work and Social Computing","","","","","","","","","","","","","","",""
"LFTIPIWV","journalArticle","2020","Schneider, Bertrand; Dich, Yong; Radu, Iulian","Unpacking the Relationship between Existing and New Measures of Physiological Synchrony and Collaborative Learning: A Mixed Methods Study","International Journal of Computer-Supported Collaborative Learning","","1556-1607","10.1007/s11412-020-09318-2","","Over the last decade, there has been a renewed interest in capturing twenty-first century skills using new data collection tools. In this article, we leverage an existing dataset where electrodermal activity (EDA) was used to identify markers of productive collaboration. The data came from 42 pairs of participants (N = 84) who had no coding experience and were asked to program a robot to solve a variety of mazes. Because little is known on how physiological synchrony relates to collaborative learning, we explored four different measures of synchrony: Signal Matching (SM), Instantaneous Derivative Matching (IDM), Directional Agreement (DA) and Pearson's Correlation (PC). Overall, we found PC to be positively associated with learning gains (r = 0.35) and DA with collaboration quality (r = 0.3). To gain further insights into these results, we also qualitatively analyzed two groups and identified situations with high or low physiological synchrony. We observed higher synchrony values when members of a productive group reacted to an external event (e.g., following instructions, receiving a hint), oscillations when they were watching a video or interacting with each other, and lower values when they were programming and / or seem to be confused. Based on these results, we developed a new measure of collaboration using electrodermal data: we computed the number of cycles between low and high synchronization. We found this measure to be significantly correlated with collaboration quality (r = 0.57) and learning gains (r = 0.47). This measure was not significantly correlated with the measures of physiological synchrony mentioned above, suggesting that it is capturing a different construct. We compare those results with prior studies and discuss implications for measuring collaborative process through physiological sensors.","2020-03","2020-06-28 08:35:24","2020-06-28 08:35:24","2020-06-28 08:35:24","89-113","","1","15","","","Unpacking the Relationship between Existing and New Measures of Physiological Synchrony and Collaborative Learning","","","","","","","en","","","","","ERIC","","Publisher: Springer","","/Users/htk/Zotero/storage/9FYU9DX8/eric.ed.gov.html","","","Data Collection; Coding; 21st Century Skills; Cooperative Learning; Educational Objectives; Group Dynamics; Measures (Individuals); Psychophysiology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5R95AA5K","journalArticle","2019","Riquelme, Fabian; Munoz, Roberto; Mac Lean, Roberto; Villarroel, Rodolfo; Barcelos, Thiago S.; de Albuquerque, Victor Hugo C.","Using multimodal learning analytics to study collaboration on discussion groups: A social network approach","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-019-00683-w","http://link.springer.com/10.1007/s10209-019-00683-w","Nowadays, companies and organizations require highly competitive professionals that have the necessary skills to confront new challenges. However, current evaluation techniques do not allow detection of skills that are valuable in the work environment, such as collaboration, teamwork, and effective communication. Multimodal learning analytics is a prominent discipline related to the analysis of several modalities of natural communication (e.g., speech, writing, gestures, sight) during educational processes. The main aim of this work is to develop a computational environment to both analyze and visualize student discussion groups working in a collaborative way to accomplish a task. ReSpeaker devices were used to collect speech data from students, and the collected data were modeled by using influence graphs. Three centrality measures were defined, namely permanence, persistence, and prompting, to measure the activity of each student and the influence exerted between them. As a proof of concept, we carried out a case study made up of 11 groups of undergraduate students that had to solve an engineering problem with everyday materials. Thus, we show that our system allows to find and visualize nontrivial information regarding interrelations between subjects in collaborative working groups; moreover, this information can help to support complex decision-making processes.","2019-08","2020-06-23 14:02:46","2020-06-23 14:02:46","2020-06-23 14:02:46","633-643","","3","18","","Univ Access Inf Soc","Using multimodal learning analytics to study collaboration on discussion groups","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/JHXPMEQU/Riquelme et al. - 2019 - Using multimodal learning analytics to study colla.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GWWNH57K","journalArticle","2019","Pijeira-Díaz, Héctor J.; Drachsler, Hendrik; Järvelä, Sanna; Kirschner, Paul A.","Sympathetic arousal commonalities and arousal contagion during collaborative learning: How attuned are triad members?","Computers in Human Behavior","","07475632","10.1016/j.chb.2018.11.008","https://linkinghub.elsevier.com/retrieve/pii/S0747563218305442","This article explores the dynamics of collaborative learning in the classroom from the perspective of the commonalities and interdependence in the degree of physiological activation from the sympathetic nervous system (i.e., sympathetic arousal) of group members. Using Empatica E4 wristbands, electrodermal activity—to derive arousal—was measured in 24 high school students working in groups of three (i.e., triads) during two runs of an advanced physics course. The participants met three times a week over six weeks for lessons of 75 min each. Most of the time (≈60–95% of the lesson) the triad members were at diﬀerent arousal levels, and, when they were on the same level, it was mainly the low arousal (or deactivated) level. Less than 4% of the time were the triad members simultaneously in high arousal. Possible within-triad arousal contagion cases (71.3%) occurred mostly on a one-to-one basis and with a latency from within a few seconds up to 10 min, but usually within 1 min. This study supports the view that only small parts of group work are collaborative, as far as the synchronicity and coordination which collaboration presupposes. Although exploratory, results also illustrate the aﬀordances of physiological measures to characterize collaborative processes.","2019-03","2020-06-23 13:58:19","2020-06-23 13:58:19","2020-06-23 13:58:19","188-197","","","92","","Computers in Human Behavior","Sympathetic arousal commonalities and arousal contagion during collaborative learning","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/B6ZLIYAP/Pijeira-Díaz et al. - 2019 - Sympathetic arousal commonalities and arousal cont.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JCSINBD","conferencePaper","2013","Luz, Saturnino","Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction","Proceedings of the 15th ACM on International conference on multimodal interaction - ICMI '13","978-1-4503-2129-7","","10.1145/2522848.2533788","http://dl.acm.org/citation.cfm?doid=2522848.2533788","An analysis of multiparty interaction in the problem solving sessions of the Multimodal Math Data Corpus is presented. The analysis focuses on non-verbal cues extracted from the audio tracks. Algorithms for expert identiﬁcation and performance prediction (correctness of solution) are implemented based on patterns of speech activity among session participants. Both of these categorisation algorithms employ an underlying graph-based representation of dialogues for each individual problem solving activities. The proposed Bayesian approach to expert prediction proved quite eﬀective, reaching accuracy levels of over 92% with as few as 6 dialogues of training data. Performance prediction was not quite as eﬀective. Although the simple graphmatching strategy employed for predicting incorrect solutions improved considerably over a Monte Carlo simulated baseline (F1 score increased by a factor of 2.3), there is still much room for improvement in this task.","2013","2020-06-23 13:56:13","2020-06-23 13:56:13","2020-06-23 13:56:13","575-582","","","","","","","","","","","ACM Press","Sydney, Australia","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/Q4IPZRR5/Luz - 2013 - Automatic identification of experts and performanc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 15th ACM","","","","","","","","","","","","","","",""
"INAAKGRG","conferencePaper","2012","Jermann, Patrick; Nüssli, Marc-Antoine","Effects of sharing text selections on gaze cross-recurrence and interaction quality in a pair programming task","Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work","978-1-4503-1086-4","","10.1145/2145204.2145371","http://doi.org/10.1145/2145204.2145371","We present a dual eye-tracking study that demonstrates the effect of sharing selection among collaborators in a remote pair-programming scenario. Forty pairs of engineering students completed several program understanding tasks while their gaze was synchronously recorded. The coupling of the programmers' focus of attention was measured by a cross-recurrence analysis of gaze that captures how much programmers look at the same sequence of spots within a short time span. A high level of gaze cross-recurrence is typical for pairs who actively engage in grounding efforts to build and maintain shared understanding. As part of their grounding efforts, programmers may use text selection to perform collaborative references. Broadcast selections serve as indexing sites for the selector as they attract non-selector's gaze shortly after they become visible. Gaze cross-recurrence is highest when selectors accompany their selections with speech to produce a multimodal reference.","2012-02-11","2020-06-23 13:30:20","2020-06-23 13:30:20","2020-06-23","1125–1134","","","","","","","CSCW '12","","","","Association for Computing Machinery","Seattle, Washington, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/A7MV4HCU/Jermann and Nüssli - 2012 - Effects of sharing text selections on gaze cross-r.pdf","","","dual eye tracking; gaze cross-recurrence; grounding; pair programming; pointing; reference; text selection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QHND6VA8","journalArticle","2017","Sharma, Kshitij; Leftheriotis, Ioannis; Noor, Jama; Giannakos, Michail","Dual Gaze as a Proxy for Collaboration in Informal Learning","","","","","https://repository.isls.org//handle/1/230","","2017-07","2020-06-23 09:15:13","2020-06-23 09:15:13","2020-06-23 09:15:13","","","","","","","","","","","","","","en","","","","","repository.isls.org","","Publisher: Philadelphia, PA: International Society of the Learning Sciences.","","/Users/htk/Zotero/storage/S8S8YHF7/Sharma et al. - 2017 - Dual Gaze as a Proxy for Collaboration in Informal.pdf; /Users/htk/Zotero/storage/UQRJ7J99/230.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6UWZNIU8","conferencePaper","2018","Murray, Gabriel; Oertel, Catharine","Predicting Group Performance in Task-Based Interaction","Proceedings of the 20th ACM International Conference on Multimodal Interaction","978-1-4503-5692-3","","10.1145/3242969.3243027","http://doi.org/10.1145/3242969.3243027","We address the problem of automatically predicting group performance on a task, using multimodal features derived from the group conversation. These include acoustic features extracted from the speech signal, and linguistic features derived from the conversation transcripts. Because much work on social signal processing has focused on nonverbal features such as voice prosody and gestures, we explicitly investigate whether features of linguistic content are useful for predicting group performance. The conclusion is that the best-performing models utilize both linguistic and acoustic features, and that linguistic features alone can also yield good performance on this task. Because there is a relatively small amount of task data available, we present experimental approaches using domain adaptation and a simple data augmentation method, both of which yield drastic improvements in predictive performance, compared with a target-only model.","2018-10-02","2020-06-23 08:59:40","2020-06-23 08:59:40","2020-06-23","14–20","","","","","","","ICMI '18","","","","Association for Computing Machinery","Boulder, CO, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/7ASAUTSW/Murray and Oertel - 2018 - Predicting Group Performance in Task-Based Interac.pdf","","","multimodal interaction; meetings; data augmentation; domain adaptation; group interaction; semi-supervised learning; social signal processing; task performance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YRUUZZWA","journalArticle","2019","Malmberg, Jonna; Haataja, Eetu; Seppänen, Tapio; Järvelä, Sanna","Are we together or not? The temporal interplay of monitoring, physiological arousal and physiological synchrony during a collaborative exam","International Journal of Computer-Supported Collaborative Learning","","1556-1615","10.1007/s11412-019-09311-4","https://doi.org/10.1007/s11412-019-09311-4","The coordination of cognitive and non-cognitive interactive processes contributes to successful collaboration in groups, but it is hard to evidence in computer-supported collaborative learning (CSCL). Monitoring is a metacognitive process that can be an indicator of a student’s ability to recognize success or failure in collaboration. This study focuses on how monitoring occurs in CSCL during a collaborative exam situation by examining how individual student contributions to monitoring processes are related to physiological synchrony and physiological arousal in groups. The participants were organized in four groups of three members each, and they wore sensors that measured their physiological activity. The data consist of video recordings from collaborative exam sessions lasting 90 minutes and physiological data captured from each student with Empatica 4.0 sensors. The video data were analyzed using qualitative content analysis to identify monitoring events. Students’ physiological arousal was determined through peak detection, and physiological concordance was used as an index for the students’ physiological synchrony. The individual and group level analysis investigated arousal and physiological synchrony in concordance with monitoring during the collaborative exam. The results showed that, in each group, each student contributed to joint monitoring. In addition, the monitoring activities exhibited a significant correlation with the arousal, indicating that monitoring events are reflected in physiological arousal. Physiological synchrony occurred within two groups, which experienced difficulties during the collaborative exam, whereas the two groups who had no physiological synchrony did not experience difficulties. It is concluded that physiological synchrony may be a new indicator for recognizing meaningful events in CSCL","2019-12-01","2020-06-23 08:47:56","2020-06-23 08:47:56","2020-06-23 08:47:56","467-490","","4","14","","Intern. J. Comput.-Support. Collab. Learn","Are we together or not?","","","","","","","en","","","","","Springer Link","","","","/Users/htk/Zotero/storage/22SUX5V4/Malmberg et al. - 2019 - Are we together or not The temporal interplay of .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SXFM7Q52","journalArticle","2010","Bachour, K; Kaplan, F; Dillenbourg, P","An Interactive Table for Supporting Participation Balance in Face-to-Face Collaborative Learning","IEEE Transactions on Learning Technologies","","1939-1382","10.1109/TLT.2010.18","http://www.computer.org/portal/web/csdl/abs/html/trans/lt/2010/03/tlt2010030203.htm","We describe an interactive table designed for supporting face-to-face collaborative learning. The table, Reflect, addresses the issue of unbalanced participation during group discussions. By displaying on its surface, a shared visualization of member participation, Reflect, is meant to encourage participants to avoid the extremes of over and underparticipation. We report on a user study that validates some of our hypotheses on the effect the table would have on its users. Namely, we show that Reflect leads to more balanced collaboration, but only under certain conditions. We also show different effects the table has on over and underparticipators.","2010-07","2020-06-22 09:29:58","2020-06-22 09:30:45","2012-09-09 00:48:19","203-213","","3","3","","","","","","","","","","","","","","","CrossRef","","","","/Users/htk/Zotero/storage/GEFB3P6B/tlt2010030203.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I9REDX9Z","journalArticle","2018","Andrist, Sean; Ruis, A.R.; Shaffer, David Williamson","A network analytic approach to gaze coordination during a collaborative task","Computers in Human Behavior","","07475632","10.1016/j.chb.2018.07.017","https://linkinghub.elsevier.com/retrieve/pii/S074756321830339X","A critical component of collaborative learning is the establishment of intersubjectivity, or the construction of mutual understanding. Collaborators coordinate their understanding with one another across various modes of communication, including speech, gesture, posture, and gaze. Given the dynamic, interdependent, and complex nature of coordination, this study sought to develop and test a method for constructing detailed and nuanced models of coordinated referential gaze patterns. In the study, 13 dyads participated in a simple collaborative task. We used dual mobile eye tracking to record each participant's gaze behavior, and we used epistemic network analysis (ENA) to model the gazes of both conversational participants synchronously. In the model, the nodes in the network represent gaze targets for each participant, and the connections between nodes indicate the likelihood of gaze coordination. Our analyses indicate: (a) properties and patterns of how gaze coordination unfolds throughout an interaction sequence; and (b) diﬀerences in gaze coordination patterns for interaction sequences that lead to breakdowns and repairs. In addition to contributing to the growing body of knowledge on the coordination of gaze behaviors in collaborative activities, this work suggests that ENA enables more eﬀective modeling of gaze coordination.","2018","2020-06-20 14:18:53","2020-06-20 14:19:42","2020-06-20 14:18:53","339-348","","","89","","Computers in Human Behavior","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/3T5XRHPU/Andrist et al. - 2018 - A network analytic approach to gaze coordination d.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WFUPKV3M","conferencePaper","2018","Kantharaju, Reshmashree B.; Ringeval, Fabien; Besacier, Laurent","Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals","Proceedings of the 2018 on International Conference on Multimodal Interaction - ICMI '18","978-1-4503-5692-3","","10.1145/3242969.3243012","http://dl.acm.org/citation.cfm?doid=3242969.3243012","Laughter is a highly spontaneous behavior that frequently occurs during social interactions. It serves as an expressive-communicative social signal which conveys a large spectrum of affect display. Even though many studies have been performed on the automatic recognition of laughter – or emotion – from audiovisual signals, very little is known about the automatic recognition of emotion conveyed by laughter. In this contribution, we provide insights on emotional laughter by extensive evaluations carried out on a corpus of dyadic spontaneous interactions, annotated with dimensional labels of emotion (arousal and valence). We evaluate, by automatic recognition experiments and correlation based analysis, how different categories of laughter, such as unvoiced laughter, voiced laughter, speech laughter, and speech (non-laughter) can be differentiated from audiovisual features, and to which extent they might convey different emotions. Results show that voiced laughter performed best in the automatic recognition of arousal and valence for both audio and visual features. The context of production is further analysed and results show that, acted and spontaneous expressions of laughter produced by a same person can be differentiated from audiovisual signals, and multilingual induced expressions can be differentiated from those produced during interactions.","2018","2020-06-20 12:14:14","2020-06-20 13:32:10","2020-06-20 12:14:14","220-228","","","","","","","","","","","ACM Press","Boulder, CO, USA","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/WUXP8LEL/Kantharaju et al. - 2018 - Automatic Recognition of Affective Laughter in Spo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2018 on International Conference on Multimodal Interaction - ICMI '18","","","","","","","","","","","","","","",""
"94PGKEMM","journalArticle","2014","Järvelä, Simo; Kivikangas, J. Matias; Kätsyri, Jari; Ravaja, Niklas","Physiological Linkage of Dyadic Gaming Experience","Simulation & Gaming","","1046-8781, 1552-826X","10.1177/1046878113513080","http://journals.sagepub.com/doi/10.1177/1046878113513080","Dyadic gaming experience was studied in a psychophysiological experiment where conflict structure and the presence of an artificial intelligence (AI) agent in a turnbased game were varied in four different conditions. Electrocardiographic and electrodermal activity signals of 41 same-gender dyads were recorded to study joint changes in their physiological signals. A strong physiological linkage was found within dyads in all conditions, but the linkage scores did not differentiate between conflict modes. The only significant difference in linkage between conditions was an increase when the AI agents were not present. In addition, linkage was associated with different self-report scales assessing social presence. These results suggest that social presence and physiological linkage within dyads are higher when dyads can focus on each others’ actions without distractions.","2014","2020-06-20 03:34:10","2020-06-20 03:35:16","2020-06-20 03:34:10","24-40","","1","45","","Simulation & Gaming","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/CXSVKNX3/Järvelä et al. - 2014 - Physiological Linkage of Dyadic Gaming Experience.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"INHMMIM4","journalArticle","","Dale, Rick; Bryant, Gregory A.; Manson, Joseph H.; Gervais, Matthew M.","Body synchrony in triadic interaction","Royal Society Open Science","","","10.1098/rsos.200095","https://royalsocietypublishing.org/doi/full/10.1098/rsos.200095","Humans subtly synchronize body movement during face-to-face conversation. In this context, bodily synchrony has been linked to affiliation and social bonding, task success and comprehension, and potential conflict. Almost all studies of conversational synchrony involve dyads, and relatively less is known about the structure of synchrony in groups larger than two. We conducted an optic flow analysis of body movement in triads engaged in face-to-face conversation, and explored a common measure of synchrony: time-aligned bodily covariation. We correlated this measure of synchrony with a diverse set of covariates related to the outcome of interactions. Triads showed higher maximum cross-correlation relative to a surrogate baseline, and ‘meta-synchrony’, in that composite dyads in a triad tended to show correlated structure. A windowed analysis also revealed that synchrony varies widely across an interaction. As in prior studies, average synchrony was low but statistically reliable in just a few minutes of interaction. In an exploratory analysis, we investigated the potential function of body synchrony by predicting it from various covariates, such as linguistic style matching, liking, laughter and cooperative play in a behavioural economic game. Exploratory results do not reveal a clear function for synchrony, though colaughter within triads was associated with greater body synchrony, and is consistent with an earlier analysis showing a positive connection between colaughter and cooperation. We end by discussing the importance of expanding and codifying analyses of synchrony and assessing its function.","","2020-09-30 06:31:36","2020-09-30 06:31:36","2020-09-30 06:31:36","200095","","9","7","","Royal Society Open Science","","","","","","","","","","","","","royalsocietypublishing.org (Atypon)","","Publisher: Royal Society","","/Users/htk/Zotero/storage/FWYDPI3I/Dale et al. - Body synchrony in triadic interaction.pdf; /Users/htk/Zotero/storage/XVKDI3GS/rsos.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K3FS3F2V","journalArticle","2020","Liu, Yang; Wang, Tingting; Wang, Kun; Zhang, Yu","Predicting Collaborative Learning Quality through Physiological Synchrony Recorded by Wearable Biosensors","bioRxiv","","","10.1101/2020.06.01.127449","https://www.biorxiv.org/content/10.1101/2020.06.01.127449v1","<h3>Abstract</h3> <p>Interpersonal physiological synchrony has been consistently found during collaborative tasks. However, few studies have applied synchrony to predict collaborative learning quality in real classroom. This study collected electrodermal activity (EDA) and heart rate (HR) in naturalistic class sessions, and compared the physiological synchrony between independent task and group discussion task. Since each student learn differently and not everyone prefers collaborative learning, participants were sorted into collaboration and independent dyads based on collaborative behaviors before data analysis. The result showed that during groups discussions, high collaboration pairs produced significantly higher synchrony than low collaboration dyads (<i>p</i> = 0.010). Given the equivalent engagement level during independent and collaborative tasks, the difference of physiological synchrony between high and low collaboration dyads was triggered by collaboration quality. Building upon this result, the classification analysis was conducted, indicating that EDA synchrony can predict collaboration quality (AUC = 0.767, <i>p</i> = 0.015).</p>","2020-06-01","2020-09-29 06:53:00","2020-09-29 06:53:00","2020-09-29 06:53:00","2020.06.01.127449","","","","","","","","","","","","","en","© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/","","","","www.biorxiv.org","","Publisher: Cold Spring Harbor Laboratory Section: Confirmatory Results","","/Users/htk/Zotero/storage/SI4I9SLR/Liu et al. - 2020 - Predicting Collaborative Learning Quality through .pdf; /Users/htk/Zotero/storage/DAFMJEAY/2020.06.01.127449v1.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JRWF34FC","journalArticle","2020","KÜTT, GRETE HELENA; TANPRASERT, TEERAPAUN; RODOLITZ, JAY; MOYZA, BERNARDO; SO, SAMUEL; KENDEROVA, GEORGIA; PAPOUTSAKI, ALEXANDRA","Effects of Shared Gaze on Audio-Versus Text-Based Remote Collaborations","","","","https://doi.org/10.1145/ 3415207","","","2020","2020-09-28 07:38:35","2020-09-28 07:39:47","","25","","","4","","Proc. ACM Hum.-Comput. Interact.","","","","","","","","","","","","","Google Scholar","","","","/Users/htk/Zotero/storage/3YZI65TM/KÜTT et al. - Effects of Shared Gaze on Audio-Versus Text-Based .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6HP3RWDQ","conferencePaper","2013","Ponce-López, Víctor; Escalera, Sergio; Baró, Xavier","Multi-modal social signal analysis for predicting agreement in conversation settings","Proceedings of the 15th ACM on International conference on multimodal interaction","978-1-4503-2129-7","","10.1145/2522848.2532594","http://doi.org/10.1145/2522848.2532594","In this paper we present a non-invasive ambient intelligence framework for the analysis of non-verbal communication applied to conversational settings. In particular, we apply feature extraction techniques to multi-modal audio-RGB-depth data. We compute a set of behavioral indicators that define communicative cues coming from the fields of psychology and observational methodology. We test our methodology over data captured in victim-offender mediation scenarios. Using different state-of-the-art classification approaches, our system achieve upon 75% of recognition predicting agreement among the parts involved in the conversations, using as ground truth the experts opinions.","2013-12-09","2020-09-27 07:14:38","2020-09-27 07:14:38","2020-09-27","495–502","","","","","","","ICMI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/TYZIWZPE/Ponce-López et al. - 2013 - Multi-modal social signal analysis for predicting .pdf","","","computer vision; machine learning; social signal processing; multi-modal human behavior analysis; pattern recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5QUJT98G","journalArticle","2017","Dikker, Suzanne; Wan, Lu; Davidesco, Ido; Kaggen, Lisa; Oostrik, Matthias; McClintock, James; Rowland, Jess; Michalareas, Georgios; Van Bavel, Jay J.; Ding, Mingzhou; Poeppel, David","Brain-to-Brain Synchrony Tracks Real-World Dynamic Group Interactions in the Classroom","Current Biology","","0960-9822","10.1016/j.cub.2017.04.002","http://www.sciencedirect.com/science/article/pii/S0960982217304116","The human brain has evolved for group living [1]. Yet we know so little about how it supports dynamic group interactions that the study of real-world social exchanges has been dubbed the “dark matter of social neuroscience” [2]. Recently, various studies have begun to approach this question by comparing brain responses of multiple individuals during a variety of (semi-naturalistic) tasks [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]. These experiments reveal how stimulus properties [13], individual differences [14], and contextual factors [15] may underpin similarities and differences in neural activity across people. However, most studies to date suffer from various limitations: they often lack direct face-to-face interaction between participants, are typically limited to dyads, do not investigate social dynamics across time, and, crucially, they rarely study social behavior under naturalistic circumstances. Here we extend such experimentation drastically, beyond dyads and beyond laboratory walls, to identify neural markers of group engagement during dynamic real-world group interactions. We used portable electroencephalogram (EEG) to simultaneously record brain activity from a class of 12 high school students over the course of a semester (11 classes) during regular classroom activities (Figures 1A–1C; Supplemental Experimental Procedures, section S1). A novel analysis technique to assess group-based neural coherence demonstrates that the extent to which brain activity is synchronized across students predicts both student class engagement and social dynamics. This suggests that brain-to-brain synchrony is a possible neural marker for dynamic social interactions, likely driven by shared attention mechanisms. This study validates a promising new method to investigate the neuroscience of group interactions in ecologically natural settings.","2017-05-08","2020-09-26 08:13:19","2020-09-26 08:13:19","2020-09-26 08:13:19","1375-1380","","9","27","","Current Biology","","","","","","","","en","","","","","ScienceDirect","","","","/Users/htk/Zotero/storage/4UYJ9EEU/Dikker et al. - 2017 - Brain-to-Brain Synchrony Tracks Real-World Dynamic.pdf; /Users/htk/Zotero/storage/IY9KMQDZ/S0960982217304116.html","","","brain synchrony; classroom engagement; educational neuroscience; group affinity; hyper-scanning; oscillations; portable EEG; real-world experimentation; social neuroscience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4LKWVKHP","journalArticle","2015","Behoora, Ishan; Tucker, Conrad S.","Machine learning classification of design team members' body language patterns for real time emotional state detection","Design Studies","","0142-694X","10.1016/j.destud.2015.04.003","http://www.sciencedirect.com/science/article/pii/S0142694X15000289","Design team interactions are one of the least understood aspects of the engineering design process. Given the integral role that designers play in the engineering design process, understanding the emotional states of individual design team members will help us quantify interpersonal interactions and how those interactions affect resulting design solutions. The methodology presented in this paper enables automated detection of individual team member's emotional states using non-wearable sensors. The methodology uses the link between body language and emotions to detect emotional states with accuracies above 98%. A case study involving human participants, enacting eight body language poses relevant to design teams, is used to illustrate the effectiveness of the methodology. This will enable researchers to further understand design team interactions.","2015-07-01","2020-09-25 07:16:44","2020-09-25 07:16:44","2020-09-25 07:16:44","100-127","","","39","","Design Studies","","","","","","","","en","","","","","ScienceDirect","","","","/Users/htk/Zotero/storage/WTGY7TR4/Behoora and Tucker - 2015 - Machine learning classification of design team mem.pdf; /Users/htk/Zotero/storage/WIPVPERW/S0142694X15000289.html","","","computational models; design activity; information processing; team work; user behavior","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTX4MTDJ","journalArticle","2016","Mønster, Dan; Håkonsson, Dorthe Døjbak; Eskildsen, Jacob Kjær; Wallot, Sebastian","Physiological evidence of interpersonal dynamics in a cooperative production task","Physiology & Behavior","","0031-9384","10.1016/j.physbeh.2016.01.004","http://www.sciencedirect.com/science/article/pii/S003193841630004X","Recent research suggests that shared behavioral dynamics during interpersonal interaction are indicative of subjective and objective outcomes of the interaction, such as feelings of rapport and success of performance. The role of shared physiological dynamics to quantify interpersonal interaction, however, has received comparatively little attention. In the present study, we investigate the coordination dynamics of multiple psychophysiological measures and their utility in capturing emotional dynamics in teams. We use data from an experiment where teams of three people built origami boats together in an assembly-line manner while their heart rate, skin conductance, and facial muscle activity were recorded. Our results show that physiological synchrony of skin conductance measures and eletromyographic measures of the corrugator supercilii develops spontaneously among team members during this cooperative production task. Moreover, high team synchrony is found indicative of team cohesion, while low team synchrony is found indicative of a teams' decision to adopt a new behavior across multiple production sessions. We conclude that team-level measures of synchrony offer new and complementary information compared to measures of individual levels of physiological activity.","2016-03-15","2020-09-23 08:17:15","2020-09-23 08:17:15","2020-09-23 08:17:15","24-34","","","156","","Physiology & Behavior","","","","","","","","en","","","","","ScienceDirect","","","","/Users/htk/Zotero/storage/GXKX2R23/Mønster et al. - 2016 - Physiological evidence of interpersonal dynamics i.pdf; /Users/htk/Zotero/storage/6CN5BM3F/S003193841630004X.html","","","Psychophysiology; Interpersonal dynamics; Recurrence quantification analysis; Synchrony","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YL3AK6TL","journalArticle","2020","Dindar, Muhterem; Järvelä, Sanna; Haataja, Eetu","What does physiological synchrony reveal about metacognitive experiences and group performance?","British Journal of Educational Technology","","1467-8535","10.1111/bjet.12981","https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1111/bjet.12981","There is a growing body of research on physiological synchrony (PS) in Collaborative Problem Solving (CPS). However, the current literature presents inconclusive findings about the way in which PS is reflected in cognitive and affective group processes and performance. In light of this, this study investigates the relationship between PS and metacognitive experiences (ie, judgement of confidence, task interest, task difficulty, mental effort and emotional valence) that are manifested during CPS. In addition, the study explores the association between PS and group performance. The participants were 77 university students who worked together on a computer-based CPS simulation in groups of three. Participants’ electrodermal activity (EDA) was recorded as they worked on the simulation and metacognitive experiences were measured with situated self-reports. A Multidimensional Recurrence Quantification Analysis was used to calculate the PS among the collaborators. The results show a positive relationship between continuous PS episodes and groups’ collective mental effort. No relationship was found between PS and judgement of confidence, task interest, task difficulty or emotional valence. The relationship between PS and group performance was also non-significant. The current work addresses several challenges in utilising multimodal data analytics in CPS research and discusses future research directions.","2020","2020-09-21 08:48:55","2020-09-21 08:48:55","2020-09-21 08:48:55","1577-1596","","5","51","","","","","","","","","","en","© 2020 British Educational Research Association","","","","Wiley Online Library","","_eprint: https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.12981","","/Users/htk/Zotero/storage/M49LJF8U/Dindar et al. - 2020 - What does physiological synchrony reveal about met.pdf; /Users/htk/Zotero/storage/X9Z9YGUU/bjet.html","","","problem solving; learning analytics; metacognition; multimodal(ity); self-regulated learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L32LQTVS","journalArticle","2006","Thomson, AM; Perry, JL","Collabration processes: Inside the black box","Public administration review","","","","","","2006","2021-11-23 20:48:43","2021-11-23 20:49:56","","20-32","","","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A38Z8CFI","journalArticle","2005","Salonen, Pekka; Vauras, Marja; Efklides, Anastasia","Social interaction-what can it tell us about metacognition and coregulation in learning?","European Psychologist","","","","","","2005","2021-10-27 15:41:51","2021-10-27 15:43:11","","199-208","","3","10","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9W37TDDD","journalArticle","2020","Dindar, Muhterem; Sanna, Jarvela; Hanna, Jarvenoja","Interplay of metacognitive experiences and performance in collaborative problem solving","Computers & Education","","","","","","2020","2021-10-27 15:39:55","2021-10-27 15:41:13","","","","","154","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQHKTSX4","journalArticle","1997","Barry, Bruce; Stewart, Greg","Composition, process, and performance in self-managed groups: The role of personality","Journal of Applied Psychology","","","","","","1997","2021-10-26 02:09:30","2021-10-26 02:11:19","","62","","1","82","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8ITLZH2","journalArticle","2012","Ogan, Amy; Finkelstein, Samantha; Walker, Erin; Carlson, Ryan; Cassell, Justine","Rudeness and rapport: Insults and learning gains in peer tutoring","International Conference on Intelligent Tutoring Systems","","","","","","2012","2021-10-26 01:56:39","2021-10-26 01:59:04","","11-21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q4NH2RU4","journalArticle","2009","Le Dantec, Christopher A.; Do, Ellen Yi-Luen","The mechanisms of value transfer in design meetings","Design Studies","","","","","","2009","2021-10-25 16:18:26","2021-10-25 16:19:35","","119-137","","2","30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KJPZX9L6","journalArticle","2001","Kelly, Janice R.; Barsade, Sigal","Mood and emotions in small groups and work teams","Organizational behavior and human decision processes","","","","","","2001","2021-10-25 16:15:29","2021-10-25 16:16:58","","99-130","","1","86","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FPJQTFSF","journalArticle","1989","Damon, William; Phelps, Erin","Critical distinctions among three approaches to peer education","International Journal of Educational Research","","","","","","1989","2021-09-27 21:36:15","2021-09-27 21:37:13","","9-19","","1","13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MNXLYIZP","journalArticle","1997","Barry, B; Stewart, G.L.","Composition, process and performance in self-managed groups: The role of personality","Journal of Applied Psychology","","","","","","1997","2021-09-27 21:33:58","2021-09-27 21:35:07","","62","","1","82","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RST5DIHZ","journalArticle","2014","Stroebe, Wolfgang; Strack, Fritz","The alleged crisis and the illusion of exact replication","Perspectives on Psychological Science","","","","","","2014","2021-09-27 21:31:28","2021-09-27 21:32:41","","59-71","","1","9","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCZZZIRM","journalArticle","2012","Ogan, Amy; Finkelstein, Samantha; Walker, Erin; Carlson, Ryan; Cassell, Justine","Rudeness and rapport: Insults and learning gains in peer tutoring","International Conference on Intelligent Tutoring Systems","","","","","","2012","2021-09-27 21:24:57","2021-09-27 21:26:09","","11-21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UCEVFZFS","journalArticle","2009","Le Dantec, Christopher; Yi-Luen Do, Ellen","The mechanisms of value transfer in design meetings","Design Studies","","","","","","2009","2021-09-27 21:22:32","2021-09-27 21:23:22","","119-137","","2","30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JGTVLBB","journalArticle","","Erkens, Gijsbert; Jaspers, Jos; Prangsma, Maaike; Kanselaar, Gellof","Computers in Human Behavior","Computers in Human Behavior","","","","","","","2021-09-27 21:17:05","2021-09-27 21:19:23","","463-486","","3","21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XGDH9KI2","journalArticle","1984","Lord, R.; Foti, R; Vader, C.D.","A test of leadership categorization theory: Internal structure, information processing, and leadership perceptions","Organizational behavior and human performance","","","","","","1984","2021-09-27 17:20:22","2021-09-27 17:21:39","","343-378","","3","34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QR7BCZGS","journalArticle","1989","Joann, Keyton; Wall, Victor D.","SYMLOG: Theory and method for measuring group and organizational communication","Management Communication Quarterly","","","","","","1989","2021-09-27 17:16:02","2021-09-27 17:17:27","","544-567","","","2.4","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JKWGF9VB","journalArticle","1998","Efklides, Anastasia; Papadaki, Maria; Papantoniou, Georgia; Kiosseoglou, Gregoris","Individual differences in feelings of difficulty: The case of school mathematics","European Journal of Psychology of Education","","","","","","1998","2021-09-27 17:10:39","2021-09-27 17:12:16","","207-226","","2","13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PL2BYX4T","journalArticle","2013","Tapola, Anna; Veermans, Marjanna; Niemivirta, Markku","Predictors and outcomes of situational interest during a science learning tasks","Instructional Science","","","","","","2013","2021-09-27 17:08:30","2021-09-27 17:09:37","","1047-1064","","6","41","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F5TZ74US","journalArticle","1992","Paas, Fred GWC","Training strategies for attaining transfer of problem-solving skill in statistics: a cognitive-load approach","Journal of education psychology","","","","","","1992","2021-09-27 17:06:39","2021-09-27 17:07:34","","429","","4","84","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U7CRK5ZR","journalArticle","2013","Manson, Joseph H.; Gregory, Bryant A.; Gervais, Matthew M.; Kline, Michelle A.","Convergence of speech rate in conversation predicts cooperation","Evolution and Human Behavior","","","","","","2013","2021-09-27 17:01:26","2021-09-27 17:02:55","","419-426","","6","34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRK3M4GY","journalArticle","1991","Pintrich, Paul","A manual for the use of the Motivated Strategies for Learning Questionnaire (MSLQ)","","","","","","","1991","2021-09-27 16:59:21","2021-09-27 17:00:05","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z53Q7PDQ","journalArticle","2013","Sanchez-Cortes, Dairazalia; Aran, Oya; Jayagopi, Dinesh Babu; Mast, Marianne Schmid; Gatica-Perez, Daniel","Emergent leaders through looking and speaking: from audio-visual data to multimodal recognition","Journal of Multimodal User Interfaces","","","","","","2013","2021-09-27 16:56:05","2021-09-27 16:57:52","","39-53","","","7.1","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IHM8YH3P","journalArticle","2013","Hadwin, Allyson F.; Webster, Elizabeth A.","Calibration in goal setting: Examining the nature of judgments of confidence","Learning and Instruction","","","","","","2013","2021-09-27 16:43:53","2021-09-27 16:45:11","","37-47","","","24","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8RMGN5HS","journalArticle","2005","Ainley, Mary; Corrigan, Matthew; Richardson, Nicholas","Students, tasks and emotions: Identifying the contribution of emotions to students' reading of popular culture and popular science texts","Learning and Instruction","","","","","","2005","2021-09-27 16:40:23","2021-09-27 16:41:58","","433-447","","5","15","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZT4RSILJ","journalArticle","2007","Yvonne A. W. de Kort; IJsselsteijn, Wijnand A.; Poels, Karolien","Digital games as social presence technology: Development of the Social Presence in Gaming Questionnaire (SPGQ)","Proceedings of PRESENCE","","","","","","2007","2021-09-27 16:33:31","2021-09-27 16:37:07","","1-9","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JU97NPF","journalArticle","2018","Worsley, Marcelo","(Dis)Engagement Matters: Identifying efficacious Learning Practices with Multimodal Learning Analytics","","","","","","Video analysis is a staple of the education research community. For many contemporary education researchers, participation in the video coding process serves as a rite of passage. However, recent developments in multimodal learning analytics may help to accelerate and enhance this process by providing researchers with a more nuanced glimpse into a set of learning experiences. As an example of how to use multimodal learning analytics towards these ends, this paper includes a preliminary analysis from 54 college students, who completed two engineering design tasks in pairs. Gesture, speech and electro-dermal activation data were collected as students completed these tasks. The gesture data was used to learn a set of canonical clusters (N=4). A decision tree was trained based on individual students’ cluster frequencies, and pre-post learning gains. The nodes in the decision tree were then used to identify a subset of video segments that were human coded based on prior work in learning analytics and engineering design. The combination of machine learning and human inference helps elucidate the practices that seem to correlate with student learning. In particular, both engagement and disengagement seem to correlate with student learning, albeit in a somewhat nuanced fashion.","2018","2021-08-27 06:33:16","2021-08-27 06:33:16","","5","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/htk/Zotero/storage/M9ZVUAGX/Worsley - 2018 - (Dis)Engagement Matters Identifying Eﬀicacious Le.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VXHH2C39","journalArticle","2019","Reilly, Joseph M; Schneider, Bertrand","Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse","","","","","","Collaborative problem solving in computer-supported environments is of critical importance to the modern workforce. Coworkers or collaborators must be able to co-create and navigate a shared problem space using discourse and non-verbal cues. Analyzing this discourse can give insights into how consensus is reached and can estimate the depth of their understanding of the problem. This study uses Coh-Metrix, a natural language processing tool that measures cohesion, to analyze participant discourse from a recent multi-modal learning analytics study where novice programmers collaborated to use a block-based programming language to instruct a robot on how to solve a series of mazes. We significantly correlated thirty-five Coh-Metrix indices from the transcripts of dyads' discourse with collaboration, learning gains, and multimodal sensor values. We then fit a variety of machine learning classifiers to predict collaboration using the indices generated by Coh-Metrix as features. This study paves the way for real-time detection of (un)productive interactions from multimodal data and could lead to real-time interventions to support collaborative learning.","2019","2021-08-27 05:56:48","2021-08-27 05:56:48","","9","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/htk/Zotero/storage/2ZI7BXRK/Reilly and Schneider - 2019 - Predicting the Quality of Collaborative Problem So.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQ5W7N9Z","conferencePaper","2012","Nakano, Yukiko; Fukuhara, Yuki","Estimating conversational dominance in multiparty interaction","Proceedings of the 14th ACM international conference on Multimodal interaction","978-1-4503-1467-1","","10.1145/2388676.2388699","https://dl.acm.org/doi/10.1145/2388676.2388699","It is important for conversational agents that manage multiparty conversations to recognize the group dynamics existing among the users. This paper proposes a method for estimating the conversational dominance of participants in group interactions. First, we conducted a Wizard-of-Oz experiment to collect conversational speech, and motion data. Then, we analyzed various paralinguistic speech and gaze behaviors to elucidate the factors that predict conversational dominance. Finally, by exploiting the speech and gaze data as estimation parameters, we created a regression model to estimate conversational dominance, and the multiple correlation coefficient of this model was 0.85.","2012-10-22","2024-01-08 10:26:24","2024-01-08 10:26:25","2024-01-08 10:26:24","77-84","","","","","","","","","","","ACM","Santa Monica California USA","en","","","","","DOI.org (Crossref)","","","","/Users/htk/Zotero/storage/MLXFVVHK/Nakano and Fukuhara - 2012 - Estimating conversational dominance in multiparty .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICMI '12: INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION","","","","","","","","","","","","","","",""
"RFZQLC8W","conferencePaper","2007","Hung, Hayley; Jayagopi, Dinesh; Yeo, Chuohao; Friedland, Gerald; Ba, Sileye; Odobez, Jean-Marc; Ramchandran, Kannan; Mirghafori, Nikki; Gatica-Perez, Daniel","Using audio and video features to classify the most dominant person in a group meeting","Proceedings of the 15th ACM international conference on Multimedia","978-1-59593-702-5","","10.1145/1291233.1291423","https://doi.org/10.1145/1291233.1291423","The automated extraction of semantically meaningful information from multi-modal data is becoming increasingly necessary due to the escalation of captured data for archival. A novel area of multi-modal data labelling, which has received relatively little attention, is the automatic estimation of the most dominant person in a group meeting. In this paper, we provide a framework for detecting dominance in group meetings using different audio and video cues. We show that by using a simple model for dominance estimation we can obtain promising results.","2007-09-29","2024-01-08 11:10:07","2024-01-08 11:10:07","2024-01-08","835–838","","","","","","","MM '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/YHFAMRHQ/Hung et al. - 2007 - Using audio and video features to classify the mos.pdf","","","audio-visual feature extraction; data annotation; dominance modelling; meetings","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KB23DXUR","conferencePaper","2011","Martinez, Roberto; Kay, Judy; Wallace, James R.; Yacef, Kalina","Modelling Symmetry of Activity as an Indicator of Collocated Group Collaboration","User Modeling, Adaption and Personalization","978-3-642-22362-4","","10.1007/978-3-642-22362-4_18","","There are many contexts where it would be helpful to model the collaboration of a group. In learning settings, this is important for classroom teachers and for students learning collaboration skills. Our approach exploits the digital and audio footprints of the users’ actions at collocated settings to automatically build a model of symmetry of activity. This paper describes our theoretical model of collaborative learning and how we implemented it. We use the Gini coefficient as a statistical indicator of symmetry of activity, which is itself an important indicator of collaboration. We built this model from a small-scale qualitative study based on concept mapping at an interactive tabletop. We then evaluated the model using a larger scale study based on a corpus of coded data from a multi-display groupware collocated setting. Our key contributions are the model of symmetry of activity as a foundation for modelling collaboration within groups that should have egalitarian participation, the operationalisation of the model and validation of the approach on both a small-scale qualitative study and a larger scale quantitative corpus of data.","2011","2024-01-08 11:10:51","2024-01-08 11:10:51","","207-218","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","","","","clustering; collaborative learning; collocated collaboration; group modelling; groupware; tabletop","Konstan, Joseph A.; Conejo, Ricardo; Marzo, José L.; Oliver, Nuria","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MAEUITPR","journalArticle","2001","Henning, R. A.; Boucsein, W.; Gil, M. C.","Social physiological compliance as a determinant of team performance","International Journal of Psychophysiology: Official Journal of the International Organization of Psychophysiology","","0167-8760","10.1016/s0167-8760(00)00190-2","","A cybernetic model of behavior predicts that team performance may depend on physiological compliance among participants. This laboratory study tested if compliance in electrodermal activity (EDA), heart rate or breathing in two-person teams (N=16) was predictive of team performance or coordination in a continuous tracking task simulating teleoperation. Visual contact among participants was manipulated. Physiological compliance was scored with weighted coherence and cross correlation. Separate multiple regression analyses revealed that the task completion time was predicted by coherence measures for EDA and heart, but only at a trend level for breathing. Task completion time was also predicted by heart cross correlation. Team tracking error was predicted by coherence measures for EDA, heart and breathing, and also heart cross correlation. While social-visual contact did not have an impact, physiological compliance was predictive of improved performance, with coherence robust over all three physiological measures. Heart cross correlation showed the strongest predictive relationships. These results provide evidence that physiological compliance among team members may benefit team performance. While further study is needed, physiological compliance may someday provide a needed tool for the study of team work, and an objective means to guide the ergonomic design of complex sociotechnical systems requiring a high degree of team proficiency.","2001-04","2024-01-08 11:11:14","2024-01-08 11:11:14","","221-232","","3","40","","Int J Psychophysiol","","","","","","","","eng","","","","","PubMed","","PMID: 11228349","","","http://www.ncbi.nlm.nih.gov/pubmed/11228349","","Adolescent; Adult; Cooperative Behavior; Female; Galvanic Skin Response; Heart Rate; Humans; Male; Psychomotor Performance; Regression Analysis; Respiratory Mechanics; Social Behavior","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QCBTVDUN","conferencePaper","1999","Stiefelhagen, R.; Yang, Jie; Waibel, A.","Modeling people's focus of attention","Proceedings IEEE International Workshop on Modelling People. MPeople'99","","","10.1109/PEOPLE.1999.798349","https://ieeexplore.ieee.org/abstract/document/798349","In this paper, we present an approach to model focus of attention of participants in a meeting via hidden Markov models (HMM). We employ HMM to encode and track focus of attention, based on the participants' gaze information and knowledge of their positions. The positions of the participants are detected by face tracking in the view of a panoramic camera mounted on the meeting table. We use neural networks to estimate the participants' gaze from camera images. We discuss the implementation of the approach in detail, including system architecture, data collection, and evaluation. The system has achieved an accuracy rate of up to 93% in detecting focus of attention on test sequences taken from meetings. We have used focus of attention as an index in a multimedia meeting browser.","1999-09","2024-01-08 11:12:39","2024-01-08 11:12:39","2024-01-08 11:12:39","79-86","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/htk/Zotero/storage/6664FAMK/798349.html; /Users/htk/Zotero/storage/DB464UN3/Stiefelhagen et al. - 1999 - Modeling people's focus of attention.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings IEEE International Workshop on Modelling People. MPeople'99","","","","","","","","","","","","","","",""
"4HYZ2894","conferencePaper","2022","Harrison, Julie; Jain, Sona Anita; Dunbar, Terri; Gorman, Jamie; Varma, Sashank","Toward Automated Detection of Phase Changes in Team Collaboration","Proceedings of the Annual Meeting of the Cognitive Science Society","","","","https://escholarship.org/uc/item/7jw0q366","Team science research heavily relies on communication data—that is, data derived from audio, video, or text-chat communication streams between team members. Between transcription and content analysis, significant overhead is required to work with these data. Recent developments in natural language processing (NLP) may help ameliorate time constraints in this domain. Using transcript data, the present study, presented as a proof-of-concept, assesses how the BERT NLP model performs in a team communication categorization task, in comparison to ground truth measures. This work builds upon past work that relied on human-coded transcripts to identify phase transitions in team collaboration. Results suggest BERT’s capabilities at phase change detection are promising for experienced teams, though further iteration is needed on the methods in the current study. Applications of this work extend to real-time collaboration with an artificial agent, as this requires the real-time semantic processing of human communication data.","2022","2024-01-08 11:13:04","2024-01-08 11:13:27","2024-01-08 11:13:04","","","","44","","","","","","","","","","en","","","","","escholarship.org","","","","/Users/htk/Zotero/storage/UPG7BZRL/Harrison et al. - 2022 - Toward Automated Detection of Phase Changes in Tea.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T6B8S7C8","journalArticle","2018","Ahonen, Lauri; Cowley, Benjamin Ultan; Hellas, Arto; Puolamäki, Kai","Biosignals reflect pair-dynamics in collaborative work: EDA and ECG study of pair-programming in a classroom environment","Scientific Reports","","2045-2322","10.1038/s41598-018-21518-3","https://www.nature.com/articles/s41598-018-21518-3","Collaboration is a complex phenomenon, where intersubjective dynamics can greatly affect the productive outcome. Evaluation of collaboration is thus of great interest, and can potentially help achieve better outcomes and performance. However, quantitative measurement of collaboration is difficult, because much of the interaction occurs in the intersubjective space between collaborators. Manual observation and/or self-reports are subjective, laborious, and have a poor temporal resolution. The problem is compounded in natural settings where task-activity and response-compliance cannot be controlled. Physiological signals provide an objective mean to quantify intersubjective rapport (as synchrony), but require novel methods to support broad deployment outside the lab. We studied 28 student dyads during a self-directed classroom pair-programming exercise. Sympathetic and parasympathetic nervous system activation was measured during task performance using electrodermal activity and electrocardiography. Results suggest that (a) we can isolate cognitive processes (mental workload) from confounding environmental effects, and (b) electrodermal signals show role-specific but correlated affective response profiles. We demonstrate the potential for social physiological compliance to quantify pair-work in natural settings, with no experimental manipulation of participants required. Our objective approach has a high temporal resolution, is scalable, non-intrusive, and robust.","2018-02-16","2024-01-08 11:13:50","2024-01-08 11:13:50","2024-01-08 11:13:50","3138","","1","8","","Sci Rep","Biosignals reflect pair-dynamics in collaborative work","","","","","","","en","2018 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","/Users/htk/Zotero/storage/UC3QIB2R/Ahonen et al. - 2018 - Biosignals reflect pair-dynamics in collaborative .pdf","","","Motivation; Neurophysiology; Predictive markers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NY593L7W","conferencePaper","2010","Ijsselmuiden, Joris; Stiefelhagen, Rainer","Towards high-level human activity recognition through computer vision and temporal logic","Proceedings of the 33rd annual German conference on Advances in artificial intelligence","978-3-642-16110-0","","","","Most approaches to the visual perception of humans do not include high-level activity recognitition. This paper presents a system that fuses and interprets the outputs of several computer vision components as well as speech recognition to obtain a high-level understanding of the perceived scene. Our laboratory for investigating new ways of human-machine interaction and teamwork support, is equipped with an assemblage of cameras, some close-talking microphones, and a videowall as main interaction device. Here, we develop state of the art real-time computer vision systems to track and identify users, and estimate their visual focus of attention and gesture activity. We also monitor the users' speech activity in real time. This paper explains our approach to highlevel activity recognition based on these perceptual components and a temporal logic engine.","2010-09-21","2024-01-08 11:15:27","2024-01-08 11:15:27","2024-01-08","426–435","","","","","","","KI'10","","","","Springer-Verlag","Berlin, Heidelberg","","","","","","ACM Digital Library","","","","","","","activity recognition; computer vision; temporal logic","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"THF4QIQW","conferencePaper","2008","Ba, Sileye O.; Odobez, Jean Marc","Visual focus of attention estimation from head pose posterior probability distributions","2008 IEEE International Conference on Multimedia and Expo","","","10.1109/ICME.2008.4607369","https://ieeexplore.ieee.org/document/4607369","We address the problem of recognizing the visual focus of attention (VFOA) of meeting participants from their head pose and contextual cues. The main contribution of the paper is the use of a head pose posterior distribution as a representation of the head pose information contained in the image data. This posterior encodes the probabilities of the different head poses given the image data, and constitute therefore a richer representation of the data than the mean or the mode of this distribution, as done in all previous work. These observations are exploited in a joint interaction model of all meeting participants pose observations, VFOAs, speaking status and of environmental contextual cues. Numerical experiments on a public database of 4 meetings of 22 min on average show that this change of representation allows for a 5.4% gain with respect to the standard approach using head pose as observation.","2008-06","2024-01-08 11:16:17","2024-01-08 11:16:17","2024-01-08 11:16:17","53-56","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 1945-788X","","/Users/htk/Zotero/storage/7P5THGR5/4607369.html; /Users/htk/Zotero/storage/MTU7UCKQ/Ba and Odobez - 2008 - Visual focus of attention estimation from head pos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2008 IEEE International Conference on Multimedia and Expo","","","","","","","","","","","","","","",""
"KWMY2PDB","conferencePaper","2008","Kim, Taemie; Chang, Agnes; Holland, Lindsey; Pentland, Alex (Sandy)","Meeting mediator: enhancing group collaboration with sociometric feedback","CHI '08 Extended Abstracts on Human Factors in Computing Systems","978-1-60558-012-8","","10.1145/1358628.1358828","https://doi.org/10.1145/1358628.1358828","In this paper we present the Meeting Mediator (MM), a real-time, personal, and portable system providing feedback to enhance group collaboration. Social interactions are captured using Sociometric badges [6] and are visualized on mobile phones to promote change in behavior. In a study on brainstorming and problem-solving meetings, MM had a significant effect on overlapping speaking time and interactivity level without distracting the subjects. Our system encourages effective group dynamics that may lead to higher performance and satisfaction. We envision MM to be deployed in real-world organizations to improve interactions across various group collaboration contexts.","2008-04-05","2024-01-08 11:18:53","2024-01-08 11:18:53","2024-01-08","3183–3188","","","","","","Meeting mediator","CHI EA '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","cscw; meeting support; social visualization; sociometric sensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ZMKL5NA","journalArticle","2007","Hanna, Joy E.; Brennan, Susan E.","Speakers’ eye gaze disambiguates referring expressions early during face-to-face conversation","Journal of Memory and Language","","0749-596X","10.1016/j.jml.2007.01.008","https://www.sciencedirect.com/science/article/pii/S0749596X07000174","In two experiments, we explored the time course and flexibility with which speakers’ eye gaze can be used to disambiguate referring expressions in spontaneous dialog. Naive director/matcher pairs were separated by a barrier and saw each other’s faces but not their displays. Displays held identical objects, with the matcher’s arranged in a row and the director’s mirroring the matcher’s or else in a circle (Experiment 1) or in a reversed row (Experiment 2). Directors instructed matchers to move targets, which were unique or had a competitor nearby or far away. When mirrored displays held far competitors, matchers used directors’ eye gaze to identify targets before the linguistic point of disambiguation. Reversed displays caused substantial competition, yet matchers still identified targets before the linguistic point of disambiguation, showing an ability to rapidly re-map directors’ eye gaze. Our findings indicate eye gaze is a powerful and flexible disambiguating cue in referential communication.","2007-11-01","2024-01-08 11:19:12","2024-01-08 11:19:12","2024-01-08 11:19:12","596-615","","4","57","","Journal of Memory and Language","","Language-Vision Interaction","","","","","","","","","","","ScienceDirect","","","","/Users/htk/Zotero/storage/GIIY92Z6/S0749596X07000174.html","","","Ambiguity resolution; Conversation; Discourse processing; Eye gaze; Speech","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EUPC4KZH","journalArticle","2019","Babiker, Areej; Faye, Ibrahima; Mumtaz, Wajid; Malik, Aamir Saeed; Sato, Hiroki","EEG in classroom: EMD features to detect situational interest of students during learning","Multimedia Tools and Applications","","1573-7721","10.1007/s11042-018-7016-z","https://doi.org/10.1007/s11042-018-7016-z","Situational interest is widely explored in the psychology and education domains. It is proven to have positive effect on learning and academic achievement. Nonetheless, not much attention is given for assessing the feasibility of detecting this interest in natural classroom physiologically. Therefore, this study investigates the possibility of detecting situational interest using Electroencephalogram (EEG) in classroom. After preprocessing of EEG data, they were decomposed using Empirical Mode Decomposition (EMD). The resulted Intrinsic Mode Functions (IMFs) were ranked based on their significance using T-test and Receiver Operator Characteristics (ROC) in descending order. A matrix was constructed for all participants using the best six features from four EEG channels. These selected features were fed into Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) classifiers with 10 cross validation. While SVM achieved high accuracy of 93.3% and 87.5% for two data sets using features from the four EEG channels, KNN classifier achieved high accuracy of 87.5% and 86.7% in the same datasets using single EEG channel. It is found that gamma and delta bands can be used successfully to detect situational interest of students during learning in classrooms. Furthermore, data of single EEG channel - F3 in this study- was efficient to detect student’s situational interest in simultaneous recording of EEG in classroom.","2019-06-01","2024-01-08 11:19:45","2024-01-08 11:19:45","2024-01-08 11:19:45","16261-16281","","12","78","","Multimed Tools Appl","EEG in classroom","","","","","","","en","","","","","Springer Link","","","","","","","Classification; Classroom; Electroencephalogram; Empirical mode decomposition; Situational interest levels","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZEQMNP7D","journalArticle","2022","Sakashita, Mose; Ricci, E. Andy; Arora, Jatin; Guimbretière, François","RemoteCoDe: Robotic Embodiment for Enhancing Peripheral Awareness in Remote Collaboration Tasks","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3512910","https://dl.acm.org/doi/10.1145/3512910","Collaborative design activities are often centered around physical artifacts. Depending on the design activity, this can be the model of a building, paper crafts, carving artwork, or a new circuit to be debugged and evaluated. In a typical setting, collaborators are seated around a table and divide their attention between the design artifact under review, at least one laptop supporting measurements and information foraging, and of course their collaborators. Although these activities involve complex sets of tools and configurations, people can easily work together when they are present in the same space. This is because the physical presence of a partner affords peripheral awareness to inform where the partner's attention is and what they are doing. This peripheral awareness allows collaborators to coordinate actions and manage coupling to achieve a shared task. For example, it is quite easy to know when your partner switches their focus from a breadboard to you as a request to start a face to face discussion.","2022-04-07","2024-01-08 11:20:20","2024-01-08 11:20:20","2024-01-08 11:20:20","63:1–63:22","","CSCW1","6","","Proc. ACM Hum.-Comput. Interact.","RemoteCoDe","","","","","","","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/XPIKZ8GG/Sakashita et al. - 2022 - RemoteCoDe Robotic Embodiment for Enhancing Perip.pdf","","","design; remote collaboration; robotic embodiment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AELXMEW4","conferencePaper","2003","Fussell, Susan R.; Setlock, Leslie D.; Kraut, Robert E.","Effects of head-mounted and scene-oriented video systems on remote collaboration on physical tasks","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-58113-630-2","","10.1145/642611.642701","https://doi.org/10.1145/642611.642701","This study assessed the value of two video configurations-a head-mounted camera with eye tracking capability and a scene camera providing a view of the work environment-on remote collaboration on physical (3D) tasks. Pairs of participants performed five robot construction tasks in five media conditions: side-by-side, audio-only, head-mounted camera, scene camera, and scene plus head cameras. Task completion times were shortest in the side-by-side condition, and shorter with the scene camera than in the audio-only condition. Participants rated their work quality highest when side-by-side, intermediate with the scene camera, and worst in the audio-only and head-camera conditions. Similarly, helpers' self-rated ability to assist workers and pairs' communication efficiency were highest in the side-by-side condition, but significantly higher with the scene camera than in the audio-only condition. The results demonstrate the value of a shared view of the work environment for remote collaboration on physical tasks.","2003-04-05","2024-01-08 11:21:08","2024-01-08 11:21:08","2024-01-08","513–520","","","","","","","CHI '03","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/F5IC75RY/Fussell et al. - 2003 - Effects of head-mounted and scene-oriented video s.pdf","","","video-conferencing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FKWQZYZV","conferencePaper","2000","Fussell, Susan R.; Kraut, Robert E.; Siegel, Jane","Coordination of communication: effects of shared visual context on collaborative work","Proceedings of the 2000 ACM conference on Computer supported cooperative work","978-1-58113-222-9","","10.1145/358916.358947","https://dl.acm.org/doi/10.1145/358916.358947","We outline some of the benefits of shared visual information for collaborative repair tasks and report on a study comparing collaborative performance on a manual task by workers and helpers who are located side-by-side or connected via audio-video or audio-only links. Results show that the dyads complete the task more quickly and accurately when helpers are co-located than when they are connected via an audio link. However, they didn't achieve similar efficiency gains when they communicated through an audio/video link. These results demonstrate the value of a shared visual work space, but raise questions about the adequacy of current video communication technology for implementing it.","2000-12-01","2024-01-08 11:22:05","2024-01-08 11:22:05","2024-01-08","21–30","","","","","","Coordination of communication","CSCW '00","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/LHJSGN5U/Fussell et al. - 2000 - Coordination of communication effects of shared v.pdf","","","computer-supported collaborative work; conversational analysis; empirical studies; video mediated communication; wearable computers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NUW3U2G9","conferencePaper","2008","Canton-Ferrer, Cristian; Segura, Carlos; Pardàs, Montse; Casas, Josep R.; Hernando, Javier","Multimodal real-time focus of attention estimation in SmartRooms","IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2008, Anchorage, AK, USA, 23-28 June, 2008","","","10.1109/CVPRW.2008.4563180","","","2008","2024-01-08 11:23:10","2024-01-08 11:23:10","","1–8","","","","","","","","","","","IEEE Computer Society","","","","","","","DBLP Computer Science Bibliography","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"34PNERM3","conferencePaper","2008","Ba, Sileye O.; Odobez, Jean-Marc","Multi-party focus of attention recognition in meetings from head pose and multimodal contextual cues","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","","","10.1109/ICASSP.2008.4518086","https://ieeexplore.ieee.org/abstract/document/4518086","This paper presents investigations on visual focus of attention (VFOA) recognition in meetings from audio-visual perceptual cues. Rather than independently recognizing the VFOA of each participant from his own head pose, we propose to recognize participants' VFOA jointly in order to introduce context dependent interaction models that relates to group activity and the social dynamics of communication. To this end, we designed an input-output hidden Markov model (IOHMM), whose hidden states are the joint VFOA of all participants, and whose main observations are the head poses. Interaction models are introduced in the form of contextual cues that affect the temporal evolution of the joint VFOA sequence, allowing us to model group dynamics that accounts for people's tendency to share the same focus, or to have their VFOA driven by contextual cues such as slide activity or the participant speaking activity. The model is rigorously evaluated on a publicly available dataset of 4 real meetings of 23min on average, showing an overall 10% relative performance increase w.r.t. the independent recognition case.","2008-03","2024-01-08 11:25:37","2024-01-08 11:25:37","2024-01-08 11:25:37","2221-2224","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2379-190X","","/Users/htk/Zotero/storage/JA2FFK8U/4518086.html; /Users/htk/Zotero/storage/MUCTVG3M/Ba and Odobez - 2008 - Multi-party focus of attention recognition in meet.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","","","","","","","","","","","","","","",""
"6TTZMBTR","conferencePaper","2007","Yonezawa, Tomoko; Yamazoe, Hirotake; Utsumi, Akira; Abe, Shinji","Gaze-communicative behavior of stuffed-toy robot with joint attention and eye contact based on ambient gaze-tracking","Proceedings of the 9th international conference on Multimodal interfaces","978-1-59593-817-6","","10.1145/1322192.1322218","https://doi.org/10.1145/1322192.1322218","This paper proposes a gaze-communicative stuffed-toy robot system with joint attention and eye-contact reactions based on ambient gaze-tracking. For free and natural interaction, we adopted our remote gaze-tracking method. Corresponding to the user's gaze, the gaze-reactive stuffed-toy robot is designed to gradually establish 1) joint attention using the direction of the robot's head and 2) eye-contact reactions from several sets of motion. From both subjective evaluations and observations of the user's gaze in the demonstration experiments, we found that i) joint attention draws the user's interest along with the user-guessed interest of the robot, ii) ""eye contact"" brings the user a favorable feeling for the robot, and iii) this feeling is enhanced when ""eye contact"" is used in combination with ""joint attention."" These results support the approach of our embodied gaze-communication model.","2007-11-12","2024-01-08 11:26:19","2024-01-08 11:26:19","2024-01-08","140–145","","","","","","","ICMI '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","eye contact; gaze communication; joint attention; stuffed-toy robot","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MAV3AKCJ","conferencePaper","2008","Oviatt, Sharon; Swindells, Colin; Arthur, Alex","Implicit user-adaptive system engagement in speech and pen interfaces","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-60558-011-1","","10.1145/1357054.1357204","https://doi.org/10.1145/1357054.1357204","As emphasis is placed on developing mobile, educational, and other applications that minimize cognitive load on users, it is becoming more essential to explore interfaces based on implicit engagement techniques so users can remain focused on their tasks. In this research, data were collected with 12 pairs of students who solved complex math problems using a tutorial system that they engaged over 100 times per session entirely implicitly via speech amplitude or pen pressure cues. Results revealed that users spontaneously, reliably, and substantially adapted these forms of communicative energy to designate and repair an intended interlocutor in a computer-mediated group setting. Furthermore, this behavior was harnessed to achieve system engagement accuracies of 75-86%, with accuracies highest using speech amplitude. However, students had limited awareness of their own adaptations. Finally, while continually using these implicit engagement techniques, students maintained their performance level at solving complex mathematics problems throughout a one-hour session.","2008-04-06","2024-01-08 11:27:42","2024-01-08 11:27:42","2024-01-08","969–978","","","","","","","CHI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","educational interface; implicit; pen pressure; speech amplitude; system engagement; user-adaptive","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EAYEJZJM","conferencePaper","2009","Dierker, Angelika; Mertes, Christian; Hermann, Thomas; Hanheide, Marc; Sagerer, Gerhard","Mediated attention with multimodal augmented reality","Proceedings of the 2009 international conference on Multimodal interfaces","978-1-60558-772-1","","10.1145/1647314.1647368","https://doi.org/10.1145/1647314.1647368","We present an Augmented Reality (AR) system to support collaborative tasks in a shared real-world interaction space by facilitating joint attention. The users are assisted by information about their interaction partner's field of view both visually and acoustically. In our study, the audiovisual improvements are compared with an AR system without these support mechanisms in terms of the participants' reaction times and error rates. The participants performed a simple object-choice task we call the ""gaze game"" to ensure controlled experimental conditions. Additionally, we asked the subjects to fill in a questionnaire to gain subjective feedback from them. We were able to show an improvement for both dependent variables as well as positive feedback for the visual augmentation in the questionnaire.","2009-11-02","2024-01-08 11:28:48","2024-01-08 11:28:48","2024-01-08","245–252","","","","","","","ICMI-MLMI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","artificial communication channels; augmented reality; collaboration; cscw; field of view; joint attention; mediated attention; multimodal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IJYPSCZZ","conferencePaper","1999","Stiefelhagen, Rainer; Yang, Jie; Waibel, Alex","Modeling focus of attention for meeting indexing","Proceedings of the seventh ACM international conference on Multimedia (Part 1)","978-1-58113-151-2","","10.1145/319463.319464","https://dl.acm.org/doi/10.1145/319463.319464","","1999-10-30","2024-01-08 13:45:52","2024-01-08 13:45:52","2024-01-08","3–10","","","","","","","MULTIMEDIA '99","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/ZB66G6IZ/Stiefelhagen et al. - 1999 - Modeling focus of attention for meeting indexing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5QEHLN25","journalArticle","2002","Stiefelhagen, R.; Yang, Jie; Waibel, A.","Modeling focus of attention for meeting indexing based on multiple cues","IEEE Transactions on Neural Networks","","1941-0093","10.1109/TNN.2002.1021893","https://ieeexplore.ieee.org/document/1021893","A user's focus of attention plays an important role in human-computer interaction applications, such as a ubiquitous computing environment and intelligent space, where the user's goal and intent have to be continuously monitored. We are interested in modeling people's focus of attention in a meeting situation. We propose to model participants' focus of attention from multiple cues. We have developed a system to estimate participants' focus of attention from gaze directions and sound sources. We employ an omnidirectional camera to simultaneously track participants' faces around a meeting table and use neural networks to estimate their head poses. In addition, we use microphones to detect who is speaking. The system predicts participants' focus of attention from acoustic and visual information separately. The system then combines the output of the audio- and video-based focus of attention predictors. We have evaluated the system using the data from three recorded meetings. The acoustic information has provided 8% relative error reduction on average compared to only using one modality. The focus of attention model can be used as an index for a multimedia meeting record. It can also be used for analyzing a meeting.","2002-07","2024-01-08 11:29:16","2024-01-08 11:29:16","2024-01-08 11:29:16","928-938","","4","13","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Neural Networks","","/Users/htk/Zotero/storage/VXTBXFYF/1021893.html; /Users/htk/Zotero/storage/4EKIT4I6/Stiefelhagen et al. - 2002 - Modeling focus of attention for meeting indexing b.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V9HSHHZ6","conferencePaper","2010","Subramanian, Ramanathan; Staiano, Jacopo; Kalimeri, Kyriaki; Sebe, Nicu; Pianesi, Fabio","Putting the pieces together: multimodal analysis of social attention in meetings","Proceedings of the 18th ACM international conference on Multimedia","978-1-60558-933-6","","10.1145/1873951.1874045","https://doi.org/10.1145/1873951.1874045","This paper presents a multimodal framework employing eye-gaze, head-pose and speech cues to explain observed social attention patterns in meeting scenes. We first investigate a few hypotheses concerning social attention and characterize meetings and individuals based on ground-truth data. This is followed by replication of ground-truth results through automated estimation of eye-gaze, head-pose and speech activity for each participant. Experimental results show that combining eye-gaze and head-pose estimates decreases error in social attention estimation by over 26%.","2010-10-25","2024-01-08 11:30:04","2024-01-08 11:30:04","2024-01-08","659–662","","","","","","Putting the pieces together","MM '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","eye-gaze; head-pose; meeting analysis; social attention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2V8RQ2EZ","conferencePaper","2001","Stiefelhagen, Rainer; Yang, Jie; Waibel, Alex","Estimating focus of attention based on gaze and sound","Proceedings of the 2001 workshop on Perceptive user interfaces","978-1-4503-7473-6","","10.1145/971478.971505","https://doi.org/10.1145/971478.971505","Estimating a person's focus of attention is useful for various human-computer interaction applications, such as smart meeting rooms, where a user's goals and intent have to be monitored. In work presented here, we are interested in modeling focus of attention in a meeting situation. We have developed a system capable of estimating participants' focus of attention from multiple cues. We employ an omnidirectional camera to simultaneously track participants' faces around a meeting table and use neural networks to estimate their head poses. In addition, we use microphones to detect who is speaking. The system predicts participants' focus of attention from acoustic and visual information separately, and then combines the output of the audio- and video-based focus of attention predictors. We have evaluated the system using the data from three recorded meetings. The acoustic information has provided 8% error reduction on average compared to using a single modality.","2001-11-15","2024-01-08 11:30:41","2024-01-08 11:30:41","2024-01-08","1–9","","","","","","","PUI '01","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/CDME6LFZ/Stiefelhagen et al. - 2001 - Estimating focus of attention based on gaze and so.pdf","","","focus of attention; gaze tracking; intelligent environments; meeting analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y8VG5UQ5","conferencePaper","2007","Morita, Tomoyuki; Mase, Kenji; Hirano, Yasushi; Kajita, Shoji","Reciprocal attentive communication in remote meeting with a humanoid robot","Proceedings of the 9th international conference on Multimodal interfaces","978-1-59593-817-6","","10.1145/1322192.1322232","https://doi.org/10.1145/1322192.1322232","In this paper, we investigate the reciprocal attention modality in remotecommunication. A remote meeting system with a humanoid robot avatar is proposedto overcome the invisible wall for a video conferencing system. Ourexperimental result shows that a tangible robot avatar provides more effectivereciprocal attention against video communication. The subjects in the experimentare asked to determine whether a remote participant with the avatar is activelylistening or not to the local presenter's talk. In this system, the head motionof a remote participant is transferred and expressed by the head motion of ahumanoid robot. While the presenter has difficulty in determining the extentof a remote participant's attention with a video conferencing system, she/he hasbetter sensing of remote attentive states with the robot. Based on theevaluation result, we propose a vision system for the remote user thatintegrates omni-directional camera and robot-eye camera images to provide a wideview with a delay compensation feature.","2007-11-12","2024-01-08 11:31:20","2024-01-08 11:31:20","2024-01-08","228–235","","","","","","","ICMI '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","gaze; gesture; humanoid robot; remote communication; robot teleconferencing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LDDRZ9NS","conferencePaper","2009","Sanchez-Cortes, Dairazalia; Jayagopi, Dinesh Babu; Gatica-Perez, Daniel","Predicting remote versus collocated group interactions using nonverbal cues","Proceedings of the ICMI-MLMI '09 Workshop on Multimodal Sensor-Based Systems and Mobile Phones for Social Computing","978-1-60558-694-6","","10.1145/1641389.1641392","https://doi.org/10.1145/1641389.1641392","This paper addresses two problems: Firstly, the problem of classifying remote and collocated small-group working meetings, and secondly, the problem of identifying the remote participant, using in both cases nonverbal behavioral cues. Such classifiers can be used to improve the design of remote collaboration technologies to make remote interactions as effective as possible to collocated interactions. We hypothesize that the difference in the dynamics between collocated and remote meetings is significant and measurable using speech activity based nonverbal cues. Our results on a publicly available dataset - the Augmented Multi-Party Interaction with Distance Access (AMIDA) corpus - show that such an approach is promising, although more controlled settings and more data are needed to explore the addressed problems further.","2009-11-06","2024-01-08 11:31:45","2024-01-08 11:31:45","2024-01-08","1–4","","","","","","","ICMI-MLMI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/ZGS39SHH/Sanchez-Cortes et al. - 2009 - Predicting remote versus collocated group interact.pdf","","","characterizing small groups; nonverbal behavior; remote meetings","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZJZUFBN","conferencePaper","2006","Ou, Jiazhi; Shi, Yanxin; Wong, Jeffrey; Fussell, Susan R.; Yang, Jie","Combining audio and video to predict helpers' focus of attention in multiparty remote collaboration on physical tasks","Proceedings of the 8th international conference on Multimodal interfaces","978-1-59593-541-0","","10.1145/1180995.1181040","https://doi.org/10.1145/1180995.1181040","The increasing interest in supporting multiparty remote collaboration has created both opportunities and challenges for the research community. The research reported here aims to develop tools to support multiparty remote collaborations and to study human behaviors using these tools. In this paper we first introduce an experimental multimedia (video and audio) system with which an expert can collaborate with several novices. We then use this system to study helpers' focus of attention (FOA) during a collaborative circuit assembly task. We investigate the relationship between FOA and language as well as activities using multimodal (audio and video) data, and use learning methods to predict helpers' FOA. We process different modalities separately and fusion the results to make a final decision. We employ a sliding window-based delayed labeling method to automatically predict changes in FOA in real time using only the dialogue among the helper and workers. We apply an adaptive background subtraction method and support vector machine to recognize the worker's activities from the video. To predict the helper's FOA, we make decisions using the information of joint project boundaries and workers' recent activities. The overall prediction accuracies are 79.52% using audio only and 81.79% using audio and video combined.","2006-11-02","2024-01-08 11:32:41","2024-01-08 11:32:41","2024-01-08","217–224","","","","","","","ICMI '06","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/XTQ7KQ6D/Ou et al. - 2006 - Combining audio and video to predict helpers' focu.pdf","","","computer-supported cooperative work; focus of attention; multimodal integration; remote collaborative physical tasks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EIRH3N7J","conferencePaper","2009","Lepri, Bruno; Mana, Nadia; Cappelletti, Alessandro; Pianesi, Fabio","Automatic prediction of individual performance from thin slices of social behavior","Proceedings of the 17th ACM international conference on Multimedia","978-1-60558-608-3","","10.1145/1631272.1631400","https://doi.org/10.1145/1631272.1631400","This paper targets the automatic detection of individual performances in group tasks by means of short sequences, ""thin slices"", of nonverbal behavior. We designed our task as a classification one. We also investigated the relevance of social context in our task and the effectiveness of our feature selection.","2009-10-19","2024-01-08 11:33:13","2024-01-08 11:33:13","2024-01-08","733–736","","","","","","","MM '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","intelligent environments; performance prediction; small group interaction; support vector machines","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6933LKVD","conferencePaper","2009","Lepri, Bruno; Mana, Nadia; Cappelletti, Alessandro; Pianesi, Fabio; Zancanaro, Massimo","Modeling the Personality of Participants During Group Interactions","Proceedings of the 17th International Conference on User Modeling, Adaptation, and Personalization: formerly UM and AH","978-3-642-02246-3","","10.1007/978-3-642-02247-0_13","https://doi.org/10.1007/978-3-642-02247-0_13","In this paper we target the automatic prediction of two personality traits, Extraversion and Locus of Control, in a meeting scenario using visual and acoustic features. We designed our task as a regression one where the goal is to predict the personality traits' scores obtained by the meeting participants. Support Vector Regression is applied to thin slices of behavior, in the form of 1-minute sequences.","2009-09-01","2024-01-08 11:33:50","2024-01-08 11:33:50","2024-01-08","114–125","","","","","","","UMAP '09","","","","Springer-Verlag","Berlin, Heidelberg","","","","","","ACM Digital Library","","","","","","","Adaptivity; Group Interactions; Personality Modeling; Support Vector Regression","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WTFPLHNR","conferencePaper","2008","Pianesi, Fabio; Mana, Nadia; Cappelletti, Alessandro; Lepri, Bruno; Zancanaro, Massimo","Multimodal recognition of personality traits in social interactions","Proceedings of the 10th international conference on Multimodal interfaces","978-1-60558-198-9","","10.1145/1452392.1452404","https://doi.org/10.1145/1452392.1452404","This paper targets the automatic detection of personality traits in a meeting environment by means of audio and visual features; information about the relational context is captured by means of acoustic features designed to that purpose. Two personality traits are considered: Extraversion (from the Big Five) and the Locus of Control. The classification task is applied to thin slices of behaviour, in the form of 1-minute sequences. SVM were used to test the performances of several training and testing instance setups, including a restricted set of audio features obtained through feature selection. The outcomes improve considerably over existing results, provide evidence about the feasibility of the multimodal analysis of personality, the role of social context, and pave the way to further studies addressing different features setups and/or targeting different personality traits.","2008-10-20","2024-01-08 11:34:39","2024-01-08 11:34:39","2024-01-08","53–60","","","","","","","ICMI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","group interaction; intelligent environments; personality modeling; support vector machines","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZAALM2Z","conferencePaper","2009","Kannetis, Theofanis; Potamianos, Alexandros","Towards adapting fantasy, curiosity and challenge in multimodal dialogue systems for preschoolers","Proceedings of the 2009 international conference on Multimodal interfaces","978-1-60558-772-1","","10.1145/1647314.1647324","https://doi.org/10.1145/1647314.1647324","We investigate how fantasy, curiosity and challenge contribute to the user experience in multimodal dialogue computer games for preschool children. For this purpose, an on-line multimodal platform has been designed, implemented and used as a starting point to develop web-based speech-enabled applications for children. Five task oriented games suitable for preschoolers have been implemented with varying levels of fantasy and curiosity elements, as well as, variable difficulty levels. Nine preschool children, ages 4-6, were asked to play these games in three sessions; in each session only one of the fantasy, curiosity or challenge factor was evaluated. Both objective and subjective criteria were used to evaluate the factors and applications. Results show that fantasy and curiosity are correlated with children's entertainment, while the level of difficulty seems to depend on each child's individual preferences and capabilities. In addition, high speech usage and high curiosity levels in the application correlate well with task completion, showing that preschoolers become more engaged when multimodal interfaces are speech enabled and contain curiosity elements.","2009-11-02","2024-01-08 13:56:56","2024-01-08 13:56:56","2024-01-08","39–46","","","","","","","ICMI-MLMI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","adaptation; dialogue; evaluation; multimodal; preschoolers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NXJ438QB","journalArticle","2020","Guo, Zhang; Barmaki, Roghayeh","Deep Neural Networks for Collaborative Learning Analytics: Evaluating Team Collaborations Using Student Gaze Point Prediction","Australasian Journal of Educational Technology","","","10.14742/ajet.6436","","Automatic assessment and evaluation of team performance during collaborative tasks is key to the research on learning analytics and computer-supported cooperative work. There is growing interest in the use of gaze-oriented cues for evaluating the collaboration and cooperativeness of teams. However, collecting gaze data using eye-trackers is not always feasible due to time and cost constraints. In this paper, we introduce an automated team assessment tool based on gaze points and joint visual attention (JVA) information drawn from computer vision solutions. We evaluated team collaborations in an undergraduate anatomy learning activity (N = 60, 30 teams) as a test user study. The results indicate that higher JVA was positively associated with student learning outcomes (r[subscript (30)] = 0.50, p < 0.005). Moreover, teams who participated in two experimental groups and used interactive 3D anatomy models, had higher JVA (F[subscript (1,28)] = 6.65, p < 0.05) and better knowledge retention (F[subscript (1,28)] = 7.56, p < 0.05) than those in the control group. Also, no significant difference was observed based on JVA for different gender compositions of teams. The findings from this work have implications in learning sciences and collaborative computing by providing a novel joint attention-based measure to objectively evaluate team collaboration dynamics.","2020","2024-01-08 14:01:39","2024-01-08 14:01:39","2024-01-08 14:01:39","53-71","","6","36","","","Deep Neural Networks for Collaborative Learning Analytics","","","","","","","en","","","","","ERIC","","Publisher: Australasian Society for Computers in Learning in Tertiary Education ERIC Number: EJ1280968","","/Users/htk/Zotero/storage/GJUI5DJ4/Guo and Barmaki - 2020 - Deep Neural Networks for Collaborative Learning An.pdf; /Users/htk/Zotero/storage/KGJWTBD4/eric.ed.gov.html","","","Anatomy; Attention Control; Comparative Analysis; Computer Assisted Instruction; Cooperative Learning; Cues; Evaluation Methods; Eye Movements; Gender Differences; Intervention; Laboratory Experiments; Learning Activities; Models; Outcomes of Education; Premedical Students; Science Instruction; Teamwork; Undergraduate Students","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C7DPFLVJ","conferencePaper","2021","Sharma, Kshitij; Olsen, Jennifer K.; Verma, Himanshu; Caballero, Daniela; Jermann, Patrick","Challenging Joint Visual Attention as a Proxy for Collaborative Performance","Proceedings of the 14th International Conference on Computer-Supported Collaborative Learning - CSCL 2021","","","","https://repository.isls.org//handle/1/7364","","2021-06","2024-01-08 14:04:01","2024-01-08 14:06:01","2024-01-08 14:04:01","91-98","","","","","","","","","","","International Society of the Learning Sciences","Bochum, Germany","en_US","","","","","repository.isls.org","","Publisher: International Society of the Learning Sciences","","/Users/htk/Zotero/storage/67LFPAWW/Sharma et al. - 2021 - Challenging Joint Visual Attention as a Proxy for .pdf","","","","Hmelo-Silver, C.E.; De Wever, B.; Oshima, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9HJDWUUD","conferencePaper","2011","Soro, Alessandro; Iacolina, Samuel Aldo; Scateni, Riccardo; Uras, Selene","Evaluation of user gestures in multi-touch interaction: a case study in pair-programming","Proceedings of the 13th international conference on multimodal interfaces","978-1-4503-0641-6","","10.1145/2070481.2070508","https://doi.org/10.1145/2070481.2070508","Natural User Interfaces are often described as familiar, evocative and intuitive, predictable, based on common skills. Though un-questionable in principle, such definitions don't provide the de-signer with effective means to design a natural interface or evalu-ate a design choice vs another. Two main issues in particular are open: (i) how do we evaluate a natural interface, is there a way to measure 'naturalness'; (ii) do natural user interfaces provide a concrete advantage in terms of efficiency, with respect to more tradi-tional interface paradigms? In this paper we discuss and compare observations of user behavior in the task of pair programming, performed at a traditional desktop versus a multi-touch table. We show how the adoption of a multi-touch user interface fosters a significant, observable and measurable, increase of nonverbal communication in general and of gestures in particular, that in turn appears related to the overall performance of the users in the task of algorithm understanding and debugging.","2011-11-14","2024-01-08 11:35:18","2024-01-08 11:35:18","2024-01-08","161–168","","","","","","Evaluation of user gestures in multi-touch interaction","ICMI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/GRQNF4DY/Soro et al. - 2011 - Evaluation of user gestures in multi-touch interac.pdf","","","evaluation methods; gesture; multi-touch; multi-user interaction / cooperation; pair programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9GN9SFQZ","conferencePaper","2006","Lunsford, Rebecca; Oviatt, Sharon; Arthur, Alexander M.","Toward Open-Microphone Engagement for Multiparty Interactions","Proceedings of the 8th international conference on Multimodal interfaces","978-1-59593-541-0","","10.1145/1180995.1181049","https://doi.org/10.1145/1180995.1181049","There currently is considerable interest in developing new open-microphone engagement techniques for speech and multimodal interfaces that perform robustly in complex mobile and multiparty field environments. State-of-the-art audio-visual open-microphone engagement systems aim to eliminate the need for explicit user engagement by processing more implicit cues that a user is addressing the system, which results in lower cognitive load for the user. This is an especially important consideration for mobile and educational interfaces due to the higher load required by explicit system engagement. In the present research, longitudinal data were collected with six triads of high-school students who engaged in peer tutoring on math problems with the aid of a simulated computer assistant. Results revealed that amplitude was 3.25dB higher when users addressed a computer rather than human peer when no lexical marker of intended interlocutor was present, and 2.4dB higher for all data. These basic results were replicated for both matched and adjacent utterances to computer versus human partners. With respect to dialogue style, speakers did not direct a higher ratio of commands to the computer, although such dialogue differences have been assumed in prior work. Results of this research reveal that amplitude is a powerful cue marking a speaker's intended addressee, which should be leveraged to design more effective microphone engagement during computer-assisted multiparty interactions.","2006-11-02","2024-01-08 11:35:44","2024-01-08 11:35:44","2024-01-08","273–280","","","","","","","ICMI '06","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","collaborative peer tutoring; computer-supported collaborative work; dialogue style; intended addressee; multimodal interaction; open-microphone engagement; spoken amplitude; user communication modeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K5TNNBCK","conferencePaper","2018","Murray, Gabriel; Lai, Catherine","Multimodal Analysis of Group Attitudes Towards Meeting Management","Proceedings of the Group Interaction Frontiers in Technology","978-1-4503-6077-7","","10.1145/3279981.3279984","https://doi.org/10.1145/3279981.3279984","We present experimental results on the task of automatically predicting group members' attitudes about management of their meeting, based on linguistic and acoustic features derived from the meeting recordings and transcripts. The group members' attitudes were gathered from detailed post-meeting questionnaires. A key finding is that features of linguistic content by themselves yield poor prediction performance on this task, but the best results are found by combining acoustic and linguistic features in a multimodal prediction model. When trying to automate the detection of group member attitudes that might be manifested subtly in their language and behaviour, a multimodal analysis is key.","2018-10-16","2024-01-08 11:36:07","2024-01-08 11:36:07","2024-01-08","1–6","","","","","","","GIFT'18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/REAWY4WH/Murray and Lai - 2018 - Multimodal Analysis of Group Attitudes Towards Mee.pdf","","","group sentiment; leadership; meeting management; multimodal interaction; social signal processing; speech and language processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KGNTGG55","journalArticle","2009","Shen, Liping; Wang, Minjuan; Shen, Ruimin","Affective e-Learning: Using “Emotional” Data to Improve Learning in Pervasive Learning Environment","Journal of Educational Technology & Society","","1176-3647","","https://www.jstor.org/stable/jeductechsoci.12.2.176","ABSTRACT Using emotion detection technologies from biophysical signals, this study explored how emotion evolves during learning process and how emotion feedback could be used to improve learning experiences. This article also described a cutting-edge pervasive e-Learning platform used in a Shanghai online college and proposed an affective e-Learning model, which combined learners' emotions with the Shanghai e-Learning platform. The study was guided by Russell's circumplex model of affect and Kort's learning spiral model. The results about emotion recognition from physiological signals achieved a best-case accuracy (86.3%) for four types of learning emotions. And results from emotion revolution study showed that engagement and confusion were the most important and frequently occurred emotions in learning, which is consistent with the findings from AutoTutor project. No evidence from this study validated Kort's learning spiral model. An experimental prototype of the affective e-Learning model was built to help improve students' learning experience by customizing learning material delivery based on students' emotional state. Experiments indicated the superiority of emotion aware over non-emotion-aware with a performance increase of 91%.","2009","2024-01-08 11:36:29","2024-01-08 11:36:29","2024-01-08 11:36:29","176-189","","2","12","","","Affective e-Learning","","","","","","","","","","","","JSTOR","","Publisher: International Forum of Educational Technology & Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PKC5CAMN","journalArticle","2010","Neider, Mark B.; Chen, Xin; Dickinson, Christopher A.; Brennan, Susan E.; Zelinsky, Gregory J.","Coordinating spatial referencing using shared gaze","Psychonomic Bulletin & Review","","1531-5320","10.3758/PBR.17.5.718","https://doi.org/10.3758/PBR.17.5.718","To better understand the problem of referencing a location in space under time pressure, we had two remotely located partners (A, B) attempt to locate and reach consensus on a sniper target, which appeared randomly in the windows of buildings in a pseudorealistic city scene. The partners were able to communicate using speech alone (shared voice), gaze cursors alone (shared gaze), or both. In the shared-gaze conditions, a gaze cursor representing Partner A’s eye position was superimposed over Partner B’s search display and vice versa. Spatial referencing times (for both partners to find and agree on targets) were faster with shared gaze than with speech, with this benefit due primarily to faster consensus (less time needed for one partner to locate the target after it was located by the other partner). These results suggest that sharing gaze can be more efficient than speaking when people collaborate on tasks requiring the rapid communication of spatial information. Supplemental materials for this article may be downloaded from http://pbr.psychonomic-journals.org/content/supplemental.","2010-10-01","2024-01-08 11:37:08","2024-01-08 11:37:08","2024-01-08 11:37:08","718-724","","5","17","","Psychonomic Bulletin & Review","","","","","","","","en","","","","","Springer Link","","","","","","","Communication Condition; Joint Attention; Locate Partner; Search Error; Spatial Reference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RC7DY22X","conferencePaper","2009","Kumano, Shiro; Otsuka, Kazuhiro; Mikami, Dan; Yamato, Junji","Recognizing communicative facial expressions for discovering interpersonal emotions in group meetings","Proceedings of the 2009 international conference on Multimodal interfaces","978-1-60558-772-1","","10.1145/1647314.1647333","https://doi.org/10.1145/1647314.1647333","This paper proposes a novel facial expression recognizer and describes its application to group meeting analysis. Our goal is to automatically discover the interpersonal emotions that evolve over time in meetings, e.g. how each person feels about the others, or who affectively influences the others the most. As the emotion cue, we focus on facial expression, more specifically smile, and aim to recognize ``who is smiling at whom, when, and how often'', since frequently smiling carries affective messages that are strongly directed to the person being looked at; this point of view is our novelty. To detect such communicative smiles, we propose a new algorithm that jointly estimates facial pose and expression in the framework of the particle filter. The main feature is its automatic selection of interest points that can robustly capture small changes in expression even in the presence of large head rotations. Based on the recognized facial expressions and their directions to others, which are indicated by the estimated head poses, we visualize interpersonal smile events as a graph structure, we call it the interpersonal emotional network; it is intended to indicate the emotional relationships among meeting participants. A four-person meeting captured by an omnidirectional video system is used to confirm the effectiveness of the proposed method and the potential of our approach for deep understanding of human relationships developed through communications.","2009-11-02","2024-01-08 12:25:18","2024-01-08 12:25:18","2024-01-08","99–106","","","","","","","ICMI-MLMI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","direction of facial expression; facial expression; interpersonal emotion; meeting analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"72W283R7","journalArticle","2002","Monk, Andrew F.; Gale, Caroline","A Look Is Worth a Thousand Words: Full Gaze Awareness in Video-Mediated Conversation","Discourse Processes","","0163-853X","10.1207/S15326950DP3303_4","https://doi.org/10.1207/S15326950DP3303_4","Full gaze awareness, defined here as knowing what someone is looking at, might be expected to be a powerful communicative resource when the conversation concerns some object of common interest in the environment. This article sets out to demonstrate this possibility in the context of video-mediated communication. An experiment is reported in which pairs complete a communication task using a novel apparatus that supports full gaze awareness (GA) and mutual gaze (eye contact). This ""GA display"" was contrasted with 2 control conditions, mutual gaze without full gaze awareness and audio only. The GA display reduced the number of turns and number of words required to complete the task by about 1/2 in comparison with the 2 control conditions. The results of a subsequent conversational games analysis suggest that at least part of this saving comes about because full gaze awareness provides an alternative nonlinguistic channel for checking one's own and the other person's understanding of what was said.","2002-05-01","2024-01-08 12:25:37","2024-01-08 12:25:37","2024-01-08 12:25:37","257-278","","3","33","","","A Look Is Worth a Thousand Words","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1207/S15326950DP3303_4","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y55WPXXF","conferencePaper","2007","Sturm, Janienke; Herwijnen, Olga Houben-van; Eyck, Anke; Terken, Jacques","Influencing social dynamics in meetings through a peripheral display","Proceedings of the 9th international conference on Multimodal interfaces","978-1-59593-817-6","","10.1145/1322192.1322238","https://doi.org/10.1145/1322192.1322238","We present a service providing real-time feedback to participants of small group meetings on the social dynamics of the meeting. The service measures and visualizes properties of participants' behaviour that are relevant to the social dynamics of the meeting: speaking time and gaze behaviour. The dynamic visualization is offered to meeting participants during the meeting through a peripheral display. Whereas an initial version was evaluated using wizards to obtain the required information about gazing behaviour and speaking activity instead of perceptual systems, in the current paper we employ a system including automated perceptual components. We describe the system properties and the perceptual components. The service was evaluated in a within-subjects experiment, where groups of participants discussed topics of general interest, with a total of 82 participants. It was found that the presence of the feedback about speaking time influenced the behaviour of the participants in such a way that it made over-participators to behave less dominant and under-participators to become more active. Feedback on eye gaze behaviour did not affect participants' gazing behaviour (both for listeners and for speakers) during the meeting.","2007-11-12","2024-01-08 12:25:56","2024-01-08 12:25:56","2024-01-08","263–270","","","","","","","ICMI '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","head orientation detection; meetings; peripheral display; social dynamics; speech activity detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YNJKY22H","conferencePaper","2003","Ou, Jiazhi; Fussell, Susan R.; Chen, Xilin; Setlock, Leslie D.; Yang, Jie","Gestural communication over video stream: supporting multimodal interaction for remote collaborative physical tasks","Proceedings of the 5th international conference on Multimodal interfaces","978-1-58113-621-0","","10.1145/958432.958477","https://doi.org/10.1145/958432.958477","We present a system integrating gesture and live video to support collaboration on physical tasks. The architecture combines network IP cameras, desktop PCs, and tablet PCs to allow a remote helper to draw on a video feed of a workspace as he/she provides task instructions. A gesture recognition component enables the system both to normalize freehand drawings to facilitate communication with remote partners and to use pen-based input as a camera control device. Results of a preliminary user study suggest that our gesture over video communication system enhances task performance over traditional video-only systems. Implications for the design of multimodal systems to support collaborative physical tasks are also discussed.","2003-11-05","2024-01-08 12:26:21","2024-01-08 12:26:21","2024-01-08","242–249","","","","","","Gestural communication over video stream","ICMI '03","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","computer-supported cooperative work; gestural communication; gesture recognition; multimodal interaction; video conferencing; video mediated communication; video stream","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCDVWHVI","conferencePaper","2005","Ou, Jiazhi; Oh, Lui Min; Fussell, Susan R.; Blum, Tal; Yang, Jie","Analyzing and predicting focus of attention in remote collaborative tasks","Proceedings of the 7th international conference on Multimodal interfaces","978-1-59593-028-6","","10.1145/1088463.1088485","https://doi.org/10.1145/1088463.1088485","To overcome the limitations of current technologies for remote collaboration, we propose a system that changes a video feed based on task properties, people's actions, and message properties. First, we examined how participants manage different visual resources in a laboratory experiment using a collaborative task in which one partner (the helper) instructs another (the worker) how to assemble online puzzles. We analyzed helpers' eye gaze as a function of the aforementioned parameters. Helpers gazed at the set of alternative pieces more frequently when it was harder for workers to differentiate these pieces, and less frequently over repeated trials. The results further suggest that a helper's desired focus of attention can be predicted based on task properties, his/her partner's actions, and message properties. We propose a conditional Markov model classifier to explore the feasibility of predicting gaze based on these properties. The accuracy of the model ranged from 65.40% for puzzles with easy-to-name pieces to 74.25% for puzzles with more difficult to name pieces. The results suggest that we can use our model to automatically manipulate video feeds to show what helpers want to see when they want to see it.","2005-10-04","2024-01-08 12:26:47","2024-01-08 12:26:47","2024-01-08","116–123","","","","","","","ICMI '05","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/HIW9GTZB/Ou et al. - 2005 - Analyzing and predicting focus of attention in rem.pdf","","","computer-supported cooperative work; eye tracking; focus of attention; keyword spotting; remote collaborative tasks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UE5UDJNE","journalArticle","2008","Brennan, Susan E.; Chen, Xin; Dickinson, Christopher A.; Neider, Mark B.; Zelinsky, Gregory J.","Coordinating cognition: the costs and benefits of shared gaze during collaborative search","Cognition","","0010-0277","10.1016/j.cognition.2007.05.012","","Collaboration has its benefits, but coordination has its costs. We explored the potential for remotely located pairs of people to collaborate during visual search, using shared gaze and speech. Pairs of searchers wearing eyetrackers jointly performed an O-in-Qs search task alone, or in one of three collaboration conditions: shared gaze (with one searcher seeing a gaze-cursor indicating where the other was looking, and vice versa), shared-voice (by speaking to each other), and shared-gaze-plus-voice (by using both gaze-cursors and speech). Although collaborating pairs performed better than solitary searchers, search in the shared gaze condition was best of all: twice as fast and efficient as solitary search. People can successfully communicate and coordinate their searching labor using shared gaze alone. Strikingly, shared gaze search was even faster than shared-gaze-plus-voice search; speaking incurred substantial coordination costs. We conclude that shared gaze affords a highly efficient method of coordinating parallel activity in a time-critical spatial task.","2008-03","2024-01-08 12:27:14","2024-01-08 12:27:14","","1465-1477","","3","106","","Cognition","Coordinating cognition","","","","","","","eng","","","","","PubMed","","PMID: 17617394","","","http://www.ncbi.nlm.nih.gov/pubmed/17617394","","Attention; Cognition; Communication; Cooperative Behavior; Fixation, Ocular; Humans; Visual Perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8A9IXEW3","conferencePaper","2004","Stein, Randy; Brennan, Susan E.","Another person's eye gaze as a cue in solving programming problems","Proceedings of the 6th international conference on Multimodal interfaces","978-1-58113-995-2","","10.1145/1027933.1027936","https://doi.org/10.1145/1027933.1027936","Expertise in computer programming can often be difficult to transfer verbally. Moreover, technical training and communication occur more and more between people who are located at a distance. We tested the hypothesis that seeing one person's visual focus of attention (represented as an eyegaze cursor) while debugging software (displayed as text on a screen) can be helpful to another person doing the same task. In an experiment, a group of professional programmers searched for bugs in small Java programs while wearing an unobtrusive head-mounted eye tracker. Later, a second set of programmers searched for bugs in the same programs. For half of the bugs, the second set of programmers first viewed a recording of an eyegaze cursor from one of the first programmers displayed over the (indistinct) screen of code, and for the other half they did not. The second set of programmers found the bugs more quickly after viewing the eye gaze of the first programmers, suggesting that another person's eye gaze, produced instrumentally (as opposed to intentionally, like pointing with a mouse), can be a useful cue in problem solving. This finding supports the potential of eye gaze as a valuable cue for collaborative interaction in a visuo-spatial task conducted at a distance.","2004-10-13","2024-01-08 12:27:31","2024-01-08 12:27:31","2024-01-08","9–15","","","","","","","ICMI '04","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","debugging; eye tracking; gaze-based & attentional interfaces; mediated communication; programming; visual co-presence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4UA8GZ6","conferencePaper","2001","Vertegaal, Roel; Slagter, Robert; van der Veer, Gerrit; Nijholt, Anton","Eye gaze patterns in conversations: there is more to conversational agents than meets the eyes","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-58113-327-1","","10.1145/365024.365119","https://doi.org/10.1145/365024.365119","In multi-agent, multi-user environments, users as well as agents should have a means of establishing who is talking to whom. In this paper, we present an experiment aimed at evaluating whether gaze directional cues of users could be used for this purpose. Using an eye tracker, we measured subject gaze at the faces of conversational partners during four-person conversations. Results indicate that when someone is listening or speaking to individuals, there is indeed a high probability that the person looked at is the person listened (p=88%) or spoken to (p=77%). We conclude that gaze is an excellent predictor of conversational attention in multiparty conversations. As such, it may form a reliable source of input for conversational systems that need to establish whom the user is speaking or listening to. We implemented our findings in FRED, a multi-agent conversational system that uses eye input to gauge which agent the user is listening or speaking to.","2001-03-01","2024-01-08 12:27:51","2024-01-08 12:27:51","2024-01-08","301–308","","","","","","Eye gaze patterns in conversations","CHI '01","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/htk/Zotero/storage/E4VDBLFP/Vertegaal et al. - 2001 - Eye gaze patterns in conversations there is more .pdf","","","attention-based interfaces; attentive agents; conversational attention; gaze; multiparty communication; tracking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FFWWUTDJ","journalArticle","2021","Reinero, Diego A.; Dikker, Suzanne; Van Bavel, Jay J.","Inter-brain synchrony in teams predicts collective performance","Social Cognitive and Affective Neuroscience","","1749-5024","10.1093/scan/nsaa135","","Despite decades of research in economics and psychology attempting to identify ingredients that make up successful teams, neuroscientists have only just begun to study how multiple brains interact. Recent research has shown that people's brain activity becomes synchronized with others' (inter-brain synchrony) during social engagement. However, little is known as to whether inter-brain synchrony relates to collective behavior within teams. Here, we merge the nascent field of group neuroscience with the extant literature of team dynamics and collective performance. We recruited 174 participants in groups of 4 and randomly assigned them to complete a series of problem-solving tasks either independently or as a team, while simultaneously recording each person's brain activity using an electroencephalography hyperscanning setup. This design allowed us to examine the relationship between group identification and inter-brain synchrony in explaining collective performance. As expected, teammates identified more strongly with one another, cooperated more on an economic game, and outperformed the average individual on most problem-solving tasks. Crucially, inter-brain synchrony, but not self-reported group identification, predicted collective performance among teams. These results suggest that inter-brain synchrony can be informative in understanding collective performance among teams where self-report measures may fail to capture behavior.","2021-01-18","2024-01-08 12:28:09","2024-01-08 12:28:09","","43-57","","1-2","16","","Soc Cogn Affect Neurosci","","","","","","","","eng","","","","","PubMed","","PMID: 32991728 PMCID: PMC7812618","","/Users/htk/Zotero/storage/GNPH7R6A/Reinero et al. - 2021 - Inter-brain synchrony in teams predicts collective.pdf; ","http://www.ncbi.nlm.nih.gov/pubmed/32991728","","Brain; collective performance; Cooperative Behavior; Electroencephalography; group identification; Group Processes; Humans; hyperscanning; inter-brain synchrony; Interpersonal Relations; Male; Problem Solving; Random Allocation; Social Identification; teams; Young Adult","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SUM6W45U","conferencePaper","2021","Feng, Shuo; Wang, Shuwen; Chen, Yufei; Zhang, Sixu; Zhang, Lishan","An Integrated Observing Technic for Collaborative Learning: The Multimodal Learning Analytics Based on the Video Coding and EEG Data Mining","2021 IEEE International Conference on Engineering, Technology & Education (TALE)","","","10.1109/TALE52509.2021.9678555","https://ieeexplore.ieee.org/document/9678555","Observing collaborative learning in real contexts has always been a problem because of the difficulty of representing the learning engagement, a multi-facet construct including behavior, cognition and emotion. This study adopts multimodal learning analytics to tackle it. Specifically, it is an integrated observation technic to represent it at both macro and micro levels through video coding and electroencephalogram (EEG) data mining. The analysis of a quasi-experiment was conducted to justify the feasibility. And this study also finds that learning engagement is related with the learning gain in collaborative learning.","2021-12","2024-01-08 12:28:31","2024-01-08 12:28:31","2024-01-08 12:28:31","1097-1101","","","","","","An Integrated Observing Technic for Collaborative Learning","","","","","","","","","","","","IEEE Xplore","","ISSN: 2470-6698","","/Users/htk/Zotero/storage/ZLNT96XP/9678555.html; /Users/htk/Zotero/storage/UJFD2VCM/Feng et al. - 2021 - An Integrated Observing Technic for Collaborative .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 IEEE International Conference on Engineering, Technology & Education (TALE)","","","","","","","","","","","","","","",""
"3G3HEKEG","conferencePaper","2020","Cukurova, Mutlu; Zhou, Qi; Spikol, Daniel; Landolfi, Lorenzo","Modelling collaborative problem-solving competence with transparent learning analytics: is video data enough?","Proceedings of the Tenth International Conference on Learning Analytics & Knowledge","978-1-4503-7712-6","","10.1145/3375462.3375484","https://doi.org/10.1145/3375462.3375484","In this study, we describe the results of our research to model collaborative problem-solving (CPS) competence based on analytics generated from video data. We have collected ~500 mins video data from 15 groups of 3 students working to solve design problems collaboratively. Initially, with the help of OpenPose, we automatically generated frequency metrics such as the number of the face-in-the-screen; and distance metrics such as the distance between bodies. Based on these metrics, we built decision trees to predict students' listening, watching, making, and speaking behaviours as well as predicting the students' CPS competence. Our results provide useful decision rules mined from analytics of video data which can be used to inform teacher dashboards. Although, the accuracy and recall values of the models built are inferior to previous machine learning work that utilizes multimodal data, the transparent nature of the decision trees provides opportunities for explainable analytics for teachers and learners. This can lead to more agency of teachers and learners, therefore can lead to easier adoption. We conclude the paper with a discussion on the value and limitations of our approach.","2020-03-23","2024-01-08 12:29:02","2024-01-08 12:29:02","2024-01-08","270–275","","","","","","Modelling collaborative problem-solving competence with transparent learning analytics","LAK '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","collaborative problem-solving; decision trees; multimodal learning analytics; physical learning analytics; video analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R6J7Z2LE","journalArticle","2021","Nasir, Jauwairia; Kothiyal, Aditi; Bruno, Barbara; Dillenbourg, Pierre","Many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities","International Journal of Computer-Supported Collaborative Learning","","1556-1615","10.1007/s11412-021-09358-2","https://doi.org/10.1007/s11412-021-09358-2","Understanding the way learners engage with learning technologies, and its relation with their learning, is crucial for motivating design of effective learning interventions. Assessing the learners’ state of engagement, however, is non-trivial. Research suggests that performance is not always a good indicator of learning, especially with open-ended constructivist activities. In this paper, we describe a combined multi-modal learning analytics and interaction analysis method that uses video, audio and log data to identify multi-modal collaborative learning behavioral profiles of 32 dyads as they work on an open-ended task around interactive tabletops with a robot mediator. These profiles, which we name Expressive Explorers, Calm Tinkerers, and Silent Wanderers, confirm previous collaborative learning findings. In particular, the amount of speech interaction and the overlap of speech between a pair of learners are behavior patterns that strongly distinguish between learning and non-learning pairs. Delving deeper, findings suggest that overlapping speech between learners can indicate engagement that is conducive to learning. When we more broadly consider learner affect and actions during the task, we are better able to characterize the range of behavioral profiles exhibited among those who learn. Specifically, we discover two behavioral dimensions along which those who learn vary, namely, problem solving strategy (actions) and emotional expressivity (affect). This finding suggests a relation between problem solving strategy and emotional behavior; one strategy leads to more frustration compared to another. These findings have implications for the design of real-time learning interventions that support productive collaborative learning in open-ended tasks.","2021-12-01","2024-01-08 12:29:47","2024-01-08 12:29:47","2024-01-08 12:29:47","485-523","","4","16","","Intern. J. Comput.-Support. Collab. Learn","","","","","","","","en","","","","","Springer Link","","","","/Users/htk/Zotero/storage/HZPHN4IX/Nasir et al. - 2021 - Many are the ways to learn identifying multi-modal.pdf","","","Collaborative learning; Engagement; Human-robot interaction; Learning analytics; Multi-modal; Social robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8YVJH4T7","conferencePaper","2008","Molinari, Gaëlle; Sangin, Mirweis; Nüssli, Marc-Antoine; Dillenbourg, Pierre","Effects of knowledge interdependence with the partner on visual and action transactivity in collaborative concept mapping","Proceedings of the 8th international conference on International conference for the learning sciences - Volume 2","","","","","In the present study, participants working in dyads were asked to build a concept map collaboratively. While interacting, they were able to access visualizations (individual concept maps) of both their own and their partner's prior knowledge (own and peer maps). Eye movements of both learning partners were recorded during the course of collaboration. Our goal was twofold. First, we focused on transactivity at both the visual and action levels. Second, we investigated the effects of knowledge interdependence with the partner on transactivity in collaborative concept mapping. We found that the degree to which participants co-manipulate the same objects in the collaborative map (action transactivity) is higher when they discussed identical (rather than complementary) information. Results from eye-gaze data showed that participants who shared complementary information transitioned more frequently between their own map and their partner's map; eye-movement transitions between own and peer maps were also negatively correlated with learning outcomes.","2008-06-24","2024-01-08 12:30:48","2024-01-08 12:30:48","2024-01-08","91–98","","","","","","","ICLS'08","","","","International Society of the Learning Sciences","Utrecht, The Netherlands","","","","","","ACM Digital Library","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLXYQ478","journalArticle","2009","Jiang, Lai; Elen, Jan; Clarebout, Geraldine","The relationships between learner variables, tool-usage behaviour and performance","Computers in Human Behavior","","0747-5632","10.1016/j.chb.2008.11.006","https://www.sciencedirect.com/science/article/pii/S0747563208002057","A variety of learner variables have been asserted to affect learners’ tool-usage behaviour. However, direct empirical evidence on which learner variables influence tool-usage behaviour and how is limited. In order to better understand the impact of learner variables on tool use, the current study investigates the relationships between learner variables, the quantitative aspects of tool-usage behaviour and its outcome (i.e., performance). More specifically, the focus is on how the variation in tool use is related to prior knowledge and goal orientation and how this variation affects performance. Tool-use data were extracted from log-file data collected in an open-ended learning environment [Clarebout, G. (2005). The enhancement of optimal tool use in open learning environments (Doctoral dissertation). Katholieke universiteit Leuven, Leuven.]. Results partly revealed the hypothesized relationships between learner variables, tool-usage behaviour (the proportion of time spent on tools) and performance. The results suggest that in order to have a more thorough understanding of the relationships between learner variables, tool-usage behaviour and learning outcomes, log-file data may need to be complemented with direct observations to acknowledge the qualitative aspects of the tool-usage behaviour (i.e., mixed method approach).","2009-03-01","2024-01-08 12:31:08","2024-01-08 12:31:08","2024-01-08 12:31:08","501-509","","2","25","","Computers in Human Behavior","","Including the Special Issue: State of the Art Research into Cognitive Load Theory","","","","","","","","","","","ScienceDirect","","","","/Users/htk/Zotero/storage/PSDKJ77E/S0747563208002057.html","","","Goal orientation; Log files; Prior knowledge; Tool use","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WMKRJHPC","conferencePaper","2013","Grafsgaard, Joseph F.; Wiggins, Joseph B.; Boyer, Kristy Elizabeth; Wiebe, Eric N.; Lester, James C.","Automatically Recognizing Facial Indicators of Frustration: A Learning-centric Analysis","2013 Humaine Association Conference on Affective Computing and Intelligent Interaction","","","10.1109/ACII.2013.33","https://ieeexplore.ieee.org/document/6681424","Affective and cognitive processes form a rich substrate on which learning plays out. Affective states often influence progress on learning tasks, resulting in positive or negative cycles of affect that impact learning outcomes. Developing a detailed account of the occurrence and timing of cognitive-affective states during learning can inform the design of affective tutorial interventions. In order to advance understanding of learning-centered affect, this paper reports on a study to analyze a video corpus of computer-mediated human tutoring using an automated facial expression recognition tool that detects fine-grained facial movements. The results reveal three significant relationships between facial expression, frustration, and learning: (1) Action Unit 2 (outer brow raise) was negatively correlated with learning gain, (2) Action Unit 4 (brow lowering) was positively correlated with frustration, and (3) Action Unit 14 (mouth dimpling) was positively correlated with both frustration and learning gain. Additionally, early prediction models demonstrated that facial actions during the first five minutes were significantly predictive of frustration and learning at the end of the tutoring session. The results represent a step toward a deeper understanding of learning-centered affective states, which will form the foundation for data-driven design of affective tutoring systems.","2013-09","2024-01-08 12:32:05","2024-01-08 12:32:05","2024-01-08 12:32:05","159-165","","","","","","Automatically Recognizing Facial Indicators of Frustration","","","","","","","","","","","","IEEE Xplore","","ISSN: 2156-8111","","/Users/htk/Zotero/storage/PYUHJL93/6681424.html; /Users/htk/Zotero/storage/IJX2PVH5/Grafsgaard et al. - 2013 - Automatically Recognizing Facial Indicators of Fru.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2013 Humaine Association Conference on Affective Computing and Intelligent Interaction","","","","","","","","","","","","","","",""
"ITTEGM8S","journalArticle","2019","Antonenko, Pavlo D.; Davis, Robert; Wang, Jiahui; Celepkolu, Mehmet","On the Same Wavelength: Exploring Team Neurosynchrony in Undergraduate Dyads Solving a Cyberlearning Problem With Collaborative Scripts","Mind, Brain, and Education","","1751-228X","10.1111/mbe.12187","https://onlinelibrary.wiley.com/doi/abs/10.1111/mbe.12187","As teammates adjust their cognition and behavior, synchronizations of information can be observed across verbal, postural, and neurophysiological systems. This study explored the synchrony of mutually interacting brains, or team neurosynchrony, during cyber-enabled collaborative problem solving. Mixed-sex dyads defined and solved an authentic problem using either a social script or an epistemic script. Alpha-band phase-locking value, or the absolute value of the sum of the phase differences of electrodes at a particular time and frequency across a number of epochs, was used as a measure of team neurosynchrony. Contrary to our hypotheses, analyses revealed greater alpha-band phase-locking values between the central and parietal electrodes of dyad members in the epistemic script condition. Mean alpha phase-locking values were positively correlated with collaborative problem solving performance and negatively correlated with time spent on the problem solving process, suggesting that epistemic scripts were more effective scaffolds of collaborative problem solving compared to social scripts in this study.","2019","2024-01-08 12:32:40","2024-01-08 12:32:40","2024-01-08 12:32:39","4-13","","1","13","","","On the Same Wavelength","","","","","","","en","© 2019 International Mind, Brain, and Education Society and Wiley Periodicals, Inc.","","","","Wiley Online Library","","_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/mbe.12187","","/Users/htk/Zotero/storage/D2VQ94FJ/mbe.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2EDTTAN","journalArticle","2005","Richardson, Daniel C.; Dale, Rick","Looking To Understand: The Coupling Between Speakers' and Listeners' Eye Movements and Its Relationship to Discourse Comprehension","Cognitive Science","","1551-6709","10.1207/s15516709cog0000_29","https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_29","We investigated the coupling between a speaker's and a listener's eye movements. Some participants talked extemporaneously about a television show whose cast members they were viewing on a screen in front of them. Later, other participants listened to these monologues while viewing the same screen. Eye movements were recorded for all speakers and listeners. According to cross-recurrence analysis, a listener's eye movements most closely matched a speaker's eye movements at a delay of 2 sec. Indeed, the more closely a listener's eye movements were coupled with a speaker's, the better the listener did on a comprehension test. In a second experiment, low-level visual cues were used to manipulate the listeners' eye movements, and these, in turn, influenced their latencies to comprehension questions. Just as eye movements reflect the mental state of an individual, the coupling between a speaker's and a listener's eye movements reflects the success of their communication.","2005","2024-01-08 12:32:58","2024-01-08 12:32:58","2024-01-08 12:32:58","1045-1060","","6","29","","","Looking To Understand","","","","","","","en","© 2005 Cognitive Science Society, Inc.","","","","Wiley Online Library","","_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog0000_29","","/Users/htk/Zotero/storage/2KLJ7RU2/Richardson and Dale - 2005 - Looking To Understand The Coupling Between Speake.pdf; /Users/htk/Zotero/storage/G6HUVC2B/s15516709cog0000_29.html","","","Attention; Communication; Discourse; Eye movements; Human experimentation; Language understanding; Perception; Psychology; Situated cognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HM9VVP66","conferencePaper","2013","Sharma, Kshitij; Jermann, Patrick; Nüssli, Marc-Antoine; Dillenbourg, Pierre","Understanding Collaborative Program Comprehension: Interlacing Gaze and Dialogues","To See the World and a Grain of Sand: Learning across Levels of Space, Time, and Scale: CSCL 2013 Conference Proceedings Volume 1 — Full Papers & Symposia","","","","","We study the interaction of the participants in a pair program comprehension task across different time scales in a dual eye-tracking setup. We identify four layers of interaction episodes at different time scales. Each layer spans across the whole interaction. The present study concerns the relationship between different layers at different time scales. The first and third layers are based on the utterances of the participants while the second and fourth layers are based on participants' gaze","2013","2024-01-08 12:37:43","2024-01-08 12:42:53","","430-437","","","","","","Understanding Collaborative Program Comprehension","","","","","International Society of the Learning Sciences","Madison, Wisconsin, USA, June 15-19, 2013","","","","","","Infoscience","","Meeting Name: Computer Supported Collaborative Learning (CSCL 2013)","","","","","Dual Eye Tracking; Interaction Segmentation; Program Comprehension; Time granularities","Rummel, Nikol; Kapur, Manu; Puntambekar, S","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Computer Supported Collaborative Learning (CSCL 2013)","","","","","","","","","","","","","","",""
"J84ZDZ6G","report","2014","Belenky, Daniel; Ringenberg, Michael; Olsen, Jennifer; Aleven, Vincent; Rummel, Nikol","Using Dual Eye-Tracking to Evaluate Students' Collaboration with an Intelligent Tutoring System for Elementary-Level Fractions","","","","","https://eric.ed.gov/?id=ED556498","As learning technologies proliferate, it is important for research to address how to best align instruction to educational goals. For example, recent evidence indicates that working collaboratively may have unique benefits for facilitating the acquisition of conceptual understanding, as opposed to procedural fluency (Mullins, Rummel & Spada, 2011). To investigate this effect, we leverage and expand upon a new methodology, dual eye-tracking, to understand how collaborators' joint attention may impact learning in a collaboration-enabled Intelligent Tutoring System for fractions. We present results from a study in which 28 pairs of 4th and 5th grade students completed a set of either conceptually-or procedurally-oriented instructional activities in a school setting. Results indicate that students collaborating exhibited learning gains for conceptual knowledge, but not for procedural knowledge, and that more joint attention was related to learning gains. These results may inform the design of future learning technologies, and illustrate the utility of using dual eye-tracking to study collaboration.","2014-07","2024-01-08 12:43:25","2024-01-08 12:43:25","2024-01-08 12:43:25","","","","","","","","","","","","","","en","","","","","ERIC","","Publication Title: Grantee Submission ERIC Number: ED556498","","/Users/htk/Zotero/storage/7LZBX8R9/Belenky et al. - 2014 - Using Dual Eye-Tracking to Evaluate Students' Coll.pdf","","","Alignment (Education); Concept Formation; Cooperative Learning; Educational Technology; Elementary School Mathematics; Eye Movements; Grade 4; Grade 5; Intelligent Tutoring Systems; Knowledge Level; Learning Activities; Mathematical Concepts; Pretests Posttests; Statistical Analysis; Teaching Methods","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9DDI7BH5","journalArticle","2017","Sharma, Kshitij; Chavez-Demoulin, Valérie; Dillenbourg, Pierre","An Application of Extreme Value Theory to Learning Analytics: Predicting Collaboration Outcome from Eye-Tracking Data","Journal of Learning Analytics","","","","https://eric.ed.gov/?id=EJ1163806","The statistics used in education research are based on central trends such as the mean or standard deviation, discarding outliers. This paper adopts another viewpoint that has emerged in statistics, called extreme value theory (EVT). EVT claims that the bulk of normal distribution is comprised mainly of uninteresting variations while the most extreme values convey more information. We apply EVT to eye-tracking data collected during online collaborative problem solving with the aim of predicting the quality of collaboration. We compare our previous approach, based on central trends, with an EVT approach focused on extreme episodes of collaboration. The latter provided a better prediction of the quality of collaboration.","2017","2024-01-08 12:44:14","2024-01-08 12:44:14","2024-01-08 12:44:14","140-164","","3","4","","","An Application of Extreme Value Theory to Learning Analytics","","","","","","","en","","","","","ERIC","","Publisher: Society for Learning Analytics Research ERIC Number: EJ1163806","","/Users/htk/Zotero/storage/NVQ95DME/Sharma et al. - 2017 - An Application of Extreme Value Theory to Learning.pdf","","","Comparative Analysis; Computer Mediated Communication; Cooperative Learning; Data Collection; Educational Research; Eye Movements; Foreign Countries; Predictor Variables; Pretests Posttests; Problem Solving; Statistical Analysis; Statistical Distributions; Theories","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"59DWV4BM","journalArticle","2020","Olsen, Jennifer K.; Sharma, Kshitij; Rummel, Nikol; Aleven, Vincent","Temporal analysis of multimodal data to predict collaborative learning outcomes","British Journal of Educational Technology","","1467-8535","10.1111/bjet.12982","https://onlinelibrary.wiley.com/doi/abs/10.1111/bjet.12982","The analysis of multiple data streams is a long-standing practice within educational research. Both multimodal data analysis and temporal analysis have been applied successfully, but in the area of collaborative learning, very few studies have investigated specific advantages of multiple modalities versus a single modality, especially combined with temporal analysis. In this paper, we investigate how both the use of multimodal data and moving from averages and counts to temporal aspects in a collaborative setting provides a better prediction of learning gains. To address these questions, we analyze multimodal data collected from 25 9–11-year-old dyads using a fractions intelligent tutoring system. Assessing the relation of dual gaze, tutor log, audio and dialog data to students' learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality. Our work contributes to the understanding of how analyzing multimodal data in temporal manner provides additional information around the collaborative learning process.","2020","2024-01-08 12:46:05","2024-01-08 12:46:05","2024-01-08 12:46:05","1527-1547","","5","51","","","","","","","","","","en","© 2020 British Educational Research Association","","","","Wiley Online Library","","_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.12982","","/Users/htk/Zotero/storage/7I2CZ32G/bjet.html","","","collaborative learning; learning analytics; multimodal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5C3M6BQU","conferencePaper","2020","Subburaj, Shree Krishna; Stewart, Angela E.B.; Ramesh Rao, Arjun; D'Mello, Sidney K.","Multimodal, Multiparty Modeling of Collaborative Problem Solving Performance","Proceedings of the 2020 International Conference on Multimodal Interaction","978-1-4503-7581-8","","10.1145/3382507.3418877","https://doi.org/10.1145/3382507.3418877","Modeling team phenomena from multiparty interactions inherently requires combining signals from multiple teammates, often by weighting strategies. Here, we explored the hypothesis that strategic weighting signals from individual teammates would outperform an equal weighting baseline. Accordingly, we explored role-, trait-, and behavior-based weighting of behavioral signals across team members. We analyzed data from 101 triads engaged in computer-mediated collaborative problem solving (CPS) in an educational physics game. We investigated the accuracy of machine-learned models trained on facial expressions, acoustic-prosodics, eye gaze, and task context information, computed one-minute prior to the end of a game level, at predicting success at solving that level. AUROCs for unimodal models that equally weighted features from the three teammates ranged from .54 to .67, whereas a combination of gaze, face, and task context features, achieved an AUROC of .73. The various multiparty weighting strategies did not outperform an equal-weighting baseline. However, our best nonverbal model (AUROC = .73) outperformed a language-based model (AUROC = .67), and there were some advantages to combining the two (AUROC = .75). Finally, models aimed at prospectively predicting performance on a minute-by-minute basis from the start of the level achieved a lower, but still above-chance, AUROC of .60. We discuss implications for multiparty modeling of team performance and other team constructs.","2020-10-22","2024-01-08 12:46:33","2024-01-08 12:46:33","2024-01-08","423–432","","","","","","","ICMI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","collaborative problem solving; multimodal multiparty modeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LGXGLKQ2","journalArticle","2018","Villanueva, Idalis; Campbell, Brett D.; Raikes, Adam C.; Jones, Suzanne H.; Putney, LeAnn G.","A Multimodal Exploration of Engineering Students Emotion and Electrodermal Activity in Design Activities","Journal of Engineering Education","","2168-9830","10.1002/jee.20225","https://onlinelibrary.wiley.com/doi/abs/10.1002/jee.20225","Background This exploratory study uses multimodal approaches to explore undergraduate student engagement via topic emotions and electrodermal activity (EDA) in different engineering design method activities and with different instructional delivery formats (e.g., lecture vs. active learning). Purpose/Hypothesis The goal of this research is to improve our understanding of how students respond, via engagement, to their engineering design activities during class. This study hypothesizes that students would experience no self-reported mean changes in topic emotions from their preassessment scores for each engineering design topic and instructional format nor would electrodermal activities (EDA) associate to these topic emotions throughout the design activities. Design/Method Eighty-eight freshmen engineering students completed online pretopic and posttopic emotions surveys for five engineering design activities. A subset of 14–18 participants, the focal point of this study, wore an EDA sensor while completing the surveys and participating in these sessions. Results Preliminary findings suggest that EDA increased for individual and collaborative active learning activities compared to lectures. No significant changes in EDA were found between individual and collaborative active learning activities. Moderate negative correlations were found between EDA and negative topic emotions in the first engineering design activity but not across the rest. At the end of the semester, active learning activities showed higher effect sizes indicating a re-enforcement of students' engagement in the engineering design method activities. Conclusion This study provides initial results showing how multimodal approaches can help researchers understand students' closer-to-real-time engagement in engineering design topics and instructional delivery formats.","2018","2024-01-08 12:46:56","2024-01-08 12:46:56","2024-01-08 12:46:56","414-441","","3","107","","","","","","","","","","en","© 2018 ASEE","","","","Wiley Online Library","","_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jee.20225","","/Users/htk/Zotero/storage/GKLPAL4F/jee.html","","","affective theories; design projects; electrodermal activity; learning environment; multimodal methods","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3DWGFDP","journalArticle","2020","Sharma, Kshitij; Leftheriotis, Ioannis; Giannakos, Michail","Utilizing Interactive Surfaces to Enhance Learning, Collaboration and Engagement: Insights from Learners’ Gaze and Speech","Sensors","","1424-8220","10.3390/s20071964","https://www.mdpi.com/1424-8220/20/7/1964","Interactive displays are becoming increasingly popular in informal learning environments as an educational technology for improving students’ learning and enhancing their engagement. Interactive displays have the potential to reinforce and maintain collaboration and rich-interaction with the content in a natural and engaging manner. Despite the increased prevalence of interactive displays for learning, there is limited knowledge about how students collaborate in informal settings and how their collaboration around the interactive surfaces influences their learning and engagement. We present a dual eye-tracking study, involving 36 participants, a two-staged within-group experiment was conducted following single-group time series design, involving repeated measurement of participants’ gaze, voice, game-logs and learning gain tests. Various correlation, regression and covariance analyses employed to investigate students’ collaboration, engagement and learning gains during the activity. The results show that collaboratively, pairs who have high gaze similarity have high learning outcomes. Individually, participants spending high proportions of time in acquiring the complementary information from images and textual parts of the learning material attain high learning outcomes. Moreover, the results show that the speech could be an interesting covariate while analyzing the relation between the gaze variables and the learning gains (and task-based performance). We also show that the gaze is an effective proxy to cognitive mechanisms underlying collaboration not only in formal settings but also in informal learning scenarios.","2020-01","2024-01-08 12:47:35","2024-01-08 12:47:35","2024-01-08 12:47:35","1964","","7","20","","","Utilizing Interactive Surfaces to Enhance Learning, Collaboration and Engagement","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","Number: 7 Publisher: Multidisciplinary Digital Publishing Institute","","/Users/htk/Zotero/storage/MB6DJEAH/Sharma et al. - 2020 - Utilizing Interactive Surfaces to Enhance Learning.pdf","","","collaboration outcome; CSCL; dual eye-tracking; eye-tracking; informal learning; MMLA; mobile eye-tracking; multimodal learning analytics; multitouch interactive displays","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4IN8A6DJ","journalArticle","2018","Spikol, Daniel; Ruffaldi, Emanuele; Dabisias, Giacomo; Cukurova, Mutlu","Supervised machine learning in multimodal learning analytics for estimating success in project-based learning","Journal of Computer Assisted Learning","","1365-2729","10.1111/jcal.12263","https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12263","Multimodal learning analytics provides researchers new tools and techniques to capture different types of data from complex learning activities in dynamic learning environments. This paper investigates the use of diverse sensors, including computer vision, user-generated content, and data from the learning objects (physical computing components), to record high-fidelity synchronised multimodal recordings of small groups of learners interacting. We processed and extracted different aspects of the students' interactions to answer the following question: Which features of student group work are good predictors of team success in open-ended tasks with physical computing? To answer this question, we have explored different supervised machine learning approaches (traditional and deep learning techniques) to analyse the data coming from multiple sources. The results illustrate that state-of-the-art computational techniques can be used to generate insights into the 'black box' of learning in students' project-based activities. The features identified from the analysis show that distance between learners' hands and faces is a strong predictor of students' artefact quality, which can indicate the value of student collaboration. Our research shows that new and promising approaches such as neural networks, and more traditional regression approaches can both be used to classify multimodal learning analytics data, and both have advantages and disadvantages depending on the research questions and contexts being investigated. The work presented here is a significant contribution towards developing techniques to automatically identify the key aspects of students success in project-based learning environments, and to ultimately help teachers provide appropriate and timely support to students in these fundamental aspects.","2018","2024-01-08 12:52:01","2024-01-08 12:52:01","2024-01-08 12:52:01","366-377","","4","34","","","","","","","","","","en","© 2018 John Wiley & Sons, Ltd","","","","Wiley Online Library","","_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcal.12263","","/Users/htk/Zotero/storage/9PXN8F7J/jcal.html; /Users/htk/Zotero/storage/MC889IFW/Spikol et al. - 2018 - Supervised machine learning in multimodal learning.pdf","","","machine learning; multimodal learning analytics; project-based learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7LIZB6DT","journalArticle","2015","Schneider, Bertrand; Blikstein, Paulo","Unraveling Students’ Interaction Around a Tangible Interface Using Multimodal Learning Analytics","Journal of Educational Data Mining","","2157-2100","10.5281/zenodo.3554729","https://jedm.educationaldatamining.org","In this paper, we describe multimodal learning analytics (MMLA) techniques to analyze data collected around an interactive learning environment. In a previous study (Schneider and Blikstein, 2015), we designed and evaluated a Tangible User Interface (TUI) where dyads (i.e., pairs) of students were asked to learn about the human auditory system by reconstructing it. In the current study, we present the analysis of the data collected in the form of logs, both from students' interaction with the tangible interface as well as from their gestures, and we describe how we extracted meaningful predictors for student learning from these two datasets. First we show how information retrieval techniques can be used on the tangible interface logs to predict learning gains. Second, we explored how KinectTM data can inform","2015-10-18","2024-01-08 12:53:17","2024-01-08 12:53:17","2024-01-08 12:53:17","89-116","","3","7","","JEDM","","","","","","","","en","Copyright (c) 2015 JEDM - Journal of Educational Data Mining","","","","jedm.educationaldatamining.org","","Number: 3","","/Users/htk/Zotero/storage/XLSPTA2J/JEDM102.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
